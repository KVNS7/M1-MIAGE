Key,Item Type,Publication Year,Author,Title,Publication Title,ISBN,ISSN,DOI,Url,Abstract Note,Date,Date Added,Date Modified,Access Date,Pages,Num Pages,Issue,Volume,Number Of Volumes,Journal Abbreviation,Short Title,Series,Series Number,Series Text,Series Title,Publisher,Place,Language,Rights,Type,Archive,Archive Location,Library Catalog,Call Number,Extra,Notes,File Attachments,Link Attachments,Manual Tags,Automatic Tags,Editor,Series Editor,Translator,Contributor,Attorney Agent,Book Author,Cast Member,Commenter,Composer,Cosponsor,Counsel,Interviewer,Producer,Recipient,Reviewed Author,Scriptwriter,Words By,Guest,Number,Edition,Running Time,Scale,Medium,Artwork Size,Filing Date,Application Number,Assignee,Issuing Authority,Country,Meeting Name,Conference Name,Court,References,Reporter,Legal Status,Priority Numbers,Programming Language,Version,System,Code,Code Number,Section,Session,Committee,History,Legislative Body,Score
BRZ7CKMX,preprint,2024.0,"Qin, Libo; Chen, Qiguang; Zhou, Yuhang; Chen, Zhi; Li, Yinghui; Liao, Lizi; Li, Min; Che, Wanxiang; Yu, Philip S.","Multilingual Large Language Model: A Survey of Resources, Taxonomy and Frontiers",,,,10.48550/arXiv.2404.04925,http://arxiv.org/abs/2404.04925,"Multilingual Large Language Models are capable of using powerful Large Language Models to handle and respond to queries in multiple languages, which achieves remarkable success in multilingual natural language processing tasks. Despite these breakthroughs, there still remains a lack of a comprehensive survey to summarize existing approaches and recent developments in this field. To this end, in this paper, we present a thorough review and provide a unified perspective to summarize the recent progress as well as emerging trends in multilingual large language models (MLLMs) literature. The contributions of this paper can be summarized: (1) First survey: to our knowledge, we take the first step and present a thorough review in MLLMs research field according to multi-lingual alignment; (2) New taxonomy: we offer a new and unified perspective to summarize the current progress of MLLMs; (3) New frontiers: we highlight several emerging frontiers and discuss the corresponding challenges; (4) Abundant resources: we collect abundant open-source resources, including relevant papers, data corpora, and leaderboards. We hope our work can provide the community with quick access and spur breakthrough research in MLLMs.",2024-04-07,2025-05-22 09:46:57,2025-05-22 09:46:57,2025-05-22 09:46:21,,,,,,,Multilingual Large Language Model,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2404.04925 [cs],,"/Users/ksoares/Zotero/storage/IW8UXR35/Qin et al. - 2024 - Multilingual Large Language Model A Survey of Resources, Taxonomy and Frontiers.pdf; /Users/ksoares/Zotero/storage/2XDX7NT3/2404.html",,,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,arXiv:2404.04925,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
XXFAPMKF,journalArticle,2024.0,"Wu, Xingyu; Wu, Sheng-hao; Wu, Jibin; Feng, Liang; Tan, Kay Chen",Evolutionary computation in the era of large language model: Survey and roadmap,IEEE Transactions on Evolutionary Computation,,,,https://ieeexplore.ieee.org/abstract/document/10767756/,"Large language models (LLMs) have not only revolutionized natural language processing but also extended their prowess to various domains, marking a significant stride toward artificial general intelligence. The interplay between LLMs and evolutionary algorithms (EAs), despite differing in objectives and methodologies, share a common pursuit of applicability in complex problems. Meanwhile, EA can provide an optimization framework for LLM’s further enhancement under closed box settings, empowering LLM with flexible global search capacities. On the other hand, the abundant domain knowledge inherent in LLMs could enable EA to conduct more intelligent searches. Furthermore, the text processing and generative capabilities of LLMs would aid in deploying EAs across a wide range of tasks. Based on these complementary advantages, this article provides a thorough review and a forward-looking roadmap, categorizing the reciprocal inspiration into two main avenues: 1) LLM-enhanced EA and 2) EA-enhanced LLM. Some integrated synergy methods are further introduced to exemplify the complementarity between LLMs and EAs in diverse scenarios, including code generation, software engineering, neural architecture search, and various generation tasks. As the first comprehensive review focused on the EA research in the era of LLMs, this article provides a foundational stepping stone for understanding the collaborative potential of LLMs and EAs. The identified challenges and future directions offer guidance for researchers and practitioners to unlock the full potential of this innovative collaboration in propelling advancements in optimization and artificial intelligence. We have created a GitHub repository to index the relevant papers: https://github.com/wuxingyu-ai/LLM4EC.",2024,2025-05-22 09:46:57,2025-05-26 13:42:45,2025-05-22 09:46:23,,,,,,,Evolutionary computation in the era of large language model,,,,,,,,,,,,Google Scholar,,Publisher: IEEE,,/Users/ksoares/Zotero/storage/3TDR8AL2/Wu et al. - 2024 - Evolutionary computation in the era of large language model Survey and roadmap.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
FXMXHWZU,journalArticle,2024.0,"Wang, Lei; Ma, Chen; Feng, Xueyang; Zhang, Zeyu; Yang, Hao; Zhang, Jingsen; Chen, Zhiyuan; Tang, Jiakai; Chen, Xu; Lin, Yankai; Zhao, Wayne Xin; Wei, Zhewei; Wen, Jirong",A survey on large language model based autonomous agents,Frontiers of Computer Science,,"2095-2228, 2095-2236",10.1007/s11704-024-40231-1,https://link.springer.com/10.1007/s11704-024-40231-1,"Abstract             Autonomous agents have long been a research focus in academic and industry communities. Previous research often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from human learning processes, and makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of Web knowledge, large language models (LLMs) have shown potential in human-level intelligence, leading to a surge in research on LLM-based autonomous agents. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of LLM-based autonomous agents from a holistic perspective. We first discuss the construction of LLM-based autonomous agents, proposing a unified framework that encompasses much of previous work. Then, we present a overview of the diverse applications of LLM-based autonomous agents in social science, natural science, and engineering. Finally, we delve into the evaluation strategies commonly used for LLM-based autonomous agents. Based on the previous studies, we also present several challenges and future directions in this field.",2024-12,2025-05-22 09:46:57,2025-05-22 09:46:57,2025-05-22 09:46:24,186345,,6.0,18.0,,Front. Comput. Sci.,,,,,,,,en,,,,,DOI.org (Crossref),,,,/Users/ksoares/Zotero/storage/F5NWZHDJ/Wang et al. - 2024 - A survey on large language model based autonomous agents.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
V8KEIG8X,preprint,2024.0,"Guo, Taicheng; Chen, Xiuying; Wang, Yaqi; Chang, Ruidi; Pei, Shichao; Chawla, Nitesh V.; Wiest, Olaf; Zhang, Xiangliang",Large Language Model based Multi-Agents: A Survey of Progress and Challenges,,,,10.48550/arXiv.2402.01680,http://arxiv.org/abs/2402.01680,"Large Language Models (LLMs) have achieved remarkable success across a wide array of tasks. Due to the impressive planning and reasoning abilities of LLMs, they have been used as autonomous agents to do many tasks automatically. Recently, based on the development of using one LLM as a single planning or decision-making agent, LLM-based multi-agent systems have achieved considerable progress in complex problem-solving and world simulation. To provide the community with an overview of this dynamic field, we present this survey to offer an in-depth discussion on the essential aspects of multi-agent systems based on LLMs, as well as the challenges. Our goal is for readers to gain substantial insights on the following questions: What domains and environments do LLM-based multi-agents simulate? How are these agents profiled and how do they communicate? What mechanisms contribute to the growth of agents' capacities? For those interested in delving into this field of study, we also summarize the commonly used datasets or benchmarks for them to have convenient access. To keep researchers updated on the latest studies, we maintain an open-source GitHub repository, dedicated to outlining the research on LLM-based multi-agent systems.",2024-04-19,2025-05-22 09:46:57,2025-05-22 09:46:57,2025-05-22 09:46:27,,,,,,,Large Language Model based Multi-Agents,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2402.01680 [cs],,/Users/ksoares/Zotero/storage/SYVU6VCD/Guo et al. - 2024 - Large Language Model based Multi-Agents A Survey of Progress and Challenges.pdf; /Users/ksoares/Zotero/storage/ZAQLAK9I/2402.html,,,Computer Science - Computation and Language; Computer Science - Artificial Intelligence; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,arXiv:2402.01680,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
HZVBHPP3,journalArticle,2024.0,"Cao, Yuji; Zhao, Huan; Cheng, Yuheng; Shu, Ting; Chen, Yue; Liu, Guolong; Liang, Gaoqi; Zhao, Junhua; Yan, Jinyue; Li, Yun","Survey on large language model-enhanced reinforcement learning: Concept, taxonomy, and methods",IEEE Transactions on Neural Networks and Learning Systems,,,,https://ieeexplore.ieee.org/abstract/document/10766898/,"With extensive pretrained knowledge and high-level general capabilities, large language models (LLMs) emerge as a promising avenue to augment reinforcement learning (RL) in aspects, such as multitask learning, sample efficiency, and high-level task planning. In this survey, we provide a comprehensive review of the existing literature in LLM-enhanced RL and summarize its characteristics compared with conventional RL methods, aiming to clarify the research scope and directions for future studies. Utilizing the classical agent–environment interaction paradigm, we propose a structured taxonomy to systematically categorize LLMs’ functionalities in RL, including four roles: information processor, reward designer, decision-maker, and generator. For each role, we summarize the methodologies, analyze the specific RL challenges that are mitigated and provide insights into future directions. Finally, the comparative analysis of each role, potential applications, prospective opportunities, and challenges of the LLM-enhanced RL are discussed. By proposing this taxonomy, we aim to provide a framework for researchers to effectively leverage LLMs in the RL field, potentially accelerating RL applications in complex applications, such as robotics, autonomous driving, and energy systems.",2024,2025-05-22 09:46:57,2025-05-26 13:52:26,2025-05-22 09:46:29,,,,,,,Survey on large language model-enhanced reinforcement learning,,,,,,,,,,,,Google Scholar,,Publisher: IEEE,,"/Users/ksoares/Zotero/storage/MTD8FZCQ/Cao et al. - 2024 - Survey on large language model-enhanced reinforcement learning Concept, taxonomy, and methods.pdf",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
587PUDSY,preprint,2024.0,"Liu, Junwei; Wang, Kaixin; Chen, Yixuan; Peng, Xin; Chen, Zhenpeng; Zhang, Lingming; Lou, Yiling",Large Language Model-Based Agents for Software Engineering: A Survey,,,,10.48550/arXiv.2409.02977,http://arxiv.org/abs/2409.02977,"The recent advance in Large Language Models (LLMs) has shaped a new paradigm of AI agents, i.e., LLM-based agents. Compared to standalone LLMs, LLM-based agents substantially extend the versatility and expertise of LLMs by enhancing LLMs with the capabilities of perceiving and utilizing external resources and tools. To date, LLM-based agents have been applied and shown remarkable effectiveness in Software Engineering (SE). The synergy between multiple agents and human interaction brings further promise in tackling complex real-world SE problems. In this work, we present a comprehensive and systematic survey on LLM-based agents for SE. We collect 106 papers and categorize them from two perspectives, i.e., the SE and agent perspectives. In addition, we discuss open challenges and future directions in this critical domain. The repository of this survey is at https://github.com/FudanSELab/Agent4SE-Paper-List.",2024-09-04,2025-05-22 09:46:57,2025-05-22 09:46:57,2025-05-22 09:46:31,,,,,,,Large Language Model-Based Agents for Software Engineering,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2409.02977 [cs],,/Users/ksoares/Zotero/storage/VHS6DM7Z/Liu et al. - 2024 - Large Language Model-Based Agents for Software Engineering A Survey.pdf; /Users/ksoares/Zotero/storage/GLPDUKAC/2409.html,,,Computer Science - Artificial Intelligence; Computer Science - Software Engineering,,,,,,,,,,,,,,,,,,,arXiv:2409.02977,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
E8LVRGZQ,journalArticle,2025.0,"Xi, Zhiheng; Chen, Wenxiang; Guo, Xin; He, Wei; Ding, Yiwen; Hong, Boyang; Zhang, Ming; Wang, Junzhe; Jin, Senjie; Zhou, Enyu; Zheng, Rui; Fan, Xiaoran; Wang, Xiao; Xiong, Limao; Zhou, Yuhao; Wang, Weiran; Jiang, Changhao; Zou, Yicheng; Liu, Xiangyang; Yin, Zhangyue; Dou, Shihan; Weng, Rongxiang; Qin, Wenjuan; Zheng, Yongyan; Qiu, Xipeng; Huang, Xuanjing; Zhang, Qi; Gui, Tao",The rise and potential of large language model based agents: a survey,Science China Information Sciences,,"1674-733X, 1869-1919",10.1007/s11432-024-4222-0,https://link.springer.com/10.1007/s11432-024-4222-0,"For a long time, researchers have sought artificial intelligence (AI) that matches or exceeds human intelligence. AI agents, which are artificial entities capable of sensing the environment, making decisions, and taking actions, are seen as a means to achieve this goal. Extensive efforts have been made to develop AI agents, with a primary focus on refining algorithms or training strategies to enhance specific skills or particular task performance. The field, however, lacks a sufficiently general and powerful model to serve as a foundation for building general agents adaptable to diverse scenarios. With their versatile capabilities, large language models (LLMs) pave a promising path for the development of general AI agents, and substantial progress has been made in the realm of LLM-based agents. In this article, we conduct a comprehensive survey on LLM-based agents, covering their construction frameworks, application scenarios, and the exploration of societies built upon LLM-based agents. We also conclude some potential future directions and open problems in this flourishing field.",2025-02,2025-05-22 09:46:57,2025-05-26 13:53:12,2025-05-22 09:46:34,121101,,2.0,68.0,,Sci. China Inf. Sci.,The rise and potential of large language model based agents,,,,,,,en,,,,,DOI.org (Crossref),,,,/Users/ksoares/Zotero/storage/TVJZQITW/Xi et al. - 2025 - The rise and potential of large language model based agents a survey.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
DZQV8TI5,preprint,2025.0,"Hu, Sihao; Huang, Tiansheng; Liu, Gaowen; Kompella, Ramana Rao; Ilhan, Fatih; Tekin, Selim Furkan; Xu, Yichang; Yahn, Zachary; Liu, Ling",A Survey on Large Language Model-Based Game Agents,,,,10.48550/arXiv.2404.02039,http://arxiv.org/abs/2404.02039,"The development of game agents holds a critical role in advancing towards Artificial General Intelligence. The progress of Large Language Models (LLMs) offers an unprecedented opportunity to evolve and empower game agents with human-like decision-making capabilities in complex computer game environments. This paper provides a comprehensive overview of LLM-based game agents from a holistic viewpoint. First, we introduce the conceptual architecture of LLM-based game agents, centered around three core functional components: memory, reasoning and in/output. Second, we survey existing representative LLM-based game agents documented in the literature with respect to methodologies and adaptation agility across six genres of games, including adventure, communication, competition, cooperation, simulation, and crafting & exploration games. Finally, we present an outlook of future research and development directions in this burgeoning field. A curated list of relevant papers is maintained and made accessible at: https://github.com/git-disl/awesome-LLM-game-agent-papers.",2025-03-30,2025-05-22 09:46:57,2025-05-22 09:46:57,2025-05-22 09:46:35,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2404.02039 [cs],,/Users/ksoares/Zotero/storage/WQN788GX/Hu et al. - 2025 - A Survey on Large Language Model-Based Game Agents.pdf; /Users/ksoares/Zotero/storage/ZNRFFIP6/2404.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,arXiv:2404.02039,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
EXET649U,preprint,2024.0,"Zhang, Zeyu; Bo, Xiaohe; Ma, Chen; Li, Rui; Chen, Xu; Dai, Quanyu; Zhu, Jieming; Dong, Zhenhua; Wen, Ji-Rong",A Survey on the Memory Mechanism of Large Language Model based Agents,,,,10.48550/arXiv.2404.13501,http://arxiv.org/abs/2404.13501,"Large language model (LLM) based agents have recently attracted much attention from the research and industry communities. Compared with original LLMs, LLM-based agents are featured in their self-evolving capability, which is the basis for solving real-world problems that need long-term and complex agent-environment interactions. The key component to support agent-environment interactions is the memory of the agents. While previous studies have proposed many promising memory mechanisms, they are scattered in different papers, and there lacks a systematical review to summarize and compare these works from a holistic perspective, failing to abstract common and effective designing patterns for inspiring future studies. To bridge this gap, in this paper, we propose a comprehensive survey on the memory mechanism of LLM-based agents. In specific, we first discuss ''what is'' and ''why do we need'' the memory in LLM-based agents. Then, we systematically review previous studies on how to design and evaluate the memory module. In addition, we also present many agent applications, where the memory module plays an important role. At last, we analyze the limitations of existing work and show important future directions. To keep up with the latest advances in this field, we create a repository at \url{https://github.com/nuster1128/LLM_Agent_Memory_Survey}.",2024-04-21,2025-05-22 09:46:57,2025-05-22 09:46:57,2025-05-22 09:46:39,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2404.13501 [cs],,/Users/ksoares/Zotero/storage/EVM2F5EM/Zhang et al. - 2024 - A Survey on the Memory Mechanism of Large Language Model based Agents.pdf; /Users/ksoares/Zotero/storage/2LXLVDMT/2404.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,arXiv:2404.13501,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
2G8QXCBI,preprint,2024.0,"Huang, Yining; Tang, Keke; Chen, Meilian; Wang, Boyuan",A Comprehensive Survey on Evaluating Large Language Model Applications in the Medical Industry,,,,10.48550/arXiv.2404.15777,http://arxiv.org/abs/2404.15777,"Since the inception of the Transformer architecture in 2017, Large Language Models (LLMs) such as GPT and BERT have evolved significantly, impacting various industries with their advanced capabilities in language understanding and generation. These models have shown potential to transform the medical field, highlighting the necessity for specialized evaluation frameworks to ensure their effective and ethical deployment. This comprehensive survey delineates the extensive application and requisite evaluation of LLMs within healthcare, emphasizing the critical need for empirical validation to fully exploit their capabilities in enhancing healthcare outcomes. Our survey is structured to provide an in-depth analysis of LLM applications across clinical settings, medical text data processing, research, education, and public health awareness. We begin by exploring the roles of LLMs in various medical applications, detailing their evaluation based on performance in tasks such as clinical diagnosis, medical text data processing, information retrieval, data analysis, and educational content generation. The subsequent sections offer a comprehensive discussion on the evaluation methods and metrics employed, including models, evaluators, and comparative experiments. We further examine the benchmarks and datasets utilized in these evaluations, providing a categorized description of benchmarks for tasks like question answering, summarization, information extraction, bioinformatics, information retrieval and general comprehensive benchmarks. This structure ensures a thorough understanding of how LLMs are assessed for their effectiveness, accuracy, usability, and ethical alignment in the medical domain. ...",2024-05-29,2025-05-22 09:46:57,2025-05-22 09:46:57,2025-05-22 09:46:43,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2404.15777 [cs],,/Users/ksoares/Zotero/storage/BSJD33XU/Huang et al. - 2024 - A Comprehensive Survey on Evaluating Large Language Model Applications in the Medical Industry.pdf; /Users/ksoares/Zotero/storage/XYCG5P8J/2404.html,,,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,arXiv:2404.15777,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
3FQKAZF2,preprint,2025.0,"Zhang, Chaoyun; He, Shilin; Qian, Jiaxu; Li, Bowen; Li, Liqun; Qin, Si; Kang, Yu; Ma, Minghua; Liu, Guyue; Lin, Qingwei; Rajmohan, Saravan; Zhang, Dongmei; Zhang, Qi",Large Language Model-Brained GUI Agents: A Survey,,,,10.48550/arXiv.2411.18279,http://arxiv.org/abs/2411.18279,"GUIs have long been central to human-computer interaction, providing an intuitive and visually-driven way to access and interact with digital systems. The advent of LLMs, particularly multimodal models, has ushered in a new era of GUI automation. They have demonstrated exceptional capabilities in natural language understanding, code generation, and visual processing. This has paved the way for a new generation of LLM-brained GUI agents capable of interpreting complex GUI elements and autonomously executing actions based on natural language instructions. These agents represent a paradigm shift, enabling users to perform intricate, multi-step tasks through simple conversational commands. Their applications span across web navigation, mobile app interactions, and desktop automation, offering a transformative user experience that revolutionizes how individuals interact with software. This emerging field is rapidly advancing, with significant progress in both research and industry. To provide a structured understanding of this trend, this paper presents a comprehensive survey of LLM-brained GUI agents, exploring their historical evolution, core components, and advanced techniques. We address research questions such as existing GUI agent frameworks, the collection and utilization of data for training specialized GUI agents, the development of large action models tailored for GUI tasks, and the evaluation metrics and benchmarks necessary to assess their effectiveness. Additionally, we examine emerging applications powered by these agents. Through a detailed analysis, this survey identifies key research gaps and outlines a roadmap for future advancements in the field. By consolidating foundational knowledge and state-of-the-art developments, this work aims to guide both researchers and practitioners in overcoming challenges and unlocking the full potential of LLM-brained GUI agents.",2025-05-06,2025-05-22 09:46:57,2025-05-22 09:46:57,2025-05-22 09:46:45,,,,,,,Large Language Model-Brained GUI Agents,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2411.18279 [cs],,/Users/ksoares/Zotero/storage/2DML8XI8/Zhang et al. - 2025 - Large Language Model-Brained GUI Agents A Survey.pdf; /Users/ksoares/Zotero/storage/5BTN52ZH/2411.html,,,Computer Science - Computation and Language; Computer Science - Artificial Intelligence; Computer Science - Human-Computer Interaction,,,,,,,,,,,,,,,,,,,arXiv:2411.18279,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
XNRKDN8K,preprint,2024.0,"Xia, Heming; Yang, Zhe; Dong, Qingxiu; Wang, Peiyi; Li, Yongqi; Ge, Tao; Liu, Tianyu; Li, Wenjie; Sui, Zhifang",Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding,,,,10.48550/arXiv.2401.07851,http://arxiv.org/abs/2401.07851,"To mitigate the high inference latency stemming from autoregressive decoding in Large Language Models (LLMs), Speculative Decoding has emerged as a novel decoding paradigm for LLM inference. In each decoding step, this method first drafts several future tokens efficiently and then verifies them in parallel. Unlike autoregressive decoding, Speculative Decoding facilitates the simultaneous decoding of multiple tokens per step, thereby accelerating inference. This paper presents a comprehensive overview and analysis of this promising decoding paradigm. We begin by providing a formal definition and formulation of Speculative Decoding. Then, we organize in-depth discussions on its key facets, such as drafter selection and verification strategies. Furthermore, we present a comparative analysis of leading methods under third-party testing environments. We aim for this work to serve as a catalyst for further research on Speculative Decoding, ultimately contributing to more efficient LLM inference.",2024-06-04,2025-05-22 09:46:57,2025-05-22 09:46:57,2025-05-22 09:46:51,,,,,,,Unlocking Efficiency in Large Language Model Inference,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2401.07851 [cs],,/Users/ksoares/Zotero/storage/T5QH29JM/Xia et al. - 2024 - Unlocking Efficiency in Large Language Model Inference A Comprehensive Survey of Speculative Decodi.pdf; /Users/ksoares/Zotero/storage/YWN6BSMF/2401.html,,,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,arXiv:2401.07851,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
3X4SIQUT,preprint,2024.0,"Ye, Jiexia; Zhang, Weiqi; Yi, Ke; Yu, Yongzi; Li, Ziyue; Li, Jia; Tsung, Fugee",A Survey of Time Series Foundation Models: Generalizing Time Series Representation with Large Language Model,,,,10.48550/arXiv.2405.02358,http://arxiv.org/abs/2405.02358,"Time series data are ubiquitous across various domains, making time series analysis critically important. Traditional time series models are task-specific, featuring singular functionality and limited generalization capacity. Recently, large language foundation models have unveiled their remarkable capabilities for cross-task transferability, zero-shot/few-shot learning, and decision-making explainability. This success has sparked interest in the exploration of foundation models to solve multiple time series challenges simultaneously. There are two main research lines, namely pre-training foundation models from scratch for time series and adapting large language foundation models for time series. They both contribute to the development of a unified model that is highly generalizable, versatile, and comprehensible for time series analysis. This survey offers a 3E analytical framework for comprehensive examination of related research. Specifically, we examine existing works from three dimensions, namely Effectiveness, Efficiency and Explainability. In each dimension, we focus on discussing how related works devise tailored solution by considering unique challenges in the realm of time series. Furthermore, we provide a domain taxonomy to help followers keep up with the domain-specific advancements. In addition, we introduce extensive resources to facilitate the field's development, including datasets, open-source, time series libraries. A GitHub repository is also maintained for resource updates (https://github.com/start2020/Awesome-TimeSeries-LLM-FM).",2024-05-07,2025-05-22 09:46:57,2025-05-22 09:46:57,2025-05-22 09:46:53,,,,,,,A Survey of Time Series Foundation Models,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2405.02358 [cs],,/Users/ksoares/Zotero/storage/T3ERKBMM/Ye et al. - 2024 - A Survey of Time Series Foundation Models Generalizing Time Series Representation with Large Langua.pdf; /Users/ksoares/Zotero/storage/YQS87XLW/2405.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,arXiv:2405.02358,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
NKHQGCW8,journalArticle,2025.0,"Lu, Weizheng; Zhang, Jing; Fan, Ju; Fu, Zihao; Chen, Yueguo; Du, Xiaoyong",Large language model for table processing: a survey,Frontiers of Computer Science,,"2095-2228, 2095-2236",10.1007/s11704-024-40763-6,https://link.springer.com/10.1007/s11704-024-40763-6,"Abstract             Tables, typically two-dimensional and structured to store large amounts of data, are essential in daily activities like database queries, spreadsheet manipulations, Web table question answering, and image table information extraction. Automating these table-centric tasks with Large Language Models (LLMs) or Visual Language Models (VLMs) offers significant public benefits, garnering interest from academia and industry. This survey provides a comprehensive overview of table-related tasks, examining both user scenarios and technical aspects. It covers traditional tasks like table question answering as well as emerging fields such as spreadsheet manipulation and table data analysis. We summarize the training techniques for LLMs and VLMs tailored for table processing. Additionally, we discuss prompt engineering, particularly the use of LLM-powered agents, for various table-related tasks. Finally, we highlight several challenges, including diverse user input when serving and slow thinking using chain-of-thought.",2025-02,2025-05-22 09:46:57,2025-05-22 09:46:57,2025-05-22 09:46:57,192350,,2.0,19.0,,Front. Comput. Sci.,Large language model for table processing,,,,,,,en,,,,,DOI.org (Crossref),,,,/Users/ksoares/Zotero/storage/NTKDS33F/Lu et al. - 2025 - Large language model for table processing a survey.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
7LLFJWHN,preprint,2025.0,"Luo, Junyu; Zhang, Weizhi; Yuan, Ye; Zhao, Yusheng; Yang, Junwei; Gu, Yiyang; Wu, Bohan; Chen, Binqi; Qiao, Ziyue; Long, Qingqing; Tu, Rongcheng; Luo, Xiao; Ju, Wei; Xiao, Zhiping; Wang, Yifan; Xiao, Meng; Liu, Chenwu; Yuan, Jingyang; Zhang, Shichang; Jin, Yiqiao; Zhang, Fan; Wu, Xian; Zhao, Hanqing; Tao, Dacheng; Yu, Philip S.; Zhang, Ming","Large Language Model Agent: A Survey on Methodology, Applications and Challenges",,,,10.48550/arXiv.2503.21460,http://arxiv.org/abs/2503.21460,"The era of intelligent agents is upon us, driven by revolutionary advancements in large language models. Large Language Model (LLM) agents, with goal-driven behaviors and dynamic adaptation capabilities, potentially represent a critical pathway toward artificial general intelligence. This survey systematically deconstructs LLM agent systems through a methodology-centered taxonomy, linking architectural foundations, collaboration mechanisms, and evolutionary pathways. We unify fragmented research threads by revealing fundamental connections between agent design principles and their emergent behaviors in complex environments. Our work provides a unified architectural perspective, examining how agents are constructed, how they collaborate, and how they evolve over time, while also addressing evaluation methodologies, tool applications, practical challenges, and diverse application domains. By surveying the latest developments in this rapidly evolving field, we offer researchers a structured taxonomy for understanding LLM agents and identify promising directions for future research. The collection is available at https://github.com/luo-junyu/Awesome-Agent-Papers.",2025-03-27,2025-05-22 09:46:57,2025-05-22 09:46:57,2025-05-22 09:46:57,,,,,,,Large Language Model Agent,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2503.21460 [cs],,"/Users/ksoares/Zotero/storage/XXU8KSZ4/Luo et al. - 2025 - Large Language Model Agent A Survey on Methodology, Applications and Challenges.pdf; /Users/ksoares/Zotero/storage/MY6CX7XV/2503.html",,,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,arXiv:2503.21460,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
U7S2ZSW6,preprint,2025.0,"Liu, Qidong; Zhao, Xiangyu; Wang, Yuhao; Wang, Yejing; Zhang, Zijian; Sun, Yuqi; Li, Xiang; Wang, Maolin; Jia, Pengyue; Chen, Chong; Huang, Wei; Tian, Feng",Large Language Model Enhanced Recommender Systems: A Survey,,,,10.48550/arXiv.2412.13432,http://arxiv.org/abs/2412.13432,"Large Language Model (LLM) has transformative potential in various domains, including recommender systems (RS). There have been a handful of research that focuses on empowering the RS by LLM. However, previous efforts mainly focus on LLM as RS, which may face the challenge of intolerant inference costs by LLM. Recently, the integration of LLM into RS, known as LLM-Enhanced Recommender Systems (LLMERS), has garnered significant interest due to its potential to address latency and memory constraints in real-world applications. This paper presents a comprehensive survey of the latest research efforts aimed at leveraging LLM to enhance RS capabilities. We identify a critical shift in the field with the move towards incorporating LLM into the online system, notably by avoiding their use during inference. Our survey categorizes the existing LLMERS approaches into three primary types based on the component of the RS model being augmented: Knowledge Enhancement, Interaction Enhancement, and Model Enhancement. We provide an in-depth analysis of each category, discussing the methodologies, challenges, and contributions of recent studies. Furthermore, we highlight several promising research directions that could further advance the field of LLMERS.",2025-03-10,2025-05-22 09:47:55,2025-05-22 09:47:55,2025-05-22 09:47:15,,,,,,,Large Language Model Enhanced Recommender Systems,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2412.13432 [cs],,/Users/ksoares/Zotero/storage/7IMBM6LC/Liu et al. - 2025 - Large Language Model Enhanced Recommender Systems A Survey.pdf; /Users/ksoares/Zotero/storage/HRJG48UD/2412.html,,,Computer Science - Artificial Intelligence; Computer Science - Information Retrieval,,,,,,,,,,,,,,,,,,,arXiv:2412.13432,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
NB2RXVIA,preprint,2024.0,"Jiang, Xuhui; Tian, Yuxing; Hua, Fengrui; Xu, Chengjin; Wang, Yuanzhuo; Guo, Jian",A Survey on Large Language Model Hallucination via a Creativity Perspective,,,,10.48550/arXiv.2402.06647,http://arxiv.org/abs/2402.06647,"Hallucinations in large language models (LLMs) are always seen as limitations. However, could they also be a source of creativity? This survey explores this possibility, suggesting that hallucinations may contribute to LLM application by fostering creativity. This survey begins with a review of the taxonomy of hallucinations and their negative impact on LLM reliability in critical applications. Then, through historical examples and recent relevant theories, the survey explores the potential creative benefits of hallucinations in LLMs. To elucidate the value and evaluation criteria of this connection, we delve into the definitions and assessment methods of creativity. Following the framework of divergent and convergent thinking phases, the survey systematically reviews the literature on transforming and harnessing hallucinations for creativity in LLMs. Finally, the survey discusses future research directions, emphasizing the need to further explore and refine the application of hallucinations in creative processes within LLMs.",2024-02-02,2025-05-22 09:47:55,2025-05-22 09:47:55,2025-05-22 09:47:21,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2402.06647 [cs],,/Users/ksoares/Zotero/storage/USKB4WMB/Jiang et al. - 2024 - A Survey on Large Language Model Hallucination via a Creativity Perspective.pdf; /Users/ksoares/Zotero/storage/FDVPU338/2402.html,,,Computer Science - Artificial Intelligence; Computer Science - Human-Computer Interaction,,,,,,,,,,,,,,,,,,,arXiv:2402.06647,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
V3IZ6BN2,journalArticle,2024.0,"Shao, Minghao; Basit, Abdul; Karri, Ramesh; Shafique, Muhammad","Survey of different large language model architectures: Trends, benchmarks, and challenges",IEEE Access,,,,https://ieeexplore.ieee.org/abstract/document/10720163/,"Large Language Models (LLMs) represent a class of deep learning models adept at understanding natural language and generating coherent responses to various prompts or queries. These models far exceed the complexity of conventional neural networks, often encompassing dozens of neural network layers and containing billions to trillions of parameters. They are typically trained on vast datasets, utilizing architectures based on transformer blocks. Present-day LLMs are multi-functional, capable of performing a range of tasks from text generation and language translation to question answering, as well as code generation and analysis. An advanced subset of these models, known as Multimodal Large Language Models (MLLMs), extends LLM capabilities to process and interpret multiple data modalities, including images, audio, and video. This enhancement empowers MLLMs with capabilities like video editing, image comprehension, and captioning for visual content. This survey provides a comprehensive overview of the recent advancements in LLMs. We begin by tracing the evolution of LLMs and subsequently delve into the advent and nuances of MLLMs. We analyze emerging state-of-the-art MLLMs, exploring their technical features, strengths, and limitations. Additionally, we present a comparative analysis of these models and discuss their challenges, potential limitations, and prospects for future development.",2024,2025-05-22 09:47:55,2025-05-26 13:52:15,2025-05-22 09:47:21,,,,,,,Survey of different large language model architectures,,,,,,,,,,,,Google Scholar,,Publisher: IEEE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
XLIQ9XD6,journalArticle,2025.0,"Zhu, Xi; Wang, Yu; Gao, Hang; Xu, Wujiang; Wang, Chen; Liu, Zhiwei; Wang, Kun; Jin, Mingyu; Pang, Linsey; Weng, Qingsong",Recommender systems meet large language model agents: A survey,Foundations and Trends® in Privacy and Security,,,10.1561/3300000050,https://www.nowpublishers.com/article/Details/SEC-050,"In recent years, the integration of Large Language Models (LLMs) and Recommender Systems (RS) has revolutionized the way personalized and intelligent user experiences are delivered. This survey provides an extensive review of critical challenges, current landscape, and future directions in the collaboration between LLM-based AI agents (LLM Agent) and recommender systems. We begin with an introduction to the foundational knowledge, exploring the components of LLM agents and the applications of LLMs in recommender systems. The survey then delves into the symbiotic relationship between LLM agents and recommender systems, illustrating how LLM agents enhance recommender systems and how recommender systems support better LLM agents. Specifically, we discuss the overall architectures for designing LLM agents for recommendation, encompassing profile, memory, planning, and action components, along with multi-agent collaboration. Conversely, we investigate how recommender systems contribute to LLM agents, focusing on areas such as memory recommendation, plan recommendation, tool recommendation, agent recommendation, and personalized LLMs and LLM agents. Furthermore, a critical evaluation of trustworthy AI agents and recommender systems follows, addressing key issues of safety, explainability, fairness, and privacy. Finally, we propose potential future research directions, highlighting emerging trends and opportunities in the intersection of AI agents and recommender systems. This survey concludes by summarizing the key insights of current research and outlining promising avenues for future exploration in this rapidly evolving field. A curated collection of relevant papers for this survey is available in the GitHub repository: https://github.com/agiresearch/AgentRecSys.",2025,2025-05-22 09:47:55,2025-05-26 13:51:39,2025-05-22 09:47:23,247–396,,4.0,7.0,,,Recommender systems meet large language model agents,,,,,,,,,,,,Google Scholar,,"Publisher: Now Publishers, Inc.",,/Users/ksoares/Zotero/storage/M2QLEW4R/Zhu et al. - 2025 - Recommender systems meet large language model agents A survey.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
VNSYESAE,journalArticle,2025.0,"Geren, Caleb; Board, Amanda; Dagher, Gaby G.; Andersen, Tim; Zhuang, Jun",Blockchain for Large Language Model Security and Safety: A Holistic Survey,ACM SIGKDD Explorations Newsletter,,"1931-0145, 1931-0153",10.1145/3715073.3715075,https://dl.acm.org/doi/10.1145/3715073.3715075,"With the growing development and deployment of large language models (LLMs) in both industrial and academic fields, their security and safety concerns have become increasingly critical. However, recent studies indicate that LLMs face numerous vulnerabilities, including data poisoning, prompt injections, and unauthorized data exposure, which conventional methods have struggled to address fully. In parallel, blockchain technology, known for its data immutability and decentralized structure, offers a promising foundation for safeguarding LLMs. In this survey, we aim to comprehensively assess how to leverage blockchain technology to enhance LLMs' security and safety. Besides, we propose a new taxonomy of blockchain for large language models (BC4LLMs) to systematically categorize related works in this emerging field. Our analysis includes novel frameworks and definitions to delineate security and safety in the context of BC4LLMs, highlighting potential research directions and challenges at this intersection.Through this study, we aim to stimulate targeted advancements in blockchain-integrated LLM security.",2025-01-21,2025-05-22 09:47:55,2025-05-22 09:47:55,2025-05-22 09:47:26,1-20,,2.0,26.0,,SIGKDD Explor. Newsl.,Blockchain for Large Language Model Security and Safety,,,,,,,en,,,,,DOI.org (Crossref),,,,/Users/ksoares/Zotero/storage/HT7PVPJZ/Geren et al. - 2025 - Blockchain for Large Language Model Security and Safety A Holistic Survey.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
JK7MWNVU,preprint,2024.0,"Ding, Han; Li, Yinheng; Wang, Junhao; Chen, Hang",Large Language Model Agent in Financial Trading: A Survey,,,,10.48550/arXiv.2408.06361,http://arxiv.org/abs/2408.06361,"Trading is a highly competitive task that requires a combination of strategy, knowledge, and psychological fortitude. With the recent success of large language models(LLMs), it is appealing to apply the emerging intelligence of LLM agents in this competitive arena and understanding if they can outperform professional traders. In this survey, we provide a comprehensive review of the current research on using LLMs as agents in financial trading. We summarize the common architecture used in the agent, the data inputs, and the performance of LLM trading agents in backtesting as well as the challenges presented in these research. This survey aims to provide insights into the current state of LLM-based financial trading agents and outline future research directions in this field.",2024-07-26,2025-05-22 09:47:55,2025-05-22 09:47:55,2025-05-22 09:47:33,,,,,,,Large Language Model Agent in Financial Trading,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2408.06361 [q-fin],,/Users/ksoares/Zotero/storage/EQVXLRS6/Ding et al. - 2024 - Large Language Model Agent in Financial Trading A Survey.pdf; /Users/ksoares/Zotero/storage/DRKYSJ6T/2408.html,,,Computer Science - Computation and Language; Quantitative Finance - Trading and Market Microstructure,,,,,,,,,,,,,,,,,,,arXiv:2408.06361,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
HDDTSN9S,preprint,2025.0,"Wang, Jiayimei; Ni, Tao; Lee, Wei-Bin; Zhao, Qingchuan",A Contemporary Survey of Large Language Model Assisted Program Analysis,,,,10.48550/arXiv.2502.18474,http://arxiv.org/abs/2502.18474,"The increasing complexity of software systems has driven significant advancements in program analysis, as traditional methods unable to meet the demands of modern software development. To address these limitations, deep learning techniques, particularly Large Language Models (LLMs), have gained attention due to their context-aware capabilities in code comprehension. Recognizing the potential of LLMs, researchers have extensively explored their application in program analysis since their introduction. Despite existing surveys on LLM applications in cybersecurity, comprehensive reviews specifically addressing their role in program analysis remain scarce. In this survey, we systematically review the application of LLMs in program analysis, categorizing the existing work into static analysis, dynamic analysis, and hybrid approaches. Moreover, by examining and synthesizing recent studies, we identify future directions and challenges in the field. This survey aims to demonstrate the potential of LLMs in advancing program analysis practices and offer actionable insights for security researchers seeking to enhance detection frameworks or develop domain-specific models.",2025-02-05,2025-05-22 09:47:55,2025-05-22 09:47:55,2025-05-22 09:47:35,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2502.18474 [cs],,/Users/ksoares/Zotero/storage/EIV7AU8L/Wang et al. - 2025 - A Contemporary Survey of Large Language Model Assisted Program Analysis.pdf; /Users/ksoares/Zotero/storage/ZV94LRGR/2502.html,,,Computer Science - Artificial Intelligence; Computer Science - Software Engineering,,,,,,,,,,,,,,,,,,,arXiv:2502.18474,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
FC5PR2G5,journalArticle,2025.0,"Wang, Peng; Lu, Wenpeng; Lu, Chunlin; Zhou, Ruoxi; Li, Min; Qin, Libo","Large Language Model for Medical Images: A Survey of Taxonomy, Systematic Review, and Future Trends",Big Data Mining and Analytics,,,,https://ieeexplore.ieee.org/abstract/document/10856853/,"The advent of Large Language Models (LLMs) has sparked considerable interest in the medical image domain, as they can generalize to multiple tasks and offer outstanding performance. While LLMs achieve promising results, there is currently a lack of a comprehensive summary of medical images, making it challenging for researchers to understand the progress within this domain. To fill this gap, we make the first attempt to present a comprehensive survey for LLM on medical images. In addition, to better summarize the current progress comprehensively, we further introduce a novel x-stage tuning paradigm for summarization, including zero-stage tuning, one-stage tuning, and multi-stage tuning, offering a unified perspective on LLMs for medical images. Finally, we discuss challenges and future directions in this domain, aiming to spur more breakthroughs in the future. We hope this work can pave the way for the broad application of LLMs in medical images and provide a valuable resource for this domain.",2025,2025-05-22 09:47:55,2025-05-26 13:48:19,2025-05-22 09:47:35,496–517,,2.0,8.0,,,Large Language Model for Medical Images,,,,,,,,,,,,Google Scholar,,Publisher: TUP,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
JAGBNNI2,preprint,2024.0,"Shi, Dan; Shen, Tianhao; Huang, Yufei; Li, Zhigen; Leng, Yongqi; Jin, Renren; Liu, Chuang; Wu, Xinwei; Guo, Zishan; Yu, Linhao; Shi, Ling; Jiang, Bojian; Xiong, Deyi",Large Language Model Safety: A Holistic Survey,,,,10.48550/arXiv.2412.17686,http://arxiv.org/abs/2412.17686,"The rapid development and deployment of large language models (LLMs) have introduced a new frontier in artificial intelligence, marked by unprecedented capabilities in natural language understanding and generation. However, the increasing integration of these models into critical applications raises substantial safety concerns, necessitating a thorough examination of their potential risks and associated mitigation strategies. This survey provides a comprehensive overview of the current landscape of LLM safety, covering four major categories: value misalignment, robustness to adversarial attacks, misuse, and autonomous AI risks. In addition to the comprehensive review of the mitigation methodologies and evaluation resources on these four aspects, we further explore four topics related to LLM safety: the safety implications of LLM agents, the role of interpretability in enhancing LLM safety, the technology roadmaps proposed and abided by a list of AI companies and institutes for LLM safety, and AI governance aimed at LLM safety with discussions on international cooperation, policy proposals, and prospective regulatory directions. Our findings underscore the necessity for a proactive, multifaceted approach to LLM safety, emphasizing the integration of technical solutions, ethical considerations, and robust governance frameworks. This survey is intended to serve as a foundational resource for academy researchers, industry practitioners, and policymakers, offering insights into the challenges and opportunities associated with the safe integration of LLMs into society. Ultimately, it seeks to contribute to the safe and beneficial development of LLMs, aligning with the overarching goal of harnessing AI for societal advancement and well-being. A curated list of related papers has been publicly available at https://github.com/tjunlp-lab/Awesome-LLM-Safety-Papers.",2024-12-23,2025-05-22 09:47:55,2025-05-22 09:47:55,2025-05-22 09:47:39,,,,,,,Large Language Model Safety,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2412.17686 [cs],,/Users/ksoares/Zotero/storage/CA8VZWDX/Shi et al. - 2024 - Large Language Model Safety A Holistic Survey.pdf; /Users/ksoares/Zotero/storage/GX3VYWRH/2412.html,,,Computer Science - Computation and Language; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,arXiv:2412.17686,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
PUCW88YM,preprint,2025.0,"Dobslaw, Felix; Feldt, Robert; Yoon, Juyeon; Yoo, Shin",Challenges in Testing Large Language Model Based Software: A Faceted Taxonomy,,,,10.48550/arXiv.2503.00481,http://arxiv.org/abs/2503.00481,"Large Language Models (LLMs) and Multi-Agent LLMs (MALLMs) introduce non-determinism unlike traditional or machine learning software, requiring new approaches to verifying correctness beyond simple output comparisons or statistical accuracy over test datasets. This paper presents a taxonomy for LLM test case design, informed by both the research literature, our experience, and open-source tools that represent the state of practice. We identify key variation points that impact test correctness and highlight open challenges that the research, industry, and open-source communities must address as LLMs become integral to software systems. Our taxonomy defines four facets of LLM test case design, addressing ambiguity in both inputs and outputs while establishing best practices. It distinguishes variability in goals, the system under test, and inputs, and introduces two key oracle types: atomic and aggregated. Our mapping indicates that current tools insufficiently account for these variability points, highlighting the need for closer collaboration between academia and practitioners to improve the reliability and reproducibility of LLM testing.",2025-03-01,2025-05-22 09:47:55,2025-05-22 09:47:55,2025-05-22 09:47:45,,,,,,,Challenges in Testing Large Language Model Based Software,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2503.00481 [cs],,/Users/ksoares/Zotero/storage/JISQHF3J/Dobslaw et al. - 2025 - Challenges in Testing Large Language Model Based Software A Faceted Taxonomy.pdf; /Users/ksoares/Zotero/storage/945WX37H/2503.html,,,Computer Science - Artificial Intelligence; Computer Science - Software Engineering,,,,,,,,,,,,,,,,,,,arXiv:2503.00481,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
VV77THWJ,journalArticle,2025.0,"Guo, Zhiqi; Tang, Fengxiao; Luo, Linfeng; Zhao, Ming; Kato, Nei",A Survey on Applications of Large Language Model-Driven Digital Twins for Intelligent Network Optimization,IEEE Communications Surveys & Tutorials,,,,https://ieeexplore.ieee.org/abstract/document/10994494/,"With the widespread application of digital twin (DT) technology in network optimization and intelligent management, its integration with large language models (LLMs) presents immense potential. LLMs excel in natural language processing, multimodal analysis, and real-time optimization, enabling innovative solutions for intelligent monitoring, resource allocation, and decision-making in complex network environments. This paper systematically reviews the development of DTs and LLMs, elaborates on their core principles and application scenarios, and examines the capabilities of LLM-driven DTs in key network optimization tasks, including traffic prediction, fault diagnosis, resource allocation, and multi-objective optimization. By leveraging real-time data from DTs, LLMs can dynamically generate optimization strategies, enabling precise monitoring and intelligent tuning. Furthermore, this paper explores the potential of integrating LLMs and DTs to address complex challenges such as data quality, latency sensitivity, and energy consumption demands, while summarizing existing technical bottlenecks. Finally, the paper proposes several potential research directions to address these challenges, offering a comprehensive perspective for advancing the efficiency and automation of next-generation intelligent networks.",2025,2025-05-22 09:47:55,2025-05-26 13:38:53,2025-05-22 09:47:55,,,,,,,,,,,,,,,,,,,Google Scholar,,Publisher: IEEE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
N3MPYQP7,conferencePaper,2025.0,"Singh, Aditi; Shetty, Akash; Ehtesham, Abul; Kumar, Saket; Khoei, Tala Talaei","A Survey of Large Language Model-Based Generative AI for Text-to-SQL: Benchmarks, Applications, Use Cases, and Challenges",2025 IEEE 15th Annual Computing and Communication Workshop and Conference (CCWC),,,,https://ieeexplore.ieee.org/abstract/document/10903689/,"Text-to-SQL systems facilitate smooth interaction with databases by translating natural language queries into Structured Query Language (SQL), bridging the gap between non-technical users and complex database management systems. This survey provides a comprehensive overview of the evolution of AI-driven text-to-SQL systems, highlighting their foundational components, advancements in large language model (LLM) architectures, and the critical role of datasets such as Spider, WikiSQL, and CoSQL in driving progress. We examine the applications of text-to-SQL in domains like healthcare, education, and finance, emphasizing their transformative potential for improving data accessibility. Additionally, we analyze persistent challenges, including domain generalization, query optimization, support for multi-turn conversational interactions, and the limited availability of datasets tailored for NoSQL databases and dynamic real-world scenarios. To address these challenges, we outline future research directions, such as extending text-to-SQL capabilities to support NoSQL databases, designing datasets for dynamic multi-turn interactions, and optimizing systems for real-world scalability and robustness. By surveying current advancements and identifying key gaps, this paper aims to guide the next generation of research and applications in LLM-based text-to-SQL systems.",2025,2025-05-22 09:49:53,2025-05-26 13:38:26,2025-05-22 09:49:20,00015–00021,,,,,,A Survey of Large Language Model-Based Generative AI for Text-to-SQL,,,,,IEEE,,,,,,,Google Scholar,,,,"/Users/ksoares/Zotero/storage/Z8GDVHCW/Singh et al. - 2025 - A Survey of Large Language Model-Based Generative AI for Text-to-SQL Benchmarks, Applications, Use.pdf",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
BXAGH7ME,preprint,2024.0,"Sun, Maojun; Han, Ruijian; Jiang, Binyan; Qi, Houduo; Sun, Defeng; Yuan, Yancheng; Huang, Jian",A Survey on Large Language Model-based Agents for Statistics and Data Science,,,,10.48550/arXiv.2412.14222,http://arxiv.org/abs/2412.14222,"In recent years, data science agents powered by Large Language Models (LLMs), known as ""data agents,"" have shown significant potential to transform the traditional data analysis paradigm. This survey provides an overview of the evolution, capabilities, and applications of LLM-based data agents, highlighting their role in simplifying complex data tasks and lowering the entry barrier for users without related expertise. We explore current trends in the design of LLM-based frameworks, detailing essential features such as planning, reasoning, reflection, multi-agent collaboration, user interface, knowledge integration, and system design, which enable agents to address data-centric problems with minimal human intervention. Furthermore, we analyze several case studies to demonstrate the practical applications of various data agents in real-world scenarios. Finally, we identify key challenges and propose future research directions to advance the development of data agents into intelligent statistical analysis software.",2024-12-18,2025-05-22 09:49:53,2025-05-22 09:49:53,2025-05-22 09:49:27,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2412.14222 [cs],,/Users/ksoares/Zotero/storage/UY6YMTMT/Sun et al. - 2024 - A Survey on Large Language Model-based Agents for Statistics and Data Science.pdf; /Users/ksoares/Zotero/storage/N4QAF23S/2412.html,,,Computer Science - Computation and Language; Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Other Statistics,,,,,,,,,,,,,,,,,,,arXiv:2412.14222,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
YJUIXT2K,journalArticle,2024.0,"Zhu, Xiaohu; Li, Qian; Cui, Lizhen; Liu, Yongkang",Large language model enhanced text-to-sql generation: A survey,arXiv preprint arXiv:2410.06011,,,,https://arxiv.org/abs/2410.06011,"Abstract—Text-to-SQL translates natural language queries into Structured Query Language (SQL) commands, enabling users to interact with databases using natural language. Essentially, the text-to-SQL task is a text generation task and its development primarily dependent on changes in language models. Especially with the rapid development of Large Language Models (LLMs), the pattern of text-to-SQL has undergone significant changes. Existing survey work mainly focuses on rule-based and neural-based approaches, still lacking a survey of Text-to-SQL with LLMs. In this paper, we survey the large language model enhanced text-to-SQL generations, classifying them into prompt engineering, fine-tuning, pre-trained and Agent groups according to training strategies. And we also summarize datasets and evaluation metrics comprehensively. This survey could help people better understand the pattern, research status, and challenges of LLM-based text-to-SQL generations.",2024,2025-05-22 09:49:53,2025-05-26 13:48:09,2025-05-22 09:49:29,,,,,,,Large language model enhanced text-to-sql generation,,,,,,,,,,,,Google Scholar,,,,/Users/ksoares/Zotero/storage/SR2HQ5S8/Zhu et al. - 2024 - Large language model enhanced text-to-sql generation A survey.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
4SPUM37I,preprint,2024.0,"Zeng, Pai; Ning, Zhenyu; Zhao, Jieru; Cui, Weihao; Xu, Mengwei; Guo, Liwei; Chen, Xusheng; Shan, Yizhou",The CAP Principle for LLM Serving: A Survey of Long-Context Large Language Model Serving,,,,10.48550/arXiv.2405.11299,http://arxiv.org/abs/2405.11299,"We survey the large language model (LLM) serving area to understand the intricate dynamics between cost-efficiency and accuracy, which is magnified by the growing need for longer contextual understanding when deploying models at a massive scale. Our findings reveal that works in this space optimize along three distinct but conflicting goals: improving serving context length (C), improving serving accuracy (A), and improving serving performance (P). Drawing inspiration from the CAP theorem in databases, we propose a CAP principle for LLM serving, which suggests that any optimization can improve at most two of these three goals simultaneously. Our survey categorizes existing works within this framework. We find the definition and continuity of user-perceived measurement metrics are crucial in determining whether a goal has been met, akin to prior CAP databases in the wild. We recognize the CAP principle for LLM serving as a guiding principle, rather than a formal theorem, to inform designers of the inherent and dynamic trade-offs in serving models. As serving accuracy and performance have been extensively studied, this survey focuses on works that extend serving context length and address the resulting challenges.",2024-05-27,2025-05-22 09:49:53,2025-05-22 09:49:53,2025-05-22 09:49:35,,,,,,,The CAP Principle for LLM Serving,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2405.11299 [cs],,/Users/ksoares/Zotero/storage/QQYM3L6L/Zeng et al. - 2024 - The CAP Principle for LLM Serving A Survey of Long-Context Large Language Model Serving.pdf; /Users/ksoares/Zotero/storage/C9PEJMN4/2405.html,,,Computer Science - Machine Learning; Computer Science - Databases,,,,,,,,,,,,,,,,,,,arXiv:2405.11299,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
9Z7TYLAF,journalArticle,2024.0,"Lou, Renze; Zhang, Kai; Yin, Wenpeng",Large language model instruction following: A survey of progresses and challenges,Computational Linguistics,,,,https://direct.mit.edu/coli/article-pdf/doi/10.1162/coli_a_00523/2464827/coli_a_00523.pdf,"Task semantics can be expressed by a set of input-output examples or a piece of textual instruction. Conventional machine learning approaches for natural language processing (NLP) mainly rely on the availability of large-scale sets of task-specific examples. Two issues arise: First, collecting task-specific labeled examples does not apply to scenarios where tasks may be too complicated or costly to annotate, or the system is required to handle a new task immediately; second, this is not user-friendly since end-users are probably more willing to provide task description rather than a set of examples before using the system. Therefore, the community is paying increasing interest in a new supervision-seeking paradigm for NLP: learning to follow task instructions, that is, instruction following. Despite its impressive progress, there are some unsolved research equations that the community struggles with. This survey tries to summarize and provide insights into the current research on instruction following, particularly, by answering the following questions: (i) What is task instruction, and what instruction types exist? (ii) How should we model instructions? (iii) What are popular instruction following datasets and evaluation metrics? (iv) What factors influence and explain the instructions performance? (v) What challenges remain in instruction following? To our knowledge, this is the first comprehensive survey about instruction following.",2024,2025-05-22 09:49:53,2025-05-26 13:49:08,2025-05-22 09:49:35,1053–1095,,3.0,50.0,,,Large language model instruction following,,,,,,,,,,,,Google Scholar,,"Publisher: MIT Press 255 Main Street, 9th Floor, Cambridge, Massachusetts 02142, USA …",,/Users/ksoares/Zotero/storage/BWH4CPAZ/Lou et al. - 2024 - Large language model instruction following A survey of progresses and challenges.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
J3G763II,preprint,2025.0,"Zou, Henry Peng; Huang, Wei-Chieh; Wu, Yaozu; Chen, Yankai; Miao, Chunyu; Nguyen, Hoang; Zhou, Yue; Zhang, Weizhi; Fang, Liancheng; He, Langzhou; Li, Yangning; Li, Dongyuan; Jiang, Renhe; Liu, Xue; Yu, Philip S.",A Survey on Large Language Model based Human-Agent Systems,,,,10.48550/arXiv.2505.00753,http://arxiv.org/abs/2505.00753,"Recent advances in large language models (LLMs) have sparked growing interest in building fully autonomous agents. However, fully autonomous LLM-based agents still face significant challenges, including limited reliability due to hallucinations, difficulty in handling complex tasks, and substantial safety and ethical risks, all of which limit their feasibility and trustworthiness in real-world applications. To overcome these limitations, LLM-based human-agent systems (LLM-HAS) incorporate human-provided information, feedback, or control into the agent system to enhance system performance, reliability and safety. This paper provides the first comprehensive and structured survey of LLM-HAS. It clarifies fundamental concepts, systematically presents core components shaping these systems, including environment & profiling, human feedback, interaction types, orchestration and communication, explores emerging applications, and discusses unique challenges and opportunities. By consolidating current knowledge and offering a structured overview, we aim to foster further research and innovation in this rapidly evolving interdisciplinary field. Paper lists and resources are available at https://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-Systems.",2025-05-20,2025-05-22 09:49:53,2025-05-22 09:49:53,2025-05-22 09:49:39,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2505.00753 [cs],,/Users/ksoares/Zotero/storage/G2SMP64U/Zou et al. - 2025 - A Survey on Large Language Model based Human-Agent Systems.pdf; /Users/ksoares/Zotero/storage/QC88KGL6/2505.html,,,Computer Science - Computation and Language; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,arXiv:2505.00753,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
B9CYWRK7,journalArticle,2025.0,"Scherbakov, Dmitry; Hubig, Nina; Jansari, Vinita; Bakumenko, Alexander; Lenert, Leslie A.",The emergence of large language models as tools in literature reviews: a large language model-assisted systematic review,Journal of the American Medical Informatics Association,,,,https://academic.oup.com/jamia/advance-article-abstract/doi/10.1093/jamia/ocaf063/8126534,"Objectives: This study aims to summarize the usage of large language models (LLMs) in the process of creating a scientific review by looking at the methodological papers that describe the use of LLMs in review automation and the review papers that mention they were made with the support of LLMs.  Materials and Methods: The search was conducted in June 2024 in PubMed, Scopus, Dimensions, and Google Scholar by human reviewers. Screening and extraction process took place in Covidence with the help of LLM add-on based on the OpenAI GPT-4o model. ChatGPT and Scite.ai were used in cleaning the data, generating the code for figures, and drafting the manuscript.  Results: Of the 3788 articles retrieved, 172 studies were deemed eligible for the final review. ChatGPT and GPT-based LLM emerged as the most dominant architecture for review automation (n 1⁄4 126, 73.2%). A significant number of review automation projects were found, but only a limited number of papers (n 1⁄4 26, 15.1%) were actual reviews that acknowledged LLM usage. Most citations focused on the automation of a particular stage of review, such as Searching for publications (n 1⁄4 60, 34.9%) and Data extraction (n 1⁄4 54, 31.4%). When comparing the pooled performance of GPT-based and BERT-based models, the former was better in data extraction with a mean precision of 83.0% (SD 1⁄4 10.4) and a recall of 86.0% (SD 1⁄4 9.8).  Discussion and Conclusion: Our LLM-assisted systematic review revealed a significant number of research projects related to review automation using LLMs. Despite limitations, such as lower accuracy of extraction for numeric data, we anticipate that LLMs will soon change the way scientific reviews are conducted.",2025,2025-05-22 09:49:53,2025-05-26 13:53:02,2025-05-22 09:49:41,ocaf063,,,,,,The emergence of large language models as tools in literature reviews,,,,,,,,,,,,Google Scholar,,Publisher: Oxford University Press,,/Users/ksoares/Zotero/storage/4R5AJY44/Scherbakov et al. - 2025 - The emergence of large language models as tools in literature reviews a large language model-assist.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
BTFLSBBC,journalArticle,2025.0,"Sargeant, Holli; Izzidien, Ahmed; Steffek, Felix",Topic Classification of Case Law Using a Large Language Model and a New Taxonomy for UK Law: AI Insights into Summary Judgment,Artificial Intelligence and Law,,"0924-8463, 1572-8382",10.1007/s10506-025-09434-0,http://arxiv.org/abs/2405.12910,"This paper addresses a critical gap in legal analytics by developing and applying a novel taxonomy for topic classification of summary judgment cases in the United Kingdom. Using a curated dataset of summary judgment cases, we use the Large Language Model Claude 3 Opus to explore functional topics and trends. We find that Claude 3 Opus correctly classified the topic with an accuracy of 87.13% and an F1 score of 0.87. The analysis reveals distinct patterns in the application of summary judgments across various legal domains. As case law in the United Kingdom is not originally labelled with keywords or a topic filtering option, the findings not only refine our understanding of the thematic underpinnings of summary judgments but also illustrate the potential of combining traditional and AI-driven approaches in legal classification. Therefore, this paper provides a new and general taxonomy for UK law. The implications of this work serve as a foundation for further research and policy discussions in the field of judicial administration and computational legal research methodologies.",2025-02-25,2025-05-22 09:49:53,2025-05-22 09:49:53,2025-05-22 09:49:46,,,,,,Artif Intell Law,Topic Classification of Case Law Using a Large Language Model and a New Taxonomy for UK Law,,,,,,,,,,,,arXiv.org,,arXiv:2405.12910 [cs],,/Users/ksoares/Zotero/storage/VSMD3V3I/Sargeant et al. - 2025 - Topic Classification of Case Law Using a Large Language Model and a New Taxonomy for UK Law AI Insi.pdf; /Users/ksoares/Zotero/storage/W7Q45VWZ/2405.html,,,Computer Science - Computation and Language; Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
ZX7X7VJL,preprint,2025.0,"Chen, Zihan; Wang, Song; Tan, Zhen; Fu, Xingbo; Lei, Zhenyu; Wang, Peng; Liu, Huan; Shen, Cong; Li, Jundong",A Survey of Scaling in Large Language Model Reasoning,,,,10.48550/arXiv.2504.02181,http://arxiv.org/abs/2504.02181,"The rapid advancements in large Language models (LLMs) have significantly enhanced their reasoning capabilities, driven by various strategies such as multi-agent collaboration. However, unlike the well-established performance improvements achieved through scaling data and model size, the scaling of reasoning in LLMs is more complex and can even negatively impact reasoning performance, introducing new challenges in model alignment and robustness. In this survey, we provide a comprehensive examination of scaling in LLM reasoning, categorizing it into multiple dimensions and analyzing how and to what extent different scaling strategies contribute to improving reasoning capabilities. We begin by exploring scaling in input size, which enables LLMs to process and utilize more extensive context for improved reasoning. Next, we analyze scaling in reasoning steps that improves multi-step inference and logical consistency. We then examine scaling in reasoning rounds, where iterative interactions refine reasoning outcomes. Furthermore, we discuss scaling in training-enabled reasoning, focusing on optimization through iterative model improvement. Finally, we review applications of scaling across domains and outline future directions for further advancing LLM reasoning. By synthesizing these diverse perspectives, this survey aims to provide insights into how scaling strategies fundamentally enhance the reasoning capabilities of LLMs and further guide the development of next-generation AI systems.",2025-04-02,2025-05-22 09:49:53,2025-05-22 09:49:53,2025-05-22 09:49:47,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2504.02181 [cs],,/Users/ksoares/Zotero/storage/HH767B39/Chen et al. - 2025 - A Survey of Scaling in Large Language Model Reasoning.pdf; /Users/ksoares/Zotero/storage/IHWWPXV9/2504.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,arXiv:2504.02181,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
TRRHYYDA,journalArticle,2025.0,"Ozkan, Ecem; Tekin, Aysun; Ozkan, Mahmut Can; Cabrera, Daniel; Niven, Alexander; Dong, Yue",Global Health care Professionals’ Perceptions of Large Language Model Use In Practice: Cross-Sectional Survey Study,JMIR Medical Education,,,,https://mededu.jmir.org/2025/1/e58801,"Abstract Background: ChatGPT is a large language model-based chatbot developed by OpenAI. ChatGPT has many potential applications to health care, including enhanced diagnostic accuracy and efficiency, improved treatment planning, and better patient outcomes. However, health care professionals’ perceptions of ChatGPT and similar artificial intelligence tools are not well known. Understanding these attitudes is important to inform the best approaches to exploring their use in medicine. Objective: Our aim was to evaluate the health care professionals’ awareness and perceptions regarding potential applications of ChatGPT in the medical field, including potential benefits and challenges of adoption. Methods: We designed a 33-question online survey that was distributed among health care professionals via targeted emails and professional Twitter and LinkedIn accounts. The survey included a range of questions to define respondents’ demographic characteristics, familiarity with ChatGPT, perceptions of this tool’s usefulness and reliability, and opinions on its potential to improve patient care, research, and education efforts. Results: One hundred and fifteen health care professionals from 21 countries responded to the survey, including physicians, nurses, researchers, and educators. Of these, 101 (87.8%) had heard of ChatGPT, mainly from peers, social media, and news, and 77 (76.2%) had used ChatGPT at least once. Participants found ChatGPT to be helpful for writing manuscripts (n=31, 45.6%), emails (n=25, 36.8%), and grants (n=12, 17.6%); accessing the latest research and evidence-based guidelines (n=21, 30.9%); providing suggestions on diagnosis or treatment (n=15, 22.1%); and improving patient communication (n=12, 17.6%). Respondents also felt that the ability of ChatGPT to access and summarize research articles (n=22, 46.8%), provide quick answers to clinical questions (n=15, 31.9%), and generate patient education materials (n=10, 21.3%) was helpful. However, there are concerns regarding the use of ChatGPT, for example, the accuracy of responses (n=14, 29.8%), limited applicability in specific practices (n=18, 38.3%), and legal and ethical considerations (n=6, 12.8%), mainly related to plagiarism or copyright violations. Participants stated that safety protocols such as data encryption (n=63, 62.4%) and access control (n=52, 51.5%) could assist in ensuring patient privacy and data security. Conclusions: Our findings show that ChatGPT use is widespread among health care professionals in daily clinical, research, and educational activities. The majority of our participants found ChatGPT to be useful; however, there are concerns about patient privacy, data security, and its legal and ethical issues as well as the accuracy of its information. Further studies are required to understand the impact of ChatGPT and other large language models on clinical, educational, and research outcomes, and the concerns regarding its use must be addressed systematically and through appropriate methods.",2025,2025-05-22 09:49:53,2025-05-26 13:44:27,2025-05-22 09:49:49,e58801,,,11.0,,,Global Health care Professionals’ Perceptions of Large Language Model Use In Practice,,,,,,,,,,,,Google Scholar,,"Publisher: JMIR Publications Toronto, Canada",,/Users/ksoares/Zotero/storage/PFH4BJU3/e58801.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
9CWUUFPR,preprint,2024.0,"Feng, Xiachong; Dou, Longxu; Li, Ella; Wang, Qinghao; Wang, Haochuan; Guo, Yu; Ma, Chang; Kong, Lingpeng",A Survey on Large Language Model-Based Social Agents in Game-Theoretic Scenarios,,,,10.48550/arXiv.2412.03920,http://arxiv.org/abs/2412.03920,"Game-theoretic scenarios have become pivotal in evaluating the social intelligence of Large Language Model (LLM)-based social agents. While numerous studies have explored these agents in such settings, there is a lack of a comprehensive survey summarizing the current progress. To address this gap, we systematically review existing research on LLM-based social agents within game-theoretic scenarios. Our survey organizes the findings into three core components: Game Framework, Social Agent, and Evaluation Protocol. The game framework encompasses diverse game scenarios, ranging from choice-focusing to communication-focusing games. The social agent part explores agents' preferences, beliefs, and reasoning abilities. The evaluation protocol covers both game-agnostic and game-specific metrics for assessing agent performance. By reflecting on the current research and identifying future research directions, this survey provides insights to advance the development and evaluation of social agents in game-theoretic scenarios.",2024-12-05,2025-05-22 09:49:53,2025-05-22 09:49:53,2025-05-22 09:49:51,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2412.03920 [cs],,/Users/ksoares/Zotero/storage/CKM58788/Feng et al. - 2024 - A Survey on Large Language Model-Based Social Agents in Game-Theoretic Scenarios.pdf; /Users/ksoares/Zotero/storage/TD7U2TIP/2412.html,,,Computer Science - Computation and Language; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,arXiv:2412.03920,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
D3KZJ4NC,journalArticle,2025.0,"Fu, Yu; Wang, Mingguo; Wang, Chengbin; Dong, Shuaixian; Chen, Jianguo; Wang, Jiyuan; Yu, Hongping; Huang, Jing; Chang, Liheng; Wang, Bo",GeoMinLM: A large language model in geology and mineral survey in Yunnan Province,Ore Geology Reviews,,,,https://www.sciencedirect.com/science/article/pii/S0169136825001982,"In recent years, the development of artificial intelligence and big data technologies has led to the advancement of tools and solutions for transforming the geological and mineral survey paradigm, which requires a large amount of geological knowledge in a complex and arduous working environment. The large language model (LLM) has a significant advantage in answering generative intelligent questions. However, LLMs for general fields have limitations in answering professional questions in a vertical domain like geology. To overcome this challenge, we proposed and developed GeoMinLM, an LLM for geological and mineral exploration scenarios in Yunnan Province, and explored its applications in intelligent Q&A. Leveraging a proprietary dataset of 5.16 million words in geology and mineral exploration, we trained GeoMinLM based on Baichuan-2, achieving superior performance through fine-tuning and hyperparameter optimization. By integrating expert knowledge via a knowledge graph, we significantly reduced hallucinations and enhanced professionalism. This study proves that GeoMinLM is helpful for accurate information retrieval and knowledge dissemination, thereby supporting the intelligent advancement of geological and mineral fields.",2025,2025-05-22 09:49:53,2025-05-26 13:44:11,2025-05-22 09:49:53,106638,,,,,,GeoMinLM,,,,,,,,,,,,Google Scholar,,Publisher: Elsevier,,/Users/ksoares/Zotero/storage/86LHETWG/S0169136825001982.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
7QMDN745,journalArticle,2025.0,"Sargeant, Holli; Izzidien, Ahmed; Steffek, Felix",Topic classification of case law using a large language model and a new taxonomy for UK law: AI insights into summary judgment,Artificial Intelligence and Law,,"0924-8463, 1572-8382",10.1007/s10506-025-09434-0,https://link.springer.com/10.1007/s10506-025-09434-0,"Abstract             This paper addresses a critical gap in legal analytics by developing and applying a novel taxonomy for topic classification of summary judgment cases in the United Kingdom. Using a curated dataset of summary judgment cases, we use the Large Language Model Claude 3 Opus to explore functional topics and trends. We find that Claude 3 Opus correctly classified the topic with an accuracy of 87.13% and an F1 score of 0.87. The analysis reveals distinct patterns in the application of summary judgments across various legal domains. As case law in the United Kingdom is not originally labelled with keywords or a topic filtering option, the findings not only refine our understanding of the thematic underpinnings of summary judgments but also illustrate the potential of combining traditional and AI-driven approaches in legal classification. Therefore, this paper provides a new and general taxonomy for UK law. The implications of this work serve as a foundation for further research and policy discussions in the field of judicial administration and computational legal research methodologies.",2025-02-25,2025-05-22 09:51:42,2025-05-22 09:51:42,2025-05-22 09:51:09,,,,,,Artif Intell Law,Topic classification of case law using a large language model and a new taxonomy for UK law,,,,,,,en,,,,,DOI.org (Crossref),,,,/Users/ksoares/Zotero/storage/ERPWZL9V/Sargeant et al. - 2025 - Topic classification of case law using a large language model and a new taxonomy for UK law AI insi.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
MZ2LQIVV,journalArticle,2025.0,"Du, Shangheng; Zhao, Jiabao; Shi, Jinxin; Xie, Zhentao; Jiang, Xin; Bai, Yanhong; He, Liang",A Survey on the Optimization of Large Language Model-based Agents,arXiv preprint arXiv:2503.12434,,,,https://arxiv.org/abs/2503.12434,"With the rapid development of Large Language Models (LLMs), LLM-based agents have been widely adopted in various fields, becoming essential for autonomous decision-making and interactive tasks. However, current work typically relies on prompt design or fine-tuning strategies applied to vanilla LLMs, which often leads to limited effectiveness or suboptimal performance in complex agent-related environments. Although LLM optimization techniques can improve model performance across many general tasks, they lack specialized optimization towards critical agent functionalities such as long-term planning, dynamic environmental interaction, and complex decision-making. Although numerous recent studies have explored various strategies to optimize LLM-based agents for complex agent tasks, a systematic review summarizing and comparing these methods from a holistic perspective is still lacking. In this survey, we provide a comprehensive review of LLM-based agent optimization approaches, categorizing them into parameter-driven and parameter-free methods. We first focus on parameter-driven optimization, covering fine-tuning-based optimization, reinforcement learning-based optimization, and hybrid strategies, analyzing key aspects such as trajectory data construction, fine-tuning techniques, reward function design, and optimization algorithms. Additionally, we briefly discuss parameter-free strategies that optimize agent behavior through prompt engineering and external knowledge retrieval. Finally, we summarize the datasets and benchmarks used for evaluation and tuning, review key applications of LLM-based agents, and discuss major challenges and promising future directions. Our repository for related references is available at this https URL.",2025,2025-05-22 09:51:42,2025-05-26 13:40:50,2025-05-22 09:51:12,,,,,,,,,,,,,,,,,,,Google Scholar,,,,/Users/ksoares/Zotero/storage/U2EX32IQ/Du et al. - 2025 - A Survey on the Optimization of Large Language Model-based Agents.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
NGNYX4R9,journalArticle,2025.0,"Li, Ruimiao; Li, Manli; Qiao, Weifeng",Engineering Students’ Use of Large Language Model Tools: An Empirical Study Based on a Survey of Students from 12 Universities,Education Sciences,,,,https://www.mdpi.com/2227-7102/15/3/280,"Large language model (LLM) tools, such as ChatGPT, are rapidly transforming engineering education by enhancing tasks like information retrieval, coding, and writing refinement, which are critical to the problem-solving and technical focus of engineering disciplines. This study investigates how engineering students use LLM tools and the challenges they face, offering insights into the adoption of AI technologies in academic settings. A survey of 539 engineering students from 12 leading Chinese universities, using the UTAUT framework, examines factors such as technological expectations, environmental support, and personal characteristics. The key findings include the following: (1) Over 40% of engineering students use LLM tools, with 18.8% regarding them as indispensable. (2) Trust in AI-generated content remains a central challenge, as students must critically evaluate its accuracy and reliability. (3) Environmental support significantly affects usage, with notable regional disparities, particularly between eastern and other regions in China. (4) A persistent digital divide, influenced by gender, academic level, and socioeconomic background, affects the depth and effectiveness of tool use. These results underscore the need for targeted support to address regional and demographic disparities and optimize LLM tool integration in engineering education.",2025,2025-05-22 09:51:42,2025-05-26 13:42:37,2025-05-22 09:51:24,280,,3.0,15.0,,,Engineering Students’ Use of Large Language Model Tools,,,,,,,,,,,,Google Scholar,,Publisher: MDPI,,/Users/ksoares/Zotero/storage/Q8P7MZ4A/280.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
BDYQ33AU,preprint,2025.0,"Varangot-Reille, Clovis; Bouvard, Christophe; Gourru, Antoine; Ciancone, Mathieu; Schaeffer, Marion; Jacquenet, François",Doing More with Less -- Implementing Routing Strategies in Large Language Model-Based Systems: An Extended Survey,,,,10.48550/arXiv.2502.00409,http://arxiv.org/abs/2502.00409,"Large Language Models (LLM)-based systems, i.e. interconnected elements that include an LLM as a central component (e.g., conversational agents), are typically monolithic static architectures that rely on a single LLM for all user queries. However, they often require different preprocessing strategies, levels of reasoning, or knowledge. Generalist LLMs (e.g. GPT-4) trained on very large multi-topic corpora can perform well in a variety of tasks. They require significant financial, energy, and hardware resources that may not be justified for basic tasks. This implies potentially investing in unnecessary costs for a given query. To overcome this problem, a routing mechanism routes user queries to the most suitable components, such as smaller LLMs or experts in specific topics. This approach may improve response quality while minimising costs. Routing can be expanded to other components of the conversational agent architecture, such as the selection of optimal embedding strategies. This paper explores key considerations for integrating routing into LLM-based systems, focusing on resource management, cost definition, and strategy selection. Our main contributions include a formalisation of the problem, a novel taxonomy of existing approaches emphasising relevance and resource efficiency, and a comparative analysis of these strategies in relation to industry practices. Finally, we identify critical challenges and directions for future research.",2025-02-04,2025-05-22 09:51:42,2025-05-22 09:51:42,2025-05-22 09:51:28,,,,,,,Doing More with Less -- Implementing Routing Strategies in Large Language Model-Based Systems,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2502.00409 [cs],,/Users/ksoares/Zotero/storage/HPPYS5E2/Varangot-Reille et al. - 2025 - Doing More with Less -- Implementing Routing Strategies in Large Language Model-Based Systems An Ex.pdf; /Users/ksoares/Zotero/storage/AWBKPXB3/2502.html,,,Computer Science - Computation and Language; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,arXiv:2502.00409,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
PRMG6724,journalArticle,2025.0,"Salehin, Imrus; Sajib, Md Tomal Ahmed; Badhon, Nazmul Huda; Rifat, Md Sakibul Hassan; Amin, Nazrul; Moon, Nazmun Nessa","Systematic Literature Review of LLM-Large Language Model in Medical: Digital Health, Technology and Applications",,,,,https://www.authorea.com/doi/full/10.22541/au.174587258.81848862,"\papertype Original Article Large language models (LLMs), like the GPT series, have recently emerged as transformative tools in the medical field due to their human-like language generation and understanding. This systematic review examines the evolution, applications, and challenges of medical LLMs in digital health and clinical technology. A structured search was conducted across ScienceDirect, PubMed, Scopus, and manual sources from 2007 to 2024, following PRISMA 2020 guidelines. After applying inclusion and exclusion criteria, 179 studies were selected from an initial pool of 698 papers. Among the 30 papers reviewed, most research centered on GPT-based models, with over 81% demonstrating strong performance in language generation, diagnostic assistance, and clinical documentation, based on automated metrics and human feedback. Notably, some models achieved up to 90% satisfaction from healthcare professionals. The findings reveal LLMs’ potential to enhance patient interaction, decision support, and overall healthcare efficiency. This review contributes by synthesizing key advancements, assessing model performance, and outlining ethical challenges such as trust, privacy, and safe deployment. It offers novel insights for researchers and practitioners seeking to adopt or improve LLM integration in healthcare. Future directions include improving transparency, developing domain-specific models, and establishing regulatory frameworks for responsible use.",2025,2025-05-22 09:51:42,2025-05-26 13:52:36,2025-05-22 09:51:28,,,,,,,Systematic Literature Review of LLM-Large Language Model in Medical,,,,,,,,,,,,Google Scholar,,,,"/Users/ksoares/Zotero/storage/IFNZ7HJF/Salehin et al. - 2025 - Systematic Literature Review of LLM-Large Language Model in Medical Digital Health, Technology and.pdf",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
2E2NUEVR,conferencePaper,2024.0,"Lu, Yuting; Sun, Chao; Yan, Yuchao; Zhu, Hegong; Song, Dongdong; Peng, Qing; Yu, Li; Wang, Xiaozheng; Jiang, Jian; Ye, Xiaolong",A Comprehensive Survey of Datasets for Large Language Model Evaluation,2024 5th Information Communication Technologies Conference (ICTC),,,,https://ieeexplore.ieee.org/abstract/document/10601918/,"Natural Language Processing is an important branch of Artificial Intelligence. In the past few years, we have witnessed the remarkable advancement of large language models, however, how to evaluate them in a comprehensive way has become an urgent problem to be solved. Datasets can help evaluate and compare their performance and clarify their weaknesses. In order to guide the subsequent research work and promote the technological progress in the field, this paper collects 147 popular evaluation datasets, and proposes a new classification method to categorize them into six categories according to the evaluation capabilities. In addition, we organize several common evaluation metrics and usage scenarios. We compile the list of datasets and the main features (introduction, samples, metrics, links, etc.) into a document, which is consistently maintain available online at: https://github.com/lyt719/LLM-evaluation-datasets.",2024,2025-05-22 09:51:42,2025-05-26 13:37:01,2025-05-22 09:51:34,330–336,,,,,,,,,,,IEEE,,,,,,,Google Scholar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
MG8A832A,conferencePaper,2025.0,"Huber, Thomas; Niklaus, Christina",LLMs meet Bloom’s Taxonomy: A Cognitive View on Large Language Model Evaluations,Proceedings of the 31st International Conference on Computational Linguistics,,,,https://aclanthology.org/2025.coling-main.350/,"Current evaluation approaches for Large Language Models (LLMs) lack a structured approach that reflects the underlying cognitive abilities required for solving the tasks. This hinders a thorough understanding of the current level of LLM capabilities. For instance, it is widely accepted that LLMs perform well in terms of grammar, but it is unclear in what specific cognitive areas they excel or struggle in. This paper introduces a novel perspective on the evaluation of LLMs that leverages a hierarchical classification of tasks. Specifically, we explore the most widely used benchmarks for LLMs to systematically identify how well these existing evaluation methods cover the levels of Bloom’s Taxonomy, a hierarchical framework for categorizing cognitive skills. This comprehensive analysis allows us to identify strengths and weaknesses in current LLM assessment strategies in terms of cognitive abilities and suggest directions for both future benchmark development as well as highlight potential avenues for LLM research. Our findings reveal that LLMs generally perform better on the lower end of Bloom’s Taxonomy. Additionally, we find that there are significant gaps in the coverage of cognitive skills in the most commonly used benchmarks.",2025,2025-05-22 09:51:42,2025-05-26 13:50:58,2025-05-22 09:51:36,5211–5246,,,,,,LLMs meet Bloom’s Taxonomy,,,,,,,,,,,,Google Scholar,,,,/Users/ksoares/Zotero/storage/IATLAGH4/Huber et Niklaus - 2025 - LLMs meet Bloom’s Taxonomy A Cognitive View on Large Language Model Evaluations.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
F6SKJTWE,journalArticle,2024.0,"Roy, Asitava Deb; Jaiswal, Ichchhit Bharat; Tiu, Devendra Nath; Das, Dipmala; Mondal, Shaikat; Behera, Joshil Kumar; Mondal, Himel; Jaiswal, Ichchhit B.; Tiu, Devendra; Behera IV, Joshil K.",Assessing the Utilization of Large Language Model Chatbots for Educational Purposes by Medical Teachers: A Nationwide Survey From India,Cureus,,,,https://www.cureus.com/articles/312879-assessing-the-utilization-of-large-language-model-chatbots-for-educational-purposes-by-medical-teachers-a-nationwide-survey-from-india.pdf,"Abstract  Background  Large language models (LLMs) are increasingly explored in healthcare and education. In medical education,  they hold the potential to enhance learning by supporting personalized teaching, resource development, and  student engagement. However, LLM use also raises concerns about ethics, accuracy, and reliance.  Understanding how educators leverage LLMs can help assess their role and implications in medical  education.  Methods  This cross-sectional online survey was conducted among medical teachers in India from December 2023 to  March 2024. A validated questionnaire with acceptable internal consistency and test-retest reliability was  used. It collected data on LLM chatbot usage patterns, as well as teachers' knowledge, attitudes, and  practices regarding LLMs for educational purposes.  Results  A total of 396 medical teachers with an average teaching experience of 4.12±2.47 (minimum six months,  maximum 13 years) years participated from different parts of India. The majority of the teachers heard about  ChatGPT (OpenAI, San Francisco, CA, USA) (85%), followed by Copilot/Bing (Microsoft, Washington, DC,  USA) (53%), and Gemini/Bard (Google, Mountain View, CA, USA) (45%) (p-value < 0.0001). However, 29% of  the respondents never used it and 47% rarely use LLMs for educational purposes (p-value < 0.0001). The  majority of the teachers use it for making any topic simple (55%), generating text for PowerPoint slides  (55%), generating multiple-choice questions (MCQs) (52%), and finding answers to student’s queries (35%).  Knowledge (3.4±0.47) showed the highest score, followed by practice (3.3±0.81) and attitude (3.14±0.46) (pvalue = 0.0023).  Conclusion  While awareness of LLMs was high among medical teachers in India, their actual usage for educational  purposes remains limited. Despite recognizing the potential of LLMs for simplifying topics, generating  teaching materials, and addressing student queries, a significant proportion of educators seldom integrate  these technologies into their teaching practices. Institutions may provide training to help medical educators  effectively integrate LLMs into teaching practices.",2024,2025-05-22 09:51:42,2025-05-26 13:41:42,2025-05-22 09:51:38,,,11.0,16.0,,,Assessing the Utilization of Large Language Model Chatbots for Educational Purposes by Medical Teachers,,,,,,,,,,,,Google Scholar,,Publisher: Cureus,,/Users/ksoares/Zotero/storage/XJNDP2SM/Roy et al. - 2024 - Assessing the Utilization of Large Language Model Chatbots for Educational Purposes by Medical Teach.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
447VX95H,journalArticle,2024.0,"Gasparini, Loretta; Phillipson, Nitya; Capurro, Daniel; Rosenberg, Revital; Buttery, Jim; Howley, Jayne; Ranganathan, Sarath; Quinlan, Catherine; Selvadurai, Niloufer; Wildenauer, Michael","A survey of Large Language Model use in a hospital, research, and teaching campus",medRxiv,,,,https://www.medrxiv.org/content/10.1101/2024.09.11.24313512.abstract,"Abstract Background The use of Large Language Models (LLMs) has exploded since November 2022 but there is sparse evidence regarding LLM use in health, medical and research contexts. Objective To summarise the current uses of and attitudes towards LLMs across the clinical, research and teaching contexts in our campus. Design We administered a survey about LLM uses and attitudes. We conducted summary quantitative analysis and inductive qualitative analysis of free text responses. Setting In August-September 2023, we circulated the survey amongst all staff and students across our campus (approximately n=7500), a fully integrated paediatric academic hospital and research institute. Participants We received 281 anonymous survey responses. Main outcome measures We asked about participants’ knowledge of LLMs, their current use of LLMs in professional or learning contexts, and perspectives on possible future uses, opportunities, and risks of LLM use. Results Over 90% of respondents have heard of LLM tools and about two-thirds have used them in their work on our campus. Respondents reported using LLMs for a range of uses, including for generating or editing text and exploring ideas. Many, but not necessarily all, respondents seem aware of the limitations and potential risks of LLMs, including privacy and security risks. Various respondents expressed enthusiasm about opportunities of LLM use, including increased efficiency. Conclusions Our findings show LLM tools are already widely used on our campus. Guidelines and governance are needed to keep up with practice. We have developed recommendations for the use of LLMs on our campus using insights from this survey. What is known The known: The use of Large Language Models (LLMs) has increased rapidly since the introduction of ChatGPT in November 2022. The new: Most survey respondents are aware of, if not using, LLMs in their work across our hospital, research, and university campus. Diverse uses were reported, including generating or editing text and exploring ideas. There were varying attitudes towards LLMs. Perceived risks included privacy and security risks. A key perceived opportunity was increased efficiency. The implications: LLM tools are already widely used on our campus, highlighting the need for guidelines and governance to keep up with practice.",2024,2025-05-22 09:51:42,2025-05-26 13:38:09,2025-05-22 09:51:42,2024–09,,,,,,,,,,,,,,,,,,Google Scholar,,Publisher: Cold Spring Harbor Laboratory Press,,"/Users/ksoares/Zotero/storage/58ZMT55Q/Gasparini et al. - 2024 - A survey of Large Language Model use in a hospital, research, and teaching campus.pdf",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
RZGXZN2M,journalArticle,2024.0,"Crandall, Johannah L.; Crandall, Aaron S.",Large Language Model-Supported Software Testing with the CS Matrix Taxonomy,Journal of Computing Sciences in Colleges,,,,https://www.ccsc.org/publications/journals/CCSCNW2024Final.pdf#page=49,"New breakthroughs in code synthesis from Generative Pre-Trained Transformers (GPT) and Large Language Model (LLM) algorithms are driving significant changes to software engineering education. Having algorithms able to generate components of a software project means that software developers will need stronger skills in requirements specification to guide code generation as well as stronger skills in code review, testing, and integration to incorporate AI-generated code into projects. Shifts in industry and classroom practices are already occurring with the availability of inline code generation tools like GitHub’s Copilot, which makes discussion of pedagogical strategies in this area a timely topic. Of immediate concern in computer science education is the potential for LLM-generated code and code help to undermine the learning of CS students. In order to avoid such undermining in even intentional uses of LLM-enhanced learning supports, it is necessary to clarify the roles such supports need to play in the pedagogical process. The Computer Science Matrix Taxonomy provides a strong framework for organizing software testing learning outcomes as well as delineating the operational space in which LLM-based feedback tools should operate to support those learning outcomes. In this paper, the authors operationalize the CS Matrix Taxonomy for software testing learning outcomes and illustrate the integration of LLM-generated test strategy suggestions as an extension of the peer coding/testing model. The work includes examples of AI-generated code testing suggestions that students would use to help guide their own code synthesis for assignments or projects.",2024,2025-05-22 09:58:02,2025-05-26 13:50:36,2025-05-22 09:57:26,49–58,,1.0,40.0,,,,,,,,,,,,,,,Google Scholar,,Publisher: Consortium for Computing Sciences in Colleges,,/Users/ksoares/Zotero/storage/TMH299YM/Crandall et Crandall - 2024 - Large Language Model-Supported Software Testing with the CS Matrix Taxonomy.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
VW3HH4K2,journalArticle,2024.0,"Gresham, Dylan",Exploring and Evaluating Large Language Model Survey Paper Categories,,,,,https://www.preprints.org/frontend/manuscript/bf48e3c9d32a32f3e3585fca5754db91/download_pub,"In February of 2024, Dr. Jun Zhuang and Dr. Casey Kennington published a paper [1] in which they  classified large language model (LLM) survey papers into different taxonomies utilizing graph learning. In this  paper, I evaluate the dataset they created and used and propose that in its current state, there is not enough  samples to classify other survey papers into specific categories.",2024,2025-05-22 09:58:02,2025-05-26 13:43:53,2025-05-22 09:57:30,,,,,,,,,,,,,,,,,,,Google Scholar,,,,/Users/ksoares/Zotero/storage/Q72WFSWL/Gresham - 2024 - Exploring and Evaluating Large Language Model Survey Paper Categories.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
6ICG7GR9,journalArticle,2024.0,"Tania, Meherunnesa",Automated Classification and Trend Analysis of Large Language Model Survey Papers Using Machine Learning and Natural Language Processing Techniques,,,,,https://engrxiv.org/preprint/view/3984,"This study investigates the application of machine learning (ML) and natural language processing (NLP) techniques to classify academic survey papers into predefined taxonomy categories. The dataset, consisting of paper titles, summaries, release dates, taxonomy labels, and categories, was analyzed to uncover trends and patterns in the publication of research papers. Exploratory data analysis (EDA) revealed important insights through visualizations, such as publication trends over time, the distribution of taxonomy categories, and the most common terms used in paper summaries. Key NLP techniques, including Term Frequency-Inverse Document Frequency (TF-IDF), were employed to transform the textual data into numerical features, while one-hot encoding was applied to the categorical data. A Random Forest Classifier was trained on the extracted feature matrix to predict the taxonomy category of each paper. The model achieved promising accuracy, effectively capturing patterns in the dataset. The study also identified areas for future improvement, including addressing class imbalance and exploring more sophisticated models. These findings demonstrate the potential of ML and NLP for automating the classification of academic papers, providing a scalable solution for managing large collections of research literature while offering insights into publication dynamics and trends.",2024,2025-05-22 09:58:02,2025-05-26 13:41:50,2025-05-22 09:57:32,,,,,,,,,,,,,,,,,,,Google Scholar,,Publisher: Engineering Archive,,/Users/ksoares/Zotero/storage/PTVSXJS9/Tania - 2024 - Automated Classification and Trend Analysis of Large Language Model Survey Papers Using Machine Lear.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
S5GKM6YN,journalArticle,,"Zhou, Duo; Zhang, Junyu; Feng, Tao; Sun, Yifan",A Survey on Alignment for Large Language Model Agents,,,,,https://openreview.net/forum?id=gkxt5kZS84,"As large language models (LLMs) evolve from passive text generators to autonomous agents capable of decision-making and real-world interaction, ensuring their alignment with human goals, values, and safety expectations becomes increasingly critical. This survey offers a comprehensive examination of alignment in the context of LLM-based agents, spanning technical, ethical, and sociotechnical dimensions. We begin by defining the multifaceted goals of agent alignment, including task fidelity, ethical compliance, and long-term behavioral robustness. We then analyze sources and challenges of alignment data, review alignment techniques such as reinforcement learning from human feedback (RLHF), adversarial training, and scalable oversight strategies, and assess benchmark methodologies across general intent following, safety robustness, ethical reasoning, and multimodal performance. Looking forward, we identify key research directions, including constitutional AI, graph-based multi-agent coordination, and super alignment for heterogeneous agent clusters. By synthesizing recent advances, this survey provides a roadmap toward building trustworthy and controllable LLM agents for real-world deployment.",,2025-05-22 09:58:02,2025-05-26 13:38:44,2025-05-22 09:57:46,,,,,,,,,,,,,,,,,,,Google Scholar,,,,/Users/ksoares/Zotero/storage/SXJQSY6F/Zhou et al. - A Survey on Alignment for Large Language Model Agents.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
WJDQUSBR,journalArticle,2025.0,"Memmert, Lucas; Borchers, Marten; Plückhahn, Juliane; Bittner, Eva",Solving Coding Challenges Jointly with a Large Language Model: Understanding Student Journeys Through Bloom’s Taxonomy,,,,,https://scholarspace.manoa.hawaii.edu/items/f8659bbb-1974-4f11-914a-5b63a3e7deae,"With the advancement of large language models (LLMs) such as ChatGPT, students increasingly employ LLMs during university courses to improve their coding skills. However, it is still unclear what using these systems means for student learning. In our study, we explore how students employ a LLM when solving a realistic coding challenge over the course of a semester in small groups. We analyze students’ LLM chat log data through the framework of Bloom’s taxonomy, a well-established education framework, to understand their problem-solving and learning behavior. We enrich our analysis with students’ responses to weekly survey questions, contextualizing our findings with subjective experiences. We find that students primarily use LLMs for lower-order thinking skills like remembering, understanding, and applying procedural knowledge. With our study, we contribute to the growing literature on understanding the effects of working and learning with LLMs and offer practical suggestions for teachers and students.",2025,2025-05-22 09:58:02,2025-05-26 13:52:08,2025-05-22 09:57:48,,,,,,,Solving Coding Challenges Jointly with a Large Language Model,,,,,,,,,,,,Google Scholar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
MWFXNCK2,journalArticle,2025.0,"Man-Li, L. I.; Wei-Feng, QIAO; Rui-Miao, L. I.",Can Large Language Model Tools Promote the Development of University Students’ Higher-Order Thinking Skills?——An Empirical Analysis Based on a Questionnaire Survey of Students from 12 Double First-Class Universities,Modern Educational Technology,,,,https://www.sciopen.com/article/10.3969/j.issn.1009-8097.2025.01.004,"In the era of intelligent technology, it has gradually become a consensus to pay attention to the cultivation of university students’ higher-order thinking skills. However, there remains debate over whether university students can use large language model tools to promote the development of higher-order thinking skills is still controversial. Clarifying this dispute not only helps to theoretically analyze the mechanism of students’ higher-order thinking development under the background of AI, but also provides a reliable basis for what measures to be taken by schools and teachers in educational practice. Accordingly, this paper made an empirical analysis on university students’ use situation of large language model tools and its impact on higher-order thinking skills using questionnaire survey data of students from 12 double first-class universities in China. The results showed that more than half of university students used large language model tools, but the deep creative application still needed to be strengthened; increasing the frequency of basic execution and deep creative application of large language model tools had a significant positive effect on the development of higher-order thinking skills; interaction quality played a mediating role in the influence of the use frequency of large language model tool on higher-order thinking skills. Based on this, the paper suggested that universities should acknowledge the role of AI technology in the teaching process of higher education and actively promote the application of AI in empowering education, combine real situations inside and outside the class to promote the deep creative application of large language model tools, and strengthen the cultivation of AI literacy of teachers and students to improve the quality of human-computer interaction.",2025,2025-05-22 09:58:02,2025-05-26 13:42:16,2025-05-22 09:57:57,34–43,,1.0,35.0,,,Can Large Language Model Tools Promote the Development of University Students’ Higher-Order Thinking Skills?,,,,,,,,,,,,Google Scholar,,Publisher: 《 现代教育技术》 编辑部,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
78YFM9M3,journalArticle,2024.0,"Zhu, Xunyu; Li, Jian; Liu, Yong; Ma, Can; Wang, Weiping",A survey on model compression for large language models,Transactions of the Association for Computational Linguistics,,,,https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00704/125482,"Large Language Models (LLMs) have transformed natural language processing tasks successfully. Yet, their large size and high computational needs pose challenges for practical use, especially in resource-limited settings. Model compression has emerged as a key research area to address these challenges. This paper presents a survey of model compression techniques for LLMs. We cover methods like quantization, pruning, and knowledge distillation, highlighting recent advancements. We also discuss benchmarking strategies and evaluation metrics crucial for assessing compressed LLMs. This survey offers valuable insights for researchers and practitioners, aiming to enhance efficiency and real-world applicability of LLMs while laying a foundation for future advancements.",2024,2025-05-22 09:58:02,2025-05-26 13:40:01,2025-05-22 09:58:00,1556–1577,,,12.0,,,,,,,,,,,,,,,Google Scholar,,"Publisher: MIT Press 255 Main Street, 9th Floor, Cambridge, Massachusetts 02142, USA …",,/Users/ksoares/Zotero/storage/CW4AEXVA/Zhu et al. - 2024 - A survey on model compression for large language models.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2
