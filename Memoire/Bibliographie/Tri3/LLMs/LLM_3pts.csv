Key,Item Type,Publication Year,Author,Title,Publication Title,ISBN,ISSN,DOI,Url,Abstract Note,Date,Date Added,Date Modified,Access Date,Pages,Num Pages,Issue,Volume,Number Of Volumes,Journal Abbreviation,Short Title,Series,Series Number,Series Text,Series Title,Publisher,Place,Language,Rights,Type,Archive,Archive Location,Library Catalog,Call Number,Extra,Notes,File Attachments,Link Attachments,Manual Tags,Automatic Tags,Editor,Series Editor,Translator,Contributor,Attorney Agent,Book Author,Cast Member,Commenter,Composer,Cosponsor,Counsel,Interviewer,Producer,Recipient,Reviewed Author,Scriptwriter,Words By,Guest,Number,Edition,Running Time,Scale,Medium,Artwork Size,Filing Date,Application Number,Assignee,Issuing Authority,Country,Meeting Name,Conference Name,Court,References,Reporter,Legal Status,Priority Numbers,Programming Language,Version,System,Code,Code Number,Section,Session,Committee,History,Legislative Body,Score
UELANWA6,preprint,2024.0,"Cui, Tianyu; Wang, Yanling; Fu, Chuanpu; Xiao, Yong; Li, Sijia; Deng, Xinhao; Liu, Yunpeng; Zhang, Qinglin; Qiu, Ziyi; Li, Peiyang; Tan, Zhixing; Xiong, Junwu; Kong, Xinyu; Wen, Zujie; Xu, Ke; Li, Qi","Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems",,,,10.48550/arXiv.2401.05778,http://arxiv.org/abs/2401.05778,"Large language models (LLMs) have strong capabilities in solving diverse natural language processing tasks. However, the safety and security issues of LLM systems have become the major obstacle to their widespread application. Many studies have extensively investigated risks in LLM systems and developed the corresponding mitigation strategies. Leading-edge enterprises such as OpenAI, Google, Meta, and Anthropic have also made lots of efforts on responsible LLMs. Therefore, there is a growing need to organize the existing studies and establish comprehensive taxonomies for the community. In this paper, we delve into four essential modules of an LLM system, including an input module for receiving prompts, a language model trained on extensive corpora, a toolchain module for development and deployment, and an output module for exporting LLM-generated content. Based on this, we propose a comprehensive taxonomy, which systematically analyzes potential risks associated with each module of an LLM system and discusses the corresponding mitigation strategies. Furthermore, we review prevalent benchmarks, aiming to facilitate the risk assessment of LLM systems. We hope that this paper can help LLM participants embrace a systematic perspective to build their responsible LLM systems.",2024-01-11,2025-05-22 09:46:57,2025-05-22 09:46:57,2025-05-22 09:46:29,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2401.05778 [cs],,"/Users/ksoares/Zotero/storage/3RKJM9SC/Cui et al. - 2024 - Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems.pdf; /Users/ksoares/Zotero/storage/6RTF3GKF/2401.html",,,Computer Science - Computation and Language; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,arXiv:2401.05778,,,,,,,,,,,,,,,,,,,,,,,,,,,,3
GX7DFJHB,journalArticle,2024.0,"Parker, Michael J.; Anderson, Caitlin; Stone, Claire; Oh, YeaRim",A Large Language Model Approach to Educational Survey Feedback Analysis,International Journal of Artificial Intelligence in Education,,"1560-4292, 1560-4306",10.1007/s40593-024-00414-0,https://link.springer.com/10.1007/s40593-024-00414-0,"Abstract             This paper assesses the potential for the large language models (LLMs) GPT-4 and GPT-3.5 to aid in deriving insight from education feedback surveys. Exploration of LLM use cases in education has focused on teaching and learning, with less exploration of capabilities in education feedback analysis. Survey analysis in education involves goals such as finding gaps in curricula or evaluating teachers, often requiring time-consuming manual processing of textual responses. LLMs have the potential to provide a flexible means of achieving these goals without specialized machine learning models or fine-tuning. We demonstrate a versatile approach to such goals by treating them as sequences of natural language processing (NLP) tasks including classification (multi-label, multi-class, and binary), extraction, thematic analysis, and sentiment analysis, each performed by LLM. We apply these workflows to a real-world dataset of 2500 end-of-course survey comments from biomedical science courses, and evaluate a zero-shot approach (i.e., requiring no examples or labeled training data) across all tasks, reflecting education settings, where labeled data is often scarce. By applying effective prompting practices, we achieve human-level performance on multiple tasks with GPT-4, enabling workflows necessary to achieve typical goals. We also show the potential of inspecting LLMsâ€™ chain-of-thought (CoT) reasoning for providing insight that may foster confidence in practice. Moreover, this study features development of a versatile set of classification categories, suitable for various course types (online, hybrid, or in-person) and amenable to customization. Our results suggest that LLMs can be used to derive a range of insights from survey text.",2024-06-25,2025-05-22 09:46:57,2025-05-22 09:46:57,2025-05-22 09:46:48,,,,,,Int J Artif Intell Educ,,,,,,,,en,,,,,DOI.org (Crossref),,,,/Users/ksoares/Zotero/storage/W8EYKNYB/Parker et al. - 2024 - A Large Language Model Approach to Educational Survey Feedback Analysis.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3
WZQ75C9Y,preprint,2024.0,"Wang, Shang; Zhu, Tianqing; Liu, Bo; Ding, Ming; Guo, Xu; Ye, Dayong; Zhou, Wanlei; Yu, Philip S.",Unique Security and Privacy Threats of Large Language Model: A Comprehensive Survey,,,,10.48550/arXiv.2406.07973,http://arxiv.org/abs/2406.07973,"With the rapid development of artificial intelligence, large language models (LLMs) have made remarkable advancements in natural language processing. These models are trained on vast datasets to exhibit powerful language understanding and generation capabilities across various applications, including machine translation, chatbots, and agents. However, LLMs have revealed a variety of privacy and security issues throughout their life cycle, drawing significant academic and industrial attention. Moreover, the risks faced by LLMs differ significantly from those encountered by traditional language models. Given that current surveys lack a clear taxonomy of unique threat models across diverse scenarios, we emphasize the unique privacy and security threats associated with five specific scenarios: pre-training, fine-tuning, retrieval-augmented generation systems, deployment, and LLM-based agents. Addressing the characteristics of each risk, this survey outlines potential threats and countermeasures. Research on attack and defense situations can offer feasible research directions, enabling more areas to benefit from LLMs.",2024-06-18,2025-05-22 09:47:55,2025-05-22 09:47:55,2025-05-22 09:47:27,,,,,,,Unique Security and Privacy Threats of Large Language Model,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2406.07973 [cs],,/Users/ksoares/Zotero/storage/QDXS7M4N/Wang et al. - 2024 - Unique Security and Privacy Threats of Large Language Model A Comprehensive Survey.pdf; /Users/ksoares/Zotero/storage/RJDIK5SG/2406.html,,,Computer Science - Cryptography and Security,,,,,,,,,,,,,,,,,,,arXiv:2406.07973,,,,,,,,,,,,,,,,,,,,,,,,,,,,3
UBD59NLQ,journalArticle,2024.0,"Li, Haoyang; Li, Yiming; Tian, Anxin; Tang, Tianhao; Xu, Zhanchao; Chen, Xuejia; Hu, Nicole; Dong, Wei; Li, Qing; Chen, Lei",A survey on large language model acceleration based on kv cache management,arXiv preprint arXiv:2412.19442,,,,https://arxiv.org/abs/2412.19442,"Large Language Models (LLMs) have revolutionized a wide range of domains such as natural language processing, computer vision, and multi-modal tasks due to their ability to comprehend context and perform logical reasoning. However, the computational and memory demands of LLMs, particularly during inference, pose significant challenges when scaling them to real-world, long-context, and real-time applications. Key-Value (KV) cache management has emerged as a critical optimization technique for accelerating LLM inference by reducing redundant computations and improving memory utilization. This survey provides a comprehensive overview of KV cache management strategies for LLM acceleration, categorizing them into token-level, modellevel, and system-level optimizations. Token-level strategies include KV cache selection, budget allocation, merging, quantization, and low-rank decomposition, while model-level optimizations focus on architectural innovations and attention mechanisms to enhance KV reuse. System-level approaches address memory management, scheduling, and hardwareaware designs to improve efficiency across diverse computing environments. Additionally, the survey provides an overview of both text and multimodal datasets and benchmarks used to evaluate these strategies. By presenting detailed taxonomies and comparative analyses, this work aims to offer useful insights for researchers and practitioners to support the development of efficient and scalable KV cache management techniques, contributing to the practical deployment of LLMs in real-world applications. The curated paper list for KV cache management is in: https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management.",2024,2025-05-22 09:49:53,2025-05-26 13:39:43,2025-05-22 09:49:31,,,,,,,,,,,,,,,,,,,Google Scholar,,,,/Users/ksoares/Zotero/storage/K5MCYNID/Li et al. - 2024 - A survey on large language model acceleration based on kv cache management.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3
B7A3CR9H,preprint,2025.0,"Nie, Zhijie; Feng, Zhangchi; Li, Mingxin; Zhang, Cunwang; Zhang, Yanzhao; Long, Dingkun; Zhang, Richong",When Text Embedding Meets Large Language Model: A Comprehensive Survey,,,,10.48550/arXiv.2412.09165,http://arxiv.org/abs/2412.09165,"Text embedding has become a foundational technology in natural language processing (NLP) during the deep learning era, driving advancements across a wide array of downstream tasks. While many natural language understanding challenges can now be modeled using generative paradigms and leverage the robust generative and comprehension capabilities of large language models (LLMs), numerous practical applications - such as semantic matching, clustering, and information retrieval - continue to rely on text embeddings for their efficiency and effectiveness. Therefore, integrating LLMs with text embeddings has become a major research focus in recent years. In this survey, we categorize the interplay between LLMs and text embeddings into three overarching themes: (1) LLM-augmented text embedding, enhancing traditional embedding methods with LLMs; (2) LLMs as text embedders, adapting their innate capabilities for high-quality embedding; and (3) Text embedding understanding with LLMs, leveraging LLMs to analyze and interpret embeddings. By organizing recent works based on interaction patterns rather than specific downstream applications, we offer a novel and systematic overview of contributions from various research and application domains in the era of LLMs. Furthermore, we highlight the unresolved challenges that persisted in the pre-LLM era with pre-trained language models (PLMs) and explore the emerging obstacles brought forth by LLMs. Building on this analysis, we outline prospective directions for the evolution of text embedding, addressing both theoretical and practical opportunities in the rapidly advancing landscape of NLP.",2025-03-20,2025-05-22 09:51:42,2025-05-22 09:51:42,2025-05-22 09:51:22,,,,,,,When Text Embedding Meets Large Language Model,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2412.09165 [cs],,/Users/ksoares/Zotero/storage/8V7Y6HXX/Nie et al. - 2025 - When Text Embedding Meets Large Language Model A Comprehensive Survey.pdf; /Users/ksoares/Zotero/storage/9IXXGT82/2412.html,,,Computer Science - Computation and Language; Computer Science - Artificial Intelligence; Computer Science - Information Retrieval,,,,,,,,,,,,,,,,,,,arXiv:2412.09165,,,,,,,,,,,,,,,,,,,,,,,,,,,,3
