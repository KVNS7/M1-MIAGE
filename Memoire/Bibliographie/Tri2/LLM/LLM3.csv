Key,Item Type,Publication Year,Author,Title,Publication Title,ISBN,ISSN,DOI,Url,Abstract Note,Date,Date Added,Date Modified,Access Date,Pages,Num Pages,Issue,Volume,Number Of Volumes,Journal Abbreviation,Short Title,Series,Series Number,Series Text,Series Title,Publisher,Place,Language,Rights,Type,Archive,Archive Location,Library Catalog,Call Number,Extra,Notes,File Attachments,Link Attachments,Manual Tags,Automatic Tags,Editor,Series Editor,Translator,Contributor,Attorney Agent,Book Author,Cast Member,Commenter,Composer,Cosponsor,Counsel,Interviewer,Producer,Recipient,Reviewed Author,Scriptwriter,Words By,Guest,Number,Edition,Running Time,Scale,Medium,Artwork Size,Filing Date,Application Number,Assignee,Issuing Authority,Country,Meeting Name,Conference Name,Court,References,Reporter,Legal Status,Priority Numbers,Programming Language,Version,System,Code,Code Number,Section,Session,Committee,History,Legislative Body
BRZ7CKMX,preprint,2024.0,"Qin, Libo; Chen, Qiguang; Zhou, Yuhang; Chen, Zhi; Li, Yinghui; Liao, Lizi; Li, Min; Che, Wanxiang; Yu, Philip S.","Multilingual Large Language Model: A Survey of Resources, Taxonomy and Frontiers",,,,10.48550/arXiv.2404.04925,http://arxiv.org/abs/2404.04925,"Multilingual Large Language Models are capable of using powerful Large Language Models to handle and respond to queries in multiple languages, which achieves remarkable success in multilingual natural language processing tasks. Despite these breakthroughs, there still remains a lack of a comprehensive survey to summarize existing approaches and recent developments in this field. To this end, in this paper, we present a thorough review and provide a unified perspective to summarize the recent progress as well as emerging trends in multilingual large language models (MLLMs) literature. The contributions of this paper can be summarized: (1) First survey: to our knowledge, we take the first step and present a thorough review in MLLMs research field according to multi-lingual alignment; (2) New taxonomy: we offer a new and unified perspective to summarize the current progress of MLLMs; (3) New frontiers: we highlight several emerging frontiers and discuss the corresponding challenges; (4) Abundant resources: we collect abundant open-source resources, including relevant papers, data corpora, and leaderboards. We hope our work can provide the community with quick access and spur breakthrough research in MLLMs.",2024-04-07,2025-05-22 09:46:57,2025-05-22 09:46:57,2025-05-22 09:46:21,,,,,,,Multilingual Large Language Model,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2404.04925 [cs],,"/Users/ksoares/Zotero/storage/IW8UXR35/Qin et al. - 2024 - Multilingual Large Language Model A Survey of Resources, Taxonomy and Frontiers.pdf; /Users/ksoares/Zotero/storage/2XDX7NT3/2404.html",,,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,arXiv:2404.04925,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXFAPMKF,journalArticle,2024.0,"Wu, Xingyu; Wu, Sheng-hao; Wu, Jibin; Feng, Liang; Tan, Kay Chen",Evolutionary computation in the era of large language model: Survey and roadmap,IEEE Transactions on Evolutionary Computation,,,,https://ieeexplore.ieee.org/abstract/document/10767756/,"Large language models (LLMs) have not only revolutionized natural language processing but also extended their prowess to various domains, marking a significant stride toward artificial general intelligence. The interplay between LLMs and evolutionary algorithms (EAs), despite differing in objectives and methodologies, share a common pursuit of applicability in complex problems. Meanwhile, EA can provide an optimization framework for LLM’s further enhancement under closed box settings, empowering LLM with flexible global search capacities. On the other hand, the abundant domain knowledge inherent in LLMs could enable EA to conduct more intelligent searches. Furthermore, the text processing and generative capabilities of LLMs would aid in deploying EAs across a wide range of tasks. Based on these complementary advantages, this article provides a thorough review and a forward-looking roadmap, categorizing the reciprocal inspiration into two main avenues: 1) LLM-enhanced EA and 2) EA-enhanced LLM. Some integrated synergy methods are further introduced to exemplify the complementarity between LLMs and EAs in diverse scenarios, including code generation, software engineering, neural architecture search, and various generation tasks. As the first comprehensive review focused on the EA research in the era of LLMs, this article provides a foundational stepping stone for understanding the collaborative potential of LLMs and EAs. The identified challenges and future directions offer guidance for researchers and practitioners to unlock the full potential of this innovative collaboration in propelling advancements in optimization and artificial intelligence. We have created a GitHub repository to index the relevant papers: https://github.com/wuxingyu-ai/LLM4EC.",2024,2025-05-22 09:46:57,2025-05-26 13:42:45,2025-05-22 09:46:23,,,,,,,Evolutionary computation in the era of large language model,,,,,,,,,,,,Google Scholar,,Publisher: IEEE,,/Users/ksoares/Zotero/storage/3TDR8AL2/Wu et al. - 2024 - Evolutionary computation in the era of large language model Survey and roadmap.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FXMXHWZU,journalArticle,2024.0,"Wang, Lei; Ma, Chen; Feng, Xueyang; Zhang, Zeyu; Yang, Hao; Zhang, Jingsen; Chen, Zhiyuan; Tang, Jiakai; Chen, Xu; Lin, Yankai; Zhao, Wayne Xin; Wei, Zhewei; Wen, Jirong",A survey on large language model based autonomous agents,Frontiers of Computer Science,,"2095-2228, 2095-2236",10.1007/s11704-024-40231-1,https://link.springer.com/10.1007/s11704-024-40231-1,"Abstract             Autonomous agents have long been a research focus in academic and industry communities. Previous research often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from human learning processes, and makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of Web knowledge, large language models (LLMs) have shown potential in human-level intelligence, leading to a surge in research on LLM-based autonomous agents. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of LLM-based autonomous agents from a holistic perspective. We first discuss the construction of LLM-based autonomous agents, proposing a unified framework that encompasses much of previous work. Then, we present a overview of the diverse applications of LLM-based autonomous agents in social science, natural science, and engineering. Finally, we delve into the evaluation strategies commonly used for LLM-based autonomous agents. Based on the previous studies, we also present several challenges and future directions in this field.",2024-12,2025-05-22 09:46:57,2025-05-22 09:46:57,2025-05-22 09:46:24,186345,,6.0,18.0,,Front. Comput. Sci.,,,,,,,,en,,,,,DOI.org (Crossref),,,,/Users/ksoares/Zotero/storage/F5NWZHDJ/Wang et al. - 2024 - A survey on large language model based autonomous agents.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
V8KEIG8X,preprint,2024.0,"Guo, Taicheng; Chen, Xiuying; Wang, Yaqi; Chang, Ruidi; Pei, Shichao; Chawla, Nitesh V.; Wiest, Olaf; Zhang, Xiangliang",Large Language Model based Multi-Agents: A Survey of Progress and Challenges,,,,10.48550/arXiv.2402.01680,http://arxiv.org/abs/2402.01680,"Large Language Models (LLMs) have achieved remarkable success across a wide array of tasks. Due to the impressive planning and reasoning abilities of LLMs, they have been used as autonomous agents to do many tasks automatically. Recently, based on the development of using one LLM as a single planning or decision-making agent, LLM-based multi-agent systems have achieved considerable progress in complex problem-solving and world simulation. To provide the community with an overview of this dynamic field, we present this survey to offer an in-depth discussion on the essential aspects of multi-agent systems based on LLMs, as well as the challenges. Our goal is for readers to gain substantial insights on the following questions: What domains and environments do LLM-based multi-agents simulate? How are these agents profiled and how do they communicate? What mechanisms contribute to the growth of agents' capacities? For those interested in delving into this field of study, we also summarize the commonly used datasets or benchmarks for them to have convenient access. To keep researchers updated on the latest studies, we maintain an open-source GitHub repository, dedicated to outlining the research on LLM-based multi-agent systems.",2024-04-19,2025-05-22 09:46:57,2025-05-22 09:46:57,2025-05-22 09:46:27,,,,,,,Large Language Model based Multi-Agents,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2402.01680 [cs],,/Users/ksoares/Zotero/storage/SYVU6VCD/Guo et al. - 2024 - Large Language Model based Multi-Agents A Survey of Progress and Challenges.pdf; /Users/ksoares/Zotero/storage/ZAQLAK9I/2402.html,,,Computer Science - Computation and Language; Computer Science - Artificial Intelligence; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,arXiv:2402.01680,,,,,,,,,,,,,,,,,,,,,,,,,,,
UELANWA6,preprint,2024.0,"Cui, Tianyu; Wang, Yanling; Fu, Chuanpu; Xiao, Yong; Li, Sijia; Deng, Xinhao; Liu, Yunpeng; Zhang, Qinglin; Qiu, Ziyi; Li, Peiyang; Tan, Zhixing; Xiong, Junwu; Kong, Xinyu; Wen, Zujie; Xu, Ke; Li, Qi","Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems",,,,10.48550/arXiv.2401.05778,http://arxiv.org/abs/2401.05778,"Large language models (LLMs) have strong capabilities in solving diverse natural language processing tasks. However, the safety and security issues of LLM systems have become the major obstacle to their widespread application. Many studies have extensively investigated risks in LLM systems and developed the corresponding mitigation strategies. Leading-edge enterprises such as OpenAI, Google, Meta, and Anthropic have also made lots of efforts on responsible LLMs. Therefore, there is a growing need to organize the existing studies and establish comprehensive taxonomies for the community. In this paper, we delve into four essential modules of an LLM system, including an input module for receiving prompts, a language model trained on extensive corpora, a toolchain module for development and deployment, and an output module for exporting LLM-generated content. Based on this, we propose a comprehensive taxonomy, which systematically analyzes potential risks associated with each module of an LLM system and discusses the corresponding mitigation strategies. Furthermore, we review prevalent benchmarks, aiming to facilitate the risk assessment of LLM systems. We hope that this paper can help LLM participants embrace a systematic perspective to build their responsible LLM systems.",2024-01-11,2025-05-22 09:46:57,2025-05-22 09:46:57,2025-05-22 09:46:29,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2401.05778 [cs],,"/Users/ksoares/Zotero/storage/3RKJM9SC/Cui et al. - 2024 - Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems.pdf; /Users/ksoares/Zotero/storage/6RTF3GKF/2401.html",,,Computer Science - Computation and Language; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,arXiv:2401.05778,,,,,,,,,,,,,,,,,,,,,,,,,,,
HZVBHPP3,journalArticle,2024.0,"Cao, Yuji; Zhao, Huan; Cheng, Yuheng; Shu, Ting; Chen, Yue; Liu, Guolong; Liang, Gaoqi; Zhao, Junhua; Yan, Jinyue; Li, Yun","Survey on large language model-enhanced reinforcement learning: Concept, taxonomy, and methods",IEEE Transactions on Neural Networks and Learning Systems,,,,https://ieeexplore.ieee.org/abstract/document/10766898/,"With extensive pretrained knowledge and high-level general capabilities, large language models (LLMs) emerge as a promising avenue to augment reinforcement learning (RL) in aspects, such as multitask learning, sample efficiency, and high-level task planning. In this survey, we provide a comprehensive review of the existing literature in LLM-enhanced RL and summarize its characteristics compared with conventional RL methods, aiming to clarify the research scope and directions for future studies. Utilizing the classical agent–environment interaction paradigm, we propose a structured taxonomy to systematically categorize LLMs’ functionalities in RL, including four roles: information processor, reward designer, decision-maker, and generator. For each role, we summarize the methodologies, analyze the specific RL challenges that are mitigated and provide insights into future directions. Finally, the comparative analysis of each role, potential applications, prospective opportunities, and challenges of the LLM-enhanced RL are discussed. By proposing this taxonomy, we aim to provide a framework for researchers to effectively leverage LLMs in the RL field, potentially accelerating RL applications in complex applications, such as robotics, autonomous driving, and energy systems.",2024,2025-05-22 09:46:57,2025-05-26 13:52:26,2025-05-22 09:46:29,,,,,,,Survey on large language model-enhanced reinforcement learning,,,,,,,,,,,,Google Scholar,,Publisher: IEEE,,"/Users/ksoares/Zotero/storage/MTD8FZCQ/Cao et al. - 2024 - Survey on large language model-enhanced reinforcement learning Concept, taxonomy, and methods.pdf",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
587PUDSY,preprint,2024.0,"Liu, Junwei; Wang, Kaixin; Chen, Yixuan; Peng, Xin; Chen, Zhenpeng; Zhang, Lingming; Lou, Yiling",Large Language Model-Based Agents for Software Engineering: A Survey,,,,10.48550/arXiv.2409.02977,http://arxiv.org/abs/2409.02977,"The recent advance in Large Language Models (LLMs) has shaped a new paradigm of AI agents, i.e., LLM-based agents. Compared to standalone LLMs, LLM-based agents substantially extend the versatility and expertise of LLMs by enhancing LLMs with the capabilities of perceiving and utilizing external resources and tools. To date, LLM-based agents have been applied and shown remarkable effectiveness in Software Engineering (SE). The synergy between multiple agents and human interaction brings further promise in tackling complex real-world SE problems. In this work, we present a comprehensive and systematic survey on LLM-based agents for SE. We collect 106 papers and categorize them from two perspectives, i.e., the SE and agent perspectives. In addition, we discuss open challenges and future directions in this critical domain. The repository of this survey is at https://github.com/FudanSELab/Agent4SE-Paper-List.",2024-09-04,2025-05-22 09:46:57,2025-05-22 09:46:57,2025-05-22 09:46:31,,,,,,,Large Language Model-Based Agents for Software Engineering,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2409.02977 [cs],,/Users/ksoares/Zotero/storage/VHS6DM7Z/Liu et al. - 2024 - Large Language Model-Based Agents for Software Engineering A Survey.pdf; /Users/ksoares/Zotero/storage/GLPDUKAC/2409.html,,,Computer Science - Artificial Intelligence; Computer Science - Software Engineering,,,,,,,,,,,,,,,,,,,arXiv:2409.02977,,,,,,,,,,,,,,,,,,,,,,,,,,,
E8LVRGZQ,journalArticle,2025.0,"Xi, Zhiheng; Chen, Wenxiang; Guo, Xin; He, Wei; Ding, Yiwen; Hong, Boyang; Zhang, Ming; Wang, Junzhe; Jin, Senjie; Zhou, Enyu; Zheng, Rui; Fan, Xiaoran; Wang, Xiao; Xiong, Limao; Zhou, Yuhao; Wang, Weiran; Jiang, Changhao; Zou, Yicheng; Liu, Xiangyang; Yin, Zhangyue; Dou, Shihan; Weng, Rongxiang; Qin, Wenjuan; Zheng, Yongyan; Qiu, Xipeng; Huang, Xuanjing; Zhang, Qi; Gui, Tao",The rise and potential of large language model based agents: a survey,Science China Information Sciences,,"1674-733X, 1869-1919",10.1007/s11432-024-4222-0,https://link.springer.com/10.1007/s11432-024-4222-0,"For a long time, researchers have sought artificial intelligence (AI) that matches or exceeds human intelligence. AI agents, which are artificial entities capable of sensing the environment, making decisions, and taking actions, are seen as a means to achieve this goal. Extensive efforts have been made to develop AI agents, with a primary focus on refining algorithms or training strategies to enhance specific skills or particular task performance. The field, however, lacks a sufficiently general and powerful model to serve as a foundation for building general agents adaptable to diverse scenarios. With their versatile capabilities, large language models (LLMs) pave a promising path for the development of general AI agents, and substantial progress has been made in the realm of LLM-based agents. In this article, we conduct a comprehensive survey on LLM-based agents, covering their construction frameworks, application scenarios, and the exploration of societies built upon LLM-based agents. We also conclude some potential future directions and open problems in this flourishing field.",2025-02,2025-05-22 09:46:57,2025-05-26 13:53:12,2025-05-22 09:46:34,121101,,2.0,68.0,,Sci. China Inf. Sci.,The rise and potential of large language model based agents,,,,,,,en,,,,,DOI.org (Crossref),,,,/Users/ksoares/Zotero/storage/TVJZQITW/Xi et al. - 2025 - The rise and potential of large language model based agents a survey.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DZQV8TI5,preprint,2025.0,"Hu, Sihao; Huang, Tiansheng; Liu, Gaowen; Kompella, Ramana Rao; Ilhan, Fatih; Tekin, Selim Furkan; Xu, Yichang; Yahn, Zachary; Liu, Ling",A Survey on Large Language Model-Based Game Agents,,,,10.48550/arXiv.2404.02039,http://arxiv.org/abs/2404.02039,"The development of game agents holds a critical role in advancing towards Artificial General Intelligence. The progress of Large Language Models (LLMs) offers an unprecedented opportunity to evolve and empower game agents with human-like decision-making capabilities in complex computer game environments. This paper provides a comprehensive overview of LLM-based game agents from a holistic viewpoint. First, we introduce the conceptual architecture of LLM-based game agents, centered around three core functional components: memory, reasoning and in/output. Second, we survey existing representative LLM-based game agents documented in the literature with respect to methodologies and adaptation agility across six genres of games, including adventure, communication, competition, cooperation, simulation, and crafting & exploration games. Finally, we present an outlook of future research and development directions in this burgeoning field. A curated list of relevant papers is maintained and made accessible at: https://github.com/git-disl/awesome-LLM-game-agent-papers.",2025-03-30,2025-05-22 09:46:57,2025-05-22 09:46:57,2025-05-22 09:46:35,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2404.02039 [cs],,/Users/ksoares/Zotero/storage/WQN788GX/Hu et al. - 2025 - A Survey on Large Language Model-Based Game Agents.pdf; /Users/ksoares/Zotero/storage/ZNRFFIP6/2404.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,arXiv:2404.02039,,,,,,,,,,,,,,,,,,,,,,,,,,,
EXET649U,preprint,2024.0,"Zhang, Zeyu; Bo, Xiaohe; Ma, Chen; Li, Rui; Chen, Xu; Dai, Quanyu; Zhu, Jieming; Dong, Zhenhua; Wen, Ji-Rong",A Survey on the Memory Mechanism of Large Language Model based Agents,,,,10.48550/arXiv.2404.13501,http://arxiv.org/abs/2404.13501,"Large language model (LLM) based agents have recently attracted much attention from the research and industry communities. Compared with original LLMs, LLM-based agents are featured in their self-evolving capability, which is the basis for solving real-world problems that need long-term and complex agent-environment interactions. The key component to support agent-environment interactions is the memory of the agents. While previous studies have proposed many promising memory mechanisms, they are scattered in different papers, and there lacks a systematical review to summarize and compare these works from a holistic perspective, failing to abstract common and effective designing patterns for inspiring future studies. To bridge this gap, in this paper, we propose a comprehensive survey on the memory mechanism of LLM-based agents. In specific, we first discuss ''what is'' and ''why do we need'' the memory in LLM-based agents. Then, we systematically review previous studies on how to design and evaluate the memory module. In addition, we also present many agent applications, where the memory module plays an important role. At last, we analyze the limitations of existing work and show important future directions. To keep up with the latest advances in this field, we create a repository at \url{https://github.com/nuster1128/LLM_Agent_Memory_Survey}.",2024-04-21,2025-05-22 09:46:57,2025-05-22 09:46:57,2025-05-22 09:46:39,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2404.13501 [cs],,/Users/ksoares/Zotero/storage/EVM2F5EM/Zhang et al. - 2024 - A Survey on the Memory Mechanism of Large Language Model based Agents.pdf; /Users/ksoares/Zotero/storage/2LXLVDMT/2404.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,arXiv:2404.13501,,,,,,,,,,,,,,,,,,,,,,,,,,,
UEQXKDFX,journalArticle,2024.0,"Yao, Yifan; Duan, Jinhao; Xu, Kaidi; Cai, Yuanfang; Sun, Zhibo; Zhang, Yue","A survey on large language model (llm) security and privacy: The good, the bad, and the ugly",High-Confidence Computing,,,,https://www.sciencedirect.com/science/article/pii/S266729522400014X,"Large Language Models (LLMs), such as ChatGPT and Bard, have revolutionized natural language understanding and generation. They possess deep language comprehension, human-like text generation capabilities, contextual awareness, and robust problem-solving skills, making them invaluable in various domains (e.g., search engines, customer support, translation). In the meantime, LLMs have also gained traction in the security community, revealing security vulnerabilities and showcasing their potential in security-related tasks. This paper explores the intersection of LLMs with security and privacy. Specifically, we investigate how LLMs positively impact security and privacy, potential risks and threats associated with their use, and inherent vulnerabilities within LLMs. Through a comprehensive literature review, the paper categorizes the papers into “The Good” (beneficial LLM applications), “The Bad” (offensive applications), and “The Ugly” (vulnerabilities of LLMs and their defenses). We have some interesting findings. For example, LLMs have proven to enhance code security (code vulnerability detection) and data privacy (data confidentiality protection), outperforming traditional methods. However, they can also be harnessed for various attacks (particularly user-level attacks) due to their human-like reasoning abilities. We have identified areas that require further research efforts. For example, Research on model and parameter extraction attacks is limited and often theoretical, hindered by LLM parameter scale and confidentiality. Safe instruction tuning, a recent development, requires more exploration. We hope that our work can shed light on the LLMs’ potential to both bolster and jeopardize cybersecurity.",2024,2025-05-22 09:46:57,2025-05-26 13:39:24,2025-05-22 09:46:41,100211,,,,,,A survey on large language model (llm) security and privacy,,,,,,,,,,,,Google Scholar,,Publisher: Elsevier,,/Users/ksoares/Zotero/storage/RB8KI6RC/S266729522400014X.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2G8QXCBI,preprint,2024.0,"Huang, Yining; Tang, Keke; Chen, Meilian; Wang, Boyuan",A Comprehensive Survey on Evaluating Large Language Model Applications in the Medical Industry,,,,10.48550/arXiv.2404.15777,http://arxiv.org/abs/2404.15777,"Since the inception of the Transformer architecture in 2017, Large Language Models (LLMs) such as GPT and BERT have evolved significantly, impacting various industries with their advanced capabilities in language understanding and generation. These models have shown potential to transform the medical field, highlighting the necessity for specialized evaluation frameworks to ensure their effective and ethical deployment. This comprehensive survey delineates the extensive application and requisite evaluation of LLMs within healthcare, emphasizing the critical need for empirical validation to fully exploit their capabilities in enhancing healthcare outcomes. Our survey is structured to provide an in-depth analysis of LLM applications across clinical settings, medical text data processing, research, education, and public health awareness. We begin by exploring the roles of LLMs in various medical applications, detailing their evaluation based on performance in tasks such as clinical diagnosis, medical text data processing, information retrieval, data analysis, and educational content generation. The subsequent sections offer a comprehensive discussion on the evaluation methods and metrics employed, including models, evaluators, and comparative experiments. We further examine the benchmarks and datasets utilized in these evaluations, providing a categorized description of benchmarks for tasks like question answering, summarization, information extraction, bioinformatics, information retrieval and general comprehensive benchmarks. This structure ensures a thorough understanding of how LLMs are assessed for their effectiveness, accuracy, usability, and ethical alignment in the medical domain. ...",2024-05-29,2025-05-22 09:46:57,2025-05-22 09:46:57,2025-05-22 09:46:43,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2404.15777 [cs],,/Users/ksoares/Zotero/storage/BSJD33XU/Huang et al. - 2024 - A Comprehensive Survey on Evaluating Large Language Model Applications in the Medical Industry.pdf; /Users/ksoares/Zotero/storage/XYCG5P8J/2404.html,,,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,arXiv:2404.15777,,,,,,,,,,,,,,,,,,,,,,,,,,,
3FQKAZF2,preprint,2025.0,"Zhang, Chaoyun; He, Shilin; Qian, Jiaxu; Li, Bowen; Li, Liqun; Qin, Si; Kang, Yu; Ma, Minghua; Liu, Guyue; Lin, Qingwei; Rajmohan, Saravan; Zhang, Dongmei; Zhang, Qi",Large Language Model-Brained GUI Agents: A Survey,,,,10.48550/arXiv.2411.18279,http://arxiv.org/abs/2411.18279,"GUIs have long been central to human-computer interaction, providing an intuitive and visually-driven way to access and interact with digital systems. The advent of LLMs, particularly multimodal models, has ushered in a new era of GUI automation. They have demonstrated exceptional capabilities in natural language understanding, code generation, and visual processing. This has paved the way for a new generation of LLM-brained GUI agents capable of interpreting complex GUI elements and autonomously executing actions based on natural language instructions. These agents represent a paradigm shift, enabling users to perform intricate, multi-step tasks through simple conversational commands. Their applications span across web navigation, mobile app interactions, and desktop automation, offering a transformative user experience that revolutionizes how individuals interact with software. This emerging field is rapidly advancing, with significant progress in both research and industry. To provide a structured understanding of this trend, this paper presents a comprehensive survey of LLM-brained GUI agents, exploring their historical evolution, core components, and advanced techniques. We address research questions such as existing GUI agent frameworks, the collection and utilization of data for training specialized GUI agents, the development of large action models tailored for GUI tasks, and the evaluation metrics and benchmarks necessary to assess their effectiveness. Additionally, we examine emerging applications powered by these agents. Through a detailed analysis, this survey identifies key research gaps and outlines a roadmap for future advancements in the field. By consolidating foundational knowledge and state-of-the-art developments, this work aims to guide both researchers and practitioners in overcoming challenges and unlocking the full potential of LLM-brained GUI agents.",2025-05-06,2025-05-22 09:46:57,2025-05-22 09:46:57,2025-05-22 09:46:45,,,,,,,Large Language Model-Brained GUI Agents,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2411.18279 [cs],,/Users/ksoares/Zotero/storage/2DML8XI8/Zhang et al. - 2025 - Large Language Model-Brained GUI Agents A Survey.pdf; /Users/ksoares/Zotero/storage/5BTN52ZH/2411.html,,,Computer Science - Computation and Language; Computer Science - Artificial Intelligence; Computer Science - Human-Computer Interaction,,,,,,,,,,,,,,,,,,,arXiv:2411.18279,,,,,,,,,,,,,,,,,,,,,,,,,,,
XH6BS2BT,journalArticle,2024.0,"Zhou, Hao; Hu, Chengming; Yuan, Ye; Cui, Yufei; Jin, Yili; Chen, Can; Wu, Haolun; Yuan, Dun; Jiang, Li; Wu, Di","Large language model (llm) for telecommunications: A comprehensive survey on principles, key techniques, and opportunities",IEEE Communications Surveys & Tutorials,,,,https://ieeexplore.ieee.org/abstract/document/10685369/,"Large language models (LLMs) have received considerable attention recently due to their outstanding comprehension and reasoning capabilities, leading to great progress in many fields. The advancement of LLM techniques also offers promising opportunities to automate many tasks in the telecommunication (telecom) field. After pre-training and fine-tuning, LLMs can perform diverse downstream tasks based on human instructions, paving the way to artificial general intelligence (AGI)-enabled 6G. Given the great potential of LLM technologies, this work aims to provide a comprehensive overview of LLM-enabled telecom networks. In particular, we first present LLM fundamentals, including model architecture, pre-training, fine-tuning, inference and utilization, model evaluation, and telecom deployment. Then, we introduce LLM-enabled key techniques and telecom applications in terms of generation, classification, optimization, and prediction problems. Specifically, the LLM-enabled generation applications include telecom domain knowledge, code, and network configuration generation. After that, the LLM-based classification applications involve network security, text, image, and traffic classification problems. Moreover, multiple LLM-enabled optimization techniques are introduced, such as automated reward function design for reinforcement learning and verbal reinforcement learning. Furthermore, for LLM-aided prediction problems, we discussed time-series prediction models and multi-modality prediction problems for telecom. Finally, we highlight the challenges and identify the future directions of LLM-enabled telecom networks.",2024,2025-05-22 09:46:57,2025-05-26 13:44:46,2025-05-22 09:46:45,,,,,,,Large language model (llm) for telecommunications,,,,,,,,,,,,Google Scholar,,Publisher: IEEE,,"/Users/ksoares/Zotero/storage/U5J542YN/Zhou et al. - 2024 - Large language model (llm) for telecommunications A comprehensive survey on principles, key techniq.pdf",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GX7DFJHB,journalArticle,2024.0,"Parker, Michael J.; Anderson, Caitlin; Stone, Claire; Oh, YeaRim",A Large Language Model Approach to Educational Survey Feedback Analysis,International Journal of Artificial Intelligence in Education,,"1560-4292, 1560-4306",10.1007/s40593-024-00414-0,https://link.springer.com/10.1007/s40593-024-00414-0,"Abstract             This paper assesses the potential for the large language models (LLMs) GPT-4 and GPT-3.5 to aid in deriving insight from education feedback surveys. Exploration of LLM use cases in education has focused on teaching and learning, with less exploration of capabilities in education feedback analysis. Survey analysis in education involves goals such as finding gaps in curricula or evaluating teachers, often requiring time-consuming manual processing of textual responses. LLMs have the potential to provide a flexible means of achieving these goals without specialized machine learning models or fine-tuning. We demonstrate a versatile approach to such goals by treating them as sequences of natural language processing (NLP) tasks including classification (multi-label, multi-class, and binary), extraction, thematic analysis, and sentiment analysis, each performed by LLM. We apply these workflows to a real-world dataset of 2500 end-of-course survey comments from biomedical science courses, and evaluate a zero-shot approach (i.e., requiring no examples or labeled training data) across all tasks, reflecting education settings, where labeled data is often scarce. By applying effective prompting practices, we achieve human-level performance on multiple tasks with GPT-4, enabling workflows necessary to achieve typical goals. We also show the potential of inspecting LLMs’ chain-of-thought (CoT) reasoning for providing insight that may foster confidence in practice. Moreover, this study features development of a versatile set of classification categories, suitable for various course types (online, hybrid, or in-person) and amenable to customization. Our results suggest that LLMs can be used to derive a range of insights from survey text.",2024-06-25,2025-05-22 09:46:57,2025-05-22 09:46:57,2025-05-22 09:46:48,,,,,,Int J Artif Intell Educ,,,,,,,,en,,,,,DOI.org (Crossref),,,,/Users/ksoares/Zotero/storage/W8EYKNYB/Parker et al. - 2024 - A Large Language Model Approach to Educational Survey Feedback Analysis.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NDFVLIH5,preprint,2024.0,"Bai, Tianyi; Liang, Hao; Wan, Binwang; Xu, Yanran; Li, Xi; Li, Shiyu; Yang, Ling; Li, Bozhou; Wang, Yifan; Cui, Bin; Huang, Ping; Shan, Jiulong; He, Conghui; Yuan, Binhang; Zhang, Wentao",A Survey of Multimodal Large Language Model from A Data-centric Perspective,,,,10.48550/arXiv.2405.16640,http://arxiv.org/abs/2405.16640,"Multimodal large language models (MLLMs) enhance the capabilities of standard large language models by integrating and processing data from multiple modalities, including text, vision, audio, video, and 3D environments. Data plays a pivotal role in the development and refinement of these models. In this survey, we comprehensively review the literature on MLLMs from a data-centric perspective. Specifically, we explore methods for preparing multimodal data during the pretraining and adaptation phases of MLLMs. Additionally, we analyze the evaluation methods for the datasets and review the benchmarks for evaluating MLLMs. Our survey also outlines potential future research directions. This work aims to provide researchers with a detailed understanding of the data-driven aspects of MLLMs, fostering further exploration and innovation in this field.",2024-07-18,2025-05-22 09:46:57,2025-05-22 09:46:57,2025-05-22 09:46:49,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2405.16640 [cs],,/Users/ksoares/Zotero/storage/9IIWKEQ4/Bai et al. - 2024 - A Survey of Multimodal Large Language Model from A Data-centric Perspective.pdf; /Users/ksoares/Zotero/storage/HW8EIEZK/2405.html,,,Computer Science - Computation and Language; Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Multimedia,,,,,,,,,,,,,,,,,,,arXiv:2405.16640,,,,,,,,,,,,,,,,,,,,,,,,,,,
XNRKDN8K,preprint,2024.0,"Xia, Heming; Yang, Zhe; Dong, Qingxiu; Wang, Peiyi; Li, Yongqi; Ge, Tao; Liu, Tianyu; Li, Wenjie; Sui, Zhifang",Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding,,,,10.48550/arXiv.2401.07851,http://arxiv.org/abs/2401.07851,"To mitigate the high inference latency stemming from autoregressive decoding in Large Language Models (LLMs), Speculative Decoding has emerged as a novel decoding paradigm for LLM inference. In each decoding step, this method first drafts several future tokens efficiently and then verifies them in parallel. Unlike autoregressive decoding, Speculative Decoding facilitates the simultaneous decoding of multiple tokens per step, thereby accelerating inference. This paper presents a comprehensive overview and analysis of this promising decoding paradigm. We begin by providing a formal definition and formulation of Speculative Decoding. Then, we organize in-depth discussions on its key facets, such as drafter selection and verification strategies. Furthermore, we present a comparative analysis of leading methods under third-party testing environments. We aim for this work to serve as a catalyst for further research on Speculative Decoding, ultimately contributing to more efficient LLM inference.",2024-06-04,2025-05-22 09:46:57,2025-05-22 09:46:57,2025-05-22 09:46:51,,,,,,,Unlocking Efficiency in Large Language Model Inference,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2401.07851 [cs],,/Users/ksoares/Zotero/storage/T5QH29JM/Xia et al. - 2024 - Unlocking Efficiency in Large Language Model Inference A Comprehensive Survey of Speculative Decodi.pdf; /Users/ksoares/Zotero/storage/YWN6BSMF/2401.html,,,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,arXiv:2401.07851,,,,,,,,,,,,,,,,,,,,,,,,,,,
3X4SIQUT,preprint,2024.0,"Ye, Jiexia; Zhang, Weiqi; Yi, Ke; Yu, Yongzi; Li, Ziyue; Li, Jia; Tsung, Fugee",A Survey of Time Series Foundation Models: Generalizing Time Series Representation with Large Language Model,,,,10.48550/arXiv.2405.02358,http://arxiv.org/abs/2405.02358,"Time series data are ubiquitous across various domains, making time series analysis critically important. Traditional time series models are task-specific, featuring singular functionality and limited generalization capacity. Recently, large language foundation models have unveiled their remarkable capabilities for cross-task transferability, zero-shot/few-shot learning, and decision-making explainability. This success has sparked interest in the exploration of foundation models to solve multiple time series challenges simultaneously. There are two main research lines, namely pre-training foundation models from scratch for time series and adapting large language foundation models for time series. They both contribute to the development of a unified model that is highly generalizable, versatile, and comprehensible for time series analysis. This survey offers a 3E analytical framework for comprehensive examination of related research. Specifically, we examine existing works from three dimensions, namely Effectiveness, Efficiency and Explainability. In each dimension, we focus on discussing how related works devise tailored solution by considering unique challenges in the realm of time series. Furthermore, we provide a domain taxonomy to help followers keep up with the domain-specific advancements. In addition, we introduce extensive resources to facilitate the field's development, including datasets, open-source, time series libraries. A GitHub repository is also maintained for resource updates (https://github.com/start2020/Awesome-TimeSeries-LLM-FM).",2024-05-07,2025-05-22 09:46:57,2025-05-22 09:46:57,2025-05-22 09:46:53,,,,,,,A Survey of Time Series Foundation Models,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2405.02358 [cs],,/Users/ksoares/Zotero/storage/T3ERKBMM/Ye et al. - 2024 - A Survey of Time Series Foundation Models Generalizing Time Series Representation with Large Langua.pdf; /Users/ksoares/Zotero/storage/YQS87XLW/2405.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,arXiv:2405.02358,,,,,,,,,,,,,,,,,,,,,,,,,,,
NKHQGCW8,journalArticle,2025.0,"Lu, Weizheng; Zhang, Jing; Fan, Ju; Fu, Zihao; Chen, Yueguo; Du, Xiaoyong",Large language model for table processing: a survey,Frontiers of Computer Science,,"2095-2228, 2095-2236",10.1007/s11704-024-40763-6,https://link.springer.com/10.1007/s11704-024-40763-6,"Abstract             Tables, typically two-dimensional and structured to store large amounts of data, are essential in daily activities like database queries, spreadsheet manipulations, Web table question answering, and image table information extraction. Automating these table-centric tasks with Large Language Models (LLMs) or Visual Language Models (VLMs) offers significant public benefits, garnering interest from academia and industry. This survey provides a comprehensive overview of table-related tasks, examining both user scenarios and technical aspects. It covers traditional tasks like table question answering as well as emerging fields such as spreadsheet manipulation and table data analysis. We summarize the training techniques for LLMs and VLMs tailored for table processing. Additionally, we discuss prompt engineering, particularly the use of LLM-powered agents, for various table-related tasks. Finally, we highlight several challenges, including diverse user input when serving and slow thinking using chain-of-thought.",2025-02,2025-05-22 09:46:57,2025-05-22 09:46:57,2025-05-22 09:46:57,192350,,2.0,19.0,,Front. Comput. Sci.,Large language model for table processing,,,,,,,en,,,,,DOI.org (Crossref),,,,/Users/ksoares/Zotero/storage/NTKDS33F/Lu et al. - 2025 - Large language model for table processing a survey.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7LLFJWHN,preprint,2025.0,"Luo, Junyu; Zhang, Weizhi; Yuan, Ye; Zhao, Yusheng; Yang, Junwei; Gu, Yiyang; Wu, Bohan; Chen, Binqi; Qiao, Ziyue; Long, Qingqing; Tu, Rongcheng; Luo, Xiao; Ju, Wei; Xiao, Zhiping; Wang, Yifan; Xiao, Meng; Liu, Chenwu; Yuan, Jingyang; Zhang, Shichang; Jin, Yiqiao; Zhang, Fan; Wu, Xian; Zhao, Hanqing; Tao, Dacheng; Yu, Philip S.; Zhang, Ming","Large Language Model Agent: A Survey on Methodology, Applications and Challenges",,,,10.48550/arXiv.2503.21460,http://arxiv.org/abs/2503.21460,"The era of intelligent agents is upon us, driven by revolutionary advancements in large language models. Large Language Model (LLM) agents, with goal-driven behaviors and dynamic adaptation capabilities, potentially represent a critical pathway toward artificial general intelligence. This survey systematically deconstructs LLM agent systems through a methodology-centered taxonomy, linking architectural foundations, collaboration mechanisms, and evolutionary pathways. We unify fragmented research threads by revealing fundamental connections between agent design principles and their emergent behaviors in complex environments. Our work provides a unified architectural perspective, examining how agents are constructed, how they collaborate, and how they evolve over time, while also addressing evaluation methodologies, tool applications, practical challenges, and diverse application domains. By surveying the latest developments in this rapidly evolving field, we offer researchers a structured taxonomy for understanding LLM agents and identify promising directions for future research. The collection is available at https://github.com/luo-junyu/Awesome-Agent-Papers.",2025-03-27,2025-05-22 09:46:57,2025-05-22 09:46:57,2025-05-22 09:46:57,,,,,,,Large Language Model Agent,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2503.21460 [cs],,"/Users/ksoares/Zotero/storage/XXU8KSZ4/Luo et al. - 2025 - Large Language Model Agent A Survey on Methodology, Applications and Challenges.pdf; /Users/ksoares/Zotero/storage/MY6CX7XV/2503.html",,,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,arXiv:2503.21460,,,,,,,,,,,,,,,,,,,,,,,,,,,
U7S2ZSW6,preprint,2025.0,"Liu, Qidong; Zhao, Xiangyu; Wang, Yuhao; Wang, Yejing; Zhang, Zijian; Sun, Yuqi; Li, Xiang; Wang, Maolin; Jia, Pengyue; Chen, Chong; Huang, Wei; Tian, Feng",Large Language Model Enhanced Recommender Systems: A Survey,,,,10.48550/arXiv.2412.13432,http://arxiv.org/abs/2412.13432,"Large Language Model (LLM) has transformative potential in various domains, including recommender systems (RS). There have been a handful of research that focuses on empowering the RS by LLM. However, previous efforts mainly focus on LLM as RS, which may face the challenge of intolerant inference costs by LLM. Recently, the integration of LLM into RS, known as LLM-Enhanced Recommender Systems (LLMERS), has garnered significant interest due to its potential to address latency and memory constraints in real-world applications. This paper presents a comprehensive survey of the latest research efforts aimed at leveraging LLM to enhance RS capabilities. We identify a critical shift in the field with the move towards incorporating LLM into the online system, notably by avoiding their use during inference. Our survey categorizes the existing LLMERS approaches into three primary types based on the component of the RS model being augmented: Knowledge Enhancement, Interaction Enhancement, and Model Enhancement. We provide an in-depth analysis of each category, discussing the methodologies, challenges, and contributions of recent studies. Furthermore, we highlight several promising research directions that could further advance the field of LLMERS.",2025-03-10,2025-05-22 09:47:55,2025-05-22 09:47:55,2025-05-22 09:47:15,,,,,,,Large Language Model Enhanced Recommender Systems,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2412.13432 [cs],,/Users/ksoares/Zotero/storage/7IMBM6LC/Liu et al. - 2025 - Large Language Model Enhanced Recommender Systems A Survey.pdf; /Users/ksoares/Zotero/storage/HRJG48UD/2412.html,,,Computer Science - Artificial Intelligence; Computer Science - Information Retrieval,,,,,,,,,,,,,,,,,,,arXiv:2412.13432,,,,,,,,,,,,,,,,,,,,,,,,,,,
NB2RXVIA,preprint,2024.0,"Jiang, Xuhui; Tian, Yuxing; Hua, Fengrui; Xu, Chengjin; Wang, Yuanzhuo; Guo, Jian",A Survey on Large Language Model Hallucination via a Creativity Perspective,,,,10.48550/arXiv.2402.06647,http://arxiv.org/abs/2402.06647,"Hallucinations in large language models (LLMs) are always seen as limitations. However, could they also be a source of creativity? This survey explores this possibility, suggesting that hallucinations may contribute to LLM application by fostering creativity. This survey begins with a review of the taxonomy of hallucinations and their negative impact on LLM reliability in critical applications. Then, through historical examples and recent relevant theories, the survey explores the potential creative benefits of hallucinations in LLMs. To elucidate the value and evaluation criteria of this connection, we delve into the definitions and assessment methods of creativity. Following the framework of divergent and convergent thinking phases, the survey systematically reviews the literature on transforming and harnessing hallucinations for creativity in LLMs. Finally, the survey discusses future research directions, emphasizing the need to further explore and refine the application of hallucinations in creative processes within LLMs.",2024-02-02,2025-05-22 09:47:55,2025-05-22 09:47:55,2025-05-22 09:47:21,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2402.06647 [cs],,/Users/ksoares/Zotero/storage/USKB4WMB/Jiang et al. - 2024 - A Survey on Large Language Model Hallucination via a Creativity Perspective.pdf; /Users/ksoares/Zotero/storage/FDVPU338/2402.html,,,Computer Science - Artificial Intelligence; Computer Science - Human-Computer Interaction,,,,,,,,,,,,,,,,,,,arXiv:2402.06647,,,,,,,,,,,,,,,,,,,,,,,,,,,
V3IZ6BN2,journalArticle,2024.0,"Shao, Minghao; Basit, Abdul; Karri, Ramesh; Shafique, Muhammad","Survey of different large language model architectures: Trends, benchmarks, and challenges",IEEE Access,,,,https://ieeexplore.ieee.org/abstract/document/10720163/,"Large Language Models (LLMs) represent a class of deep learning models adept at understanding natural language and generating coherent responses to various prompts or queries. These models far exceed the complexity of conventional neural networks, often encompassing dozens of neural network layers and containing billions to trillions of parameters. They are typically trained on vast datasets, utilizing architectures based on transformer blocks. Present-day LLMs are multi-functional, capable of performing a range of tasks from text generation and language translation to question answering, as well as code generation and analysis. An advanced subset of these models, known as Multimodal Large Language Models (MLLMs), extends LLM capabilities to process and interpret multiple data modalities, including images, audio, and video. This enhancement empowers MLLMs with capabilities like video editing, image comprehension, and captioning for visual content. This survey provides a comprehensive overview of the recent advancements in LLMs. We begin by tracing the evolution of LLMs and subsequently delve into the advent and nuances of MLLMs. We analyze emerging state-of-the-art MLLMs, exploring their technical features, strengths, and limitations. Additionally, we present a comparative analysis of these models and discuss their challenges, potential limitations, and prospects for future development.",2024,2025-05-22 09:47:55,2025-05-26 13:52:15,2025-05-22 09:47:21,,,,,,,Survey of different large language model architectures,,,,,,,,,,,,Google Scholar,,Publisher: IEEE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XLIQ9XD6,journalArticle,2025.0,"Zhu, Xi; Wang, Yu; Gao, Hang; Xu, Wujiang; Wang, Chen; Liu, Zhiwei; Wang, Kun; Jin, Mingyu; Pang, Linsey; Weng, Qingsong",Recommender systems meet large language model agents: A survey,Foundations and Trends® in Privacy and Security,,,10.1561/3300000050,https://www.nowpublishers.com/article/Details/SEC-050,"In recent years, the integration of Large Language Models (LLMs) and Recommender Systems (RS) has revolutionized the way personalized and intelligent user experiences are delivered. This survey provides an extensive review of critical challenges, current landscape, and future directions in the collaboration between LLM-based AI agents (LLM Agent) and recommender systems. We begin with an introduction to the foundational knowledge, exploring the components of LLM agents and the applications of LLMs in recommender systems. The survey then delves into the symbiotic relationship between LLM agents and recommender systems, illustrating how LLM agents enhance recommender systems and how recommender systems support better LLM agents. Specifically, we discuss the overall architectures for designing LLM agents for recommendation, encompassing profile, memory, planning, and action components, along with multi-agent collaboration. Conversely, we investigate how recommender systems contribute to LLM agents, focusing on areas such as memory recommendation, plan recommendation, tool recommendation, agent recommendation, and personalized LLMs and LLM agents. Furthermore, a critical evaluation of trustworthy AI agents and recommender systems follows, addressing key issues of safety, explainability, fairness, and privacy. Finally, we propose potential future research directions, highlighting emerging trends and opportunities in the intersection of AI agents and recommender systems. This survey concludes by summarizing the key insights of current research and outlining promising avenues for future exploration in this rapidly evolving field. A curated collection of relevant papers for this survey is available in the GitHub repository: https://github.com/agiresearch/AgentRecSys.",2025,2025-05-22 09:47:55,2025-05-26 13:51:39,2025-05-22 09:47:23,247–396,,4.0,7.0,,,Recommender systems meet large language model agents,,,,,,,,,,,,Google Scholar,,"Publisher: Now Publishers, Inc.",,/Users/ksoares/Zotero/storage/M2QLEW4R/Zhu et al. - 2025 - Recommender systems meet large language model agents A survey.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JFZI46DN,journalArticle,2025.0,"Schmitt, Valentin J.",Disentangling patent quality: using a large language model for a systematic literature review,Scientometrics,,"0138-9130, 1588-2861",10.1007/s11192-024-05206-w,https://link.springer.com/10.1007/s11192-024-05206-w,"Abstract             Assessing patent quality has long been the subject of research interest due to interchangeable terminology, overlapping indicators, and diverse perspectives. To address these challenges, this study presents a comprehensive framework for assessing patent quality, that draws on stakeholder theory and adopts a multidimensional perspective encompassing economic, legal, and technological dimensions, each of which is clearly defined within the study. Using the capabilities of the large language model GPT-4, a systematic literature review was conducted, analyzing an initial sample of 5141 scientific articles and selecting 762 as relevant. From these selected articles, 985 distinct indicators for assessing patent quality were identified and classifed in accordance with the dimensions of patent quality. The findings reveal that forward citations, family size, and the number of claims are among the most frequently used indicators, highlighting a predominant focus on technological quality in nearly two-thirds of the literature. In addition, the study highlights several challenges in patent quality assessment, such as poor research reproducibility due to inconsistent definitions and applications of indicators such as family size. In response, eight research propositions are proposed, emphasizing the critical evaluation of indicators, the application of sophisticated methods, and the quantification of complex metrics. As a contribution to management and scholarship, this research underscores the complexity of patent quality assessment and provides a structured framework for future studies, emphasizing the importance of a multidimensional perspective. It also illustrates the transformative potential of large language models in enhancing systematic literature reviews, setting a new standard for future research.",2025-01,2025-05-22 09:47:55,2025-05-22 09:47:55,2025-05-22 09:47:24,267-311,,1.0,130.0,,Scientometrics,Disentangling patent quality,,,,,,,en,,,,,DOI.org (Crossref),,,,/Users/ksoares/Zotero/storage/5F3Y4Y7Y/Schmitt - 2025 - Disentangling patent quality using a large language model for a systematic literature review.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VNSYESAE,journalArticle,2025.0,"Geren, Caleb; Board, Amanda; Dagher, Gaby G.; Andersen, Tim; Zhuang, Jun",Blockchain for Large Language Model Security and Safety: A Holistic Survey,ACM SIGKDD Explorations Newsletter,,"1931-0145, 1931-0153",10.1145/3715073.3715075,https://dl.acm.org/doi/10.1145/3715073.3715075,"With the growing development and deployment of large language models (LLMs) in both industrial and academic fields, their security and safety concerns have become increasingly critical. However, recent studies indicate that LLMs face numerous vulnerabilities, including data poisoning, prompt injections, and unauthorized data exposure, which conventional methods have struggled to address fully. In parallel, blockchain technology, known for its data immutability and decentralized structure, offers a promising foundation for safeguarding LLMs. In this survey, we aim to comprehensively assess how to leverage blockchain technology to enhance LLMs' security and safety. Besides, we propose a new taxonomy of blockchain for large language models (BC4LLMs) to systematically categorize related works in this emerging field. Our analysis includes novel frameworks and definitions to delineate security and safety in the context of BC4LLMs, highlighting potential research directions and challenges at this intersection.Through this study, we aim to stimulate targeted advancements in blockchain-integrated LLM security.",2025-01-21,2025-05-22 09:47:55,2025-05-22 09:47:55,2025-05-22 09:47:26,1-20,,2.0,26.0,,SIGKDD Explor. Newsl.,Blockchain for Large Language Model Security and Safety,,,,,,,en,,,,,DOI.org (Crossref),,,,/Users/ksoares/Zotero/storage/HT7PVPJZ/Geren et al. - 2025 - Blockchain for Large Language Model Security and Safety A Holistic Survey.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WZQ75C9Y,preprint,2024.0,"Wang, Shang; Zhu, Tianqing; Liu, Bo; Ding, Ming; Guo, Xu; Ye, Dayong; Zhou, Wanlei; Yu, Philip S.",Unique Security and Privacy Threats of Large Language Model: A Comprehensive Survey,,,,10.48550/arXiv.2406.07973,http://arxiv.org/abs/2406.07973,"With the rapid development of artificial intelligence, large language models (LLMs) have made remarkable advancements in natural language processing. These models are trained on vast datasets to exhibit powerful language understanding and generation capabilities across various applications, including machine translation, chatbots, and agents. However, LLMs have revealed a variety of privacy and security issues throughout their life cycle, drawing significant academic and industrial attention. Moreover, the risks faced by LLMs differ significantly from those encountered by traditional language models. Given that current surveys lack a clear taxonomy of unique threat models across diverse scenarios, we emphasize the unique privacy and security threats associated with five specific scenarios: pre-training, fine-tuning, retrieval-augmented generation systems, deployment, and LLM-based agents. Addressing the characteristics of each risk, this survey outlines potential threats and countermeasures. Research on attack and defense situations can offer feasible research directions, enabling more areas to benefit from LLMs.",2024-06-18,2025-05-22 09:47:55,2025-05-22 09:47:55,2025-05-22 09:47:27,,,,,,,Unique Security and Privacy Threats of Large Language Model,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2406.07973 [cs],,/Users/ksoares/Zotero/storage/QDXS7M4N/Wang et al. - 2024 - Unique Security and Privacy Threats of Large Language Model A Comprehensive Survey.pdf; /Users/ksoares/Zotero/storage/RJDIK5SG/2406.html,,,Computer Science - Cryptography and Security,,,,,,,,,,,,,,,,,,,arXiv:2406.07973,,,,,,,,,,,,,,,,,,,,,,,,,,,
JHEBDR2K,journalArticle,2025.0,"Wang, Xin; Chen, Zirui; Wang, Haofen; Hou U, Leong; Li, Zhao; Guo, Wenbin",Large Language Model Enhanced Knowledge Representation Learning: A Survey,Data Science and Engineering,,"2364-1185, 2364-1541",10.1007/s41019-025-00285-y,https://link.springer.com/10.1007/s41019-025-00285-y,"Abstract             Knowledge Representation Learning (KRL) is crucial for enabling applications of symbolic knowledge from Knowledge Graphs (KGs) to downstream tasks by projecting knowledge facts into vector spaces. Despite their effectiveness in modeling KG structural information, KRL methods are suffering from the sparseness of KGs. The rise of Large Language Models (LLMs) built on the Transformer architecture presents promising opportunities for enhancing KRL by incorporating textual information to address information sparsity in KGs. LLM-enhanced KRL methods, including three key approaches, encoder-based methods that leverage detailed contextual information, encoder-decoder-based methods that utilize a unified Seq2Seq model for comprehensive encoding and decoding, and decoder-based methods that utilize extensive knowledge from large corpora, have significantly advanced the effectiveness and generalization of KRL in addressing a wide range of downstream tasks. This work provides a broad overview of downstream tasks while simultaneously identifying emerging research directions in these evolving domains.",2025-04-07,2025-05-22 09:47:55,2025-05-22 09:47:55,2025-05-22 09:47:30,,,,,,Data Sci. Eng.,Large Language Model Enhanced Knowledge Representation Learning,,,,,,,en,,,,,DOI.org (Crossref),,,,/Users/ksoares/Zotero/storage/5R3IB4EN/Wang et al. - 2025 - Large Language Model Enhanced Knowledge Representation Learning A Survey.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JK7MWNVU,preprint,2024.0,"Ding, Han; Li, Yinheng; Wang, Junhao; Chen, Hang",Large Language Model Agent in Financial Trading: A Survey,,,,10.48550/arXiv.2408.06361,http://arxiv.org/abs/2408.06361,"Trading is a highly competitive task that requires a combination of strategy, knowledge, and psychological fortitude. With the recent success of large language models(LLMs), it is appealing to apply the emerging intelligence of LLM agents in this competitive arena and understanding if they can outperform professional traders. In this survey, we provide a comprehensive review of the current research on using LLMs as agents in financial trading. We summarize the common architecture used in the agent, the data inputs, and the performance of LLM trading agents in backtesting as well as the challenges presented in these research. This survey aims to provide insights into the current state of LLM-based financial trading agents and outline future research directions in this field.",2024-07-26,2025-05-22 09:47:55,2025-05-22 09:47:55,2025-05-22 09:47:33,,,,,,,Large Language Model Agent in Financial Trading,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2408.06361 [q-fin],,/Users/ksoares/Zotero/storage/EQVXLRS6/Ding et al. - 2024 - Large Language Model Agent in Financial Trading A Survey.pdf; /Users/ksoares/Zotero/storage/DRKYSJ6T/2408.html,,,Computer Science - Computation and Language; Quantitative Finance - Trading and Market Microstructure,,,,,,,,,,,,,,,,,,,arXiv:2408.06361,,,,,,,,,,,,,,,,,,,,,,,,,,,
HDDTSN9S,preprint,2025.0,"Wang, Jiayimei; Ni, Tao; Lee, Wei-Bin; Zhao, Qingchuan",A Contemporary Survey of Large Language Model Assisted Program Analysis,,,,10.48550/arXiv.2502.18474,http://arxiv.org/abs/2502.18474,"The increasing complexity of software systems has driven significant advancements in program analysis, as traditional methods unable to meet the demands of modern software development. To address these limitations, deep learning techniques, particularly Large Language Models (LLMs), have gained attention due to their context-aware capabilities in code comprehension. Recognizing the potential of LLMs, researchers have extensively explored their application in program analysis since their introduction. Despite existing surveys on LLM applications in cybersecurity, comprehensive reviews specifically addressing their role in program analysis remain scarce. In this survey, we systematically review the application of LLMs in program analysis, categorizing the existing work into static analysis, dynamic analysis, and hybrid approaches. Moreover, by examining and synthesizing recent studies, we identify future directions and challenges in the field. This survey aims to demonstrate the potential of LLMs in advancing program analysis practices and offer actionable insights for security researchers seeking to enhance detection frameworks or develop domain-specific models.",2025-02-05,2025-05-22 09:47:55,2025-05-22 09:47:55,2025-05-22 09:47:35,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2502.18474 [cs],,/Users/ksoares/Zotero/storage/EIV7AU8L/Wang et al. - 2025 - A Contemporary Survey of Large Language Model Assisted Program Analysis.pdf; /Users/ksoares/Zotero/storage/ZV94LRGR/2502.html,,,Computer Science - Artificial Intelligence; Computer Science - Software Engineering,,,,,,,,,,,,,,,,,,,arXiv:2502.18474,,,,,,,,,,,,,,,,,,,,,,,,,,,
FC5PR2G5,journalArticle,2025.0,"Wang, Peng; Lu, Wenpeng; Lu, Chunlin; Zhou, Ruoxi; Li, Min; Qin, Libo","Large Language Model for Medical Images: A Survey of Taxonomy, Systematic Review, and Future Trends",Big Data Mining and Analytics,,,,https://ieeexplore.ieee.org/abstract/document/10856853/,"The advent of Large Language Models (LLMs) has sparked considerable interest in the medical image domain, as they can generalize to multiple tasks and offer outstanding performance. While LLMs achieve promising results, there is currently a lack of a comprehensive summary of medical images, making it challenging for researchers to understand the progress within this domain. To fill this gap, we make the first attempt to present a comprehensive survey for LLM on medical images. In addition, to better summarize the current progress comprehensively, we further introduce a novel x-stage tuning paradigm for summarization, including zero-stage tuning, one-stage tuning, and multi-stage tuning, offering a unified perspective on LLMs for medical images. Finally, we discuss challenges and future directions in this domain, aiming to spur more breakthroughs in the future. We hope this work can pave the way for the broad application of LLMs in medical images and provide a valuable resource for this domain.",2025,2025-05-22 09:47:55,2025-05-26 13:48:19,2025-05-22 09:47:35,496–517,,2.0,8.0,,,Large Language Model for Medical Images,,,,,,,,,,,,Google Scholar,,Publisher: TUP,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WT4JFGE7,preprint,2024.0,"Ma, Qun; Xue, Xiao; Zhou, Deyu; Yu, Xiangning; Liu, Donghua; Zhang, Xuwen; Zhao, Zihan; Shen, Yifan; Ji, Peilin; Li, Juanjuan; Wang, Gang; Ma, Wanpeng",Computational Experiments Meet Large Language Model Based Agents: A Survey and Perspective,,,,10.48550/arXiv.2402.00262,http://arxiv.org/abs/2402.00262,"Computational experiments have emerged as a valuable method for studying complex systems, involving the algorithmization of counterfactuals. However, accurately representing real social systems in Agent-based Modeling (ABM) is challenging due to the diverse and intricate characteristics of humans, including bounded rationality and heterogeneity. To address this limitation, the integration of Large Language Models (LLMs) has been proposed, enabling agents to possess anthropomorphic abilities such as complex reasoning and autonomous learning. These agents, known as LLM-based Agent, offer the potential to enhance the anthropomorphism lacking in ABM. Nonetheless, the absence of explicit explainability in LLMs significantly hinders their application in the social sciences. Conversely, computational experiments excel in providing causal analysis of individual behaviors and complex phenomena. Thus, combining computational experiments with LLM-based Agent holds substantial research potential. This paper aims to present a comprehensive exploration of this fusion. Primarily, it outlines the historical development of agent structures and their evolution into artificial societies, emphasizing their importance in computational experiments. Then it elucidates the advantages that computational experiments and LLM-based Agents offer each other, considering the perspectives of LLM-based Agent for computational experiments and vice versa. Finally, this paper addresses the challenges and future trends in this research domain, offering guidance for subsequent related studies.",2024-02-01,2025-05-22 09:47:55,2025-05-22 09:47:55,2025-05-22 09:47:37,,,,,,,Computational Experiments Meet Large Language Model Based Agents,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2402.00262 [cs],,/Users/ksoares/Zotero/storage/RWHKJWDC/Ma et al. - 2024 - Computational Experiments Meet Large Language Model Based Agents A Survey and Perspective.pdf; /Users/ksoares/Zotero/storage/W2JYD6KT/2402.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,arXiv:2402.00262,,,,,,,,,,,,,,,,,,,,,,,,,,,
JAGBNNI2,preprint,2024.0,"Shi, Dan; Shen, Tianhao; Huang, Yufei; Li, Zhigen; Leng, Yongqi; Jin, Renren; Liu, Chuang; Wu, Xinwei; Guo, Zishan; Yu, Linhao; Shi, Ling; Jiang, Bojian; Xiong, Deyi",Large Language Model Safety: A Holistic Survey,,,,10.48550/arXiv.2412.17686,http://arxiv.org/abs/2412.17686,"The rapid development and deployment of large language models (LLMs) have introduced a new frontier in artificial intelligence, marked by unprecedented capabilities in natural language understanding and generation. However, the increasing integration of these models into critical applications raises substantial safety concerns, necessitating a thorough examination of their potential risks and associated mitigation strategies. This survey provides a comprehensive overview of the current landscape of LLM safety, covering four major categories: value misalignment, robustness to adversarial attacks, misuse, and autonomous AI risks. In addition to the comprehensive review of the mitigation methodologies and evaluation resources on these four aspects, we further explore four topics related to LLM safety: the safety implications of LLM agents, the role of interpretability in enhancing LLM safety, the technology roadmaps proposed and abided by a list of AI companies and institutes for LLM safety, and AI governance aimed at LLM safety with discussions on international cooperation, policy proposals, and prospective regulatory directions. Our findings underscore the necessity for a proactive, multifaceted approach to LLM safety, emphasizing the integration of technical solutions, ethical considerations, and robust governance frameworks. This survey is intended to serve as a foundational resource for academy researchers, industry practitioners, and policymakers, offering insights into the challenges and opportunities associated with the safe integration of LLMs into society. Ultimately, it seeks to contribute to the safe and beneficial development of LLMs, aligning with the overarching goal of harnessing AI for societal advancement and well-being. A curated list of related papers has been publicly available at https://github.com/tjunlp-lab/Awesome-LLM-Safety-Papers.",2024-12-23,2025-05-22 09:47:55,2025-05-22 09:47:55,2025-05-22 09:47:39,,,,,,,Large Language Model Safety,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2412.17686 [cs],,/Users/ksoares/Zotero/storage/CA8VZWDX/Shi et al. - 2024 - Large Language Model Safety A Holistic Survey.pdf; /Users/ksoares/Zotero/storage/GX3VYWRH/2412.html,,,Computer Science - Computation and Language; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,arXiv:2412.17686,,,,,,,,,,,,,,,,,,,,,,,,,,,
IBWK4XMJ,preprint,2025.0,"Ma, Jing",Causal Inference with Large Language Model: A Survey,,,,10.48550/arXiv.2409.09822,http://arxiv.org/abs/2409.09822,"Causal inference has been a pivotal challenge across diverse domains such as medicine and economics, demanding a complicated integration of human knowledge, mathematical reasoning, and data mining capabilities. Recent advancements in natural language processing (NLP), particularly with the advent of large language models (LLMs), have introduced promising opportunities for traditional causal inference tasks. This paper reviews recent progress in applying LLMs to causal inference, encompassing various tasks spanning different levels of causation. We summarize the main causal problems and approaches, and present a comparison of their evaluation results in different causal scenarios. Furthermore, we discuss key findings and outline directions for future research, underscoring the potential implications of integrating LLMs in advancing causal inference methodologies.",2025-02-09,2025-05-22 09:47:55,2025-05-22 09:47:55,2025-05-22 09:47:41,,,,,,,Causal Inference with Large Language Model,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2409.09822 [cs],,/Users/ksoares/Zotero/storage/G9VZWTWV/Ma - 2025 - Causal Inference with Large Language Model A Survey.pdf; /Users/ksoares/Zotero/storage/48MEWIN5/2409.html,,,Computer Science - Computation and Language; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,arXiv:2409.09822,,,,,,,,,,,,,,,,,,,,,,,,,,,
BEGTI665,journalArticle,2025.0,"Wang, Xuhong; Jiang, Haoyu; Yu, Yi; Yu, Jingru; Lin, Yilun; Yi, Ping; Wang, Yingchun; Qiao, Yu; Li, Li; Wang, Fei-Yue",Building intelligence identification system via large language model watermarking: a survey and beyond,Artificial Intelligence Review,,1573-7462,10.1007/s10462-025-11222-w,https://link.springer.com/10.1007/s10462-025-11222-w,"Large Language Models (LLMs) are increasingly integrated into diverse industries, posing substantial security risks due to unauthorized replication and misuse. To mitigate these concerns, robust identification mechanisms are widely acknowledged as an effective strategy. Identification systems for LLMs now rely heavily on watermarking technology to manage and protect intellectual property and ensure data security. However, previous studies have primarily concentrated on the basic principles of algorithms and lacked a comprehensive analysis of watermarking theory and practice from the perspective of intelligent identification. To bridge this gap, firstly, we explore how a robust identity recognition system can be effectively implemented and managed within LLMs by various participants using watermarking technology. Secondly, we propose a mathematical framework based on mutual information theory, which systematizes the identification process to achieve more precise and customized watermarking. Additionally, we present a comprehensive evaluation of performance metrics for LLM watermarking, reflecting participant preferences and advancing discussions on its identification applications. Lastly, we outline the existing challenges in current watermarking technologies and theoretical frameworks, and provide directional guidance to address these challenges. Our systematic classification and detailed exposition aim to enhance the comparison and evaluation of various methods, fostering further research and development toward a transparent, secure, and equitable LLM ecosystem.",2025-05-15,2025-05-22 09:47:55,2025-05-26 13:42:02,2025-05-22 09:47:44,249,,8.0,58.0,,Artif Intell Rev,Building intelligence identification system via large language model watermarking,,,,,,,en,,,,,DOI.org (Crossref),,,,/Users/ksoares/Zotero/storage/Y22JL5T3/Wang et al. - 2025 - Building intelligence identification system via large language model watermarking a survey and beyo.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PUCW88YM,preprint,2025.0,"Dobslaw, Felix; Feldt, Robert; Yoon, Juyeon; Yoo, Shin",Challenges in Testing Large Language Model Based Software: A Faceted Taxonomy,,,,10.48550/arXiv.2503.00481,http://arxiv.org/abs/2503.00481,"Large Language Models (LLMs) and Multi-Agent LLMs (MALLMs) introduce non-determinism unlike traditional or machine learning software, requiring new approaches to verifying correctness beyond simple output comparisons or statistical accuracy over test datasets. This paper presents a taxonomy for LLM test case design, informed by both the research literature, our experience, and open-source tools that represent the state of practice. We identify key variation points that impact test correctness and highlight open challenges that the research, industry, and open-source communities must address as LLMs become integral to software systems. Our taxonomy defines four facets of LLM test case design, addressing ambiguity in both inputs and outputs while establishing best practices. It distinguishes variability in goals, the system under test, and inputs, and introduces two key oracle types: atomic and aggregated. Our mapping indicates that current tools insufficiently account for these variability points, highlighting the need for closer collaboration between academia and practitioners to improve the reliability and reproducibility of LLM testing.",2025-03-01,2025-05-22 09:47:55,2025-05-22 09:47:55,2025-05-22 09:47:45,,,,,,,Challenges in Testing Large Language Model Based Software,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2503.00481 [cs],,/Users/ksoares/Zotero/storage/JISQHF3J/Dobslaw et al. - 2025 - Challenges in Testing Large Language Model Based Software A Faceted Taxonomy.pdf; /Users/ksoares/Zotero/storage/945WX37H/2503.html,,,Computer Science - Artificial Intelligence; Computer Science - Software Engineering,,,,,,,,,,,,,,,,,,,arXiv:2503.00481,,,,,,,,,,,,,,,,,,,,,,,,,,,
6QYMUI24,preprint,2025.0,"Yue, Murong",A Survey of Large Language Model Agents for Question Answering,,,,10.48550/arXiv.2503.19213,http://arxiv.org/abs/2503.19213,"This paper surveys the development of large language model (LLM)-based agents for question answering (QA). Traditional agents face significant limitations, including substantial data requirements and difficulty in generalizing to new environments. LLM-based agents address these challenges by leveraging LLMs as their core reasoning engine. These agents achieve superior QA results compared to traditional QA pipelines and naive LLM QA systems by enabling interaction with external environments. We systematically review the design of LLM agents in the context of QA tasks, organizing our discussion across key stages: planning, question understanding, information retrieval, and answer generation. Additionally, this paper identifies ongoing challenges and explores future research directions to enhance the performance of LLM agent QA systems.",2025-03-24,2025-05-22 09:47:55,2025-05-22 09:47:55,2025-05-22 09:47:49,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2503.19213 [cs],,/Users/ksoares/Zotero/storage/MH3C9ABB/Yue - 2025 - A Survey of Large Language Model Agents for Question Answering.pdf; /Users/ksoares/Zotero/storage/GNQI3TXP/2503.html,,,Computer Science - Computation and Language; Computer Science - Artificial Intelligence; Computer Science - Human-Computer Interaction,,,,,,,,,,,,,,,,,,,arXiv:2503.19213,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZBJYNJ2U,conferencePaper,2024.0,"Fan, Yihe; Cao, Yuxin; Zhao, Ziyu; Liu, Ziyao; Li, Shaofeng",Unbridled icarus: A survey of the potential perils of image inputs in multimodal large language model security,"2024 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",,,,https://ieeexplore.ieee.org/abstract/document/10831129/,"Multimodal Large Language Models (MLLMs) demonstrate remarkable capabilities that increasingly influence various aspects of our daily lives, constantly defining the new boundary of Artificial General Intelligence (AGI). Image modalities, enriched with profound semantic information and a more continuous mathematical nature compared to other modalities, greatly enhance the functionalities of MLLMs when integrated. However, this integration serves as a double-edged sword, providing attackers with expansive vulnerabilities to exploit for highly covert and harmful attacks. The pursuit of reliable AI systems like powerful MLLMs has emerged as a pivotal area of contemporary research. In this paper, we endeavor to demostrate the multifaceted risks associated with the incorporation of image modalities into MLLMs. Initially, we delineate the foundational components and training processes of MLLMs. Subsequently, we construct a threat model, outlining the security vulnerabilities intrinsic to MLLMs. Moreover, we analyze and summarize existing scholarly discourses on MLLMs' attack and defense mechanisms, culminating in suggestions for the future research on MLLM security. Through this comprehensive analysis, we aim to deepen the academic understanding of MLLM security challenges and propel forward the development of trustworthy MLLM systems.",2024,2025-05-22 09:47:55,2025-05-26 13:54:41,2025-05-22 09:47:51,3428–3433,,,,,,Unbridled icarus,,,,,IEEE,,,,,,,Google Scholar,,,,/Users/ksoares/Zotero/storage/VYBQMBHZ/Fan et al. - 2024 - Unbridled icarus A survey of the potential perils of image inputs in multimodal large language mode.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RWJ3DN2L,preprint,2025.0,"Chen, Chaoran; Yao, Bingsheng; Zou, Ruishi; Hua, Wenyue; Lyu, Weimin; Ye, Yanfang; Li, Toby Jia-Jun; Wang, Dakuo",Towards a Design Guideline for RPA Evaluation: A Survey of Large Language Model-Based Role-Playing Agents,,,,10.48550/arXiv.2502.13012,http://arxiv.org/abs/2502.13012,"Role-Playing Agent (RPA) is an increasingly popular type of LLM Agent that simulates human-like behaviors in a variety of tasks. However, evaluating RPAs is challenging due to diverse task requirements and agent designs. This paper proposes an evidence-based, actionable, and generalizable evaluation design guideline for LLM-based RPA by systematically reviewing 1,676 papers published between Jan. 2021 and Dec. 2024. Our analysis identifies six agent attributes, seven task attributes, and seven evaluation metrics from existing literature. Based on these findings, we present an RPA evaluation design guideline to help researchers develop more systematic and consistent evaluation methods.",2025-03-27,2025-05-22 09:47:55,2025-05-22 09:47:55,2025-05-22 09:47:53,,,,,,,Towards a Design Guideline for RPA Evaluation,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2502.13012 [cs],,/Users/ksoares/Zotero/storage/E6CEGJUL/Chen et al. - 2025 - Towards a Design Guideline for RPA Evaluation A Survey of Large Language Model-Based Role-Playing A.pdf; /Users/ksoares/Zotero/storage/PNFJJUC2/2502.html,,,Computer Science - Computation and Language; Computer Science - Human-Computer Interaction,,,,,,,,,,,,,,,,,,,arXiv:2502.13012,,,,,,,,,,,,,,,,,,,,,,,,,,,
VV77THWJ,journalArticle,2025.0,"Guo, Zhiqi; Tang, Fengxiao; Luo, Linfeng; Zhao, Ming; Kato, Nei",A Survey on Applications of Large Language Model-Driven Digital Twins for Intelligent Network Optimization,IEEE Communications Surveys & Tutorials,,,,https://ieeexplore.ieee.org/abstract/document/10994494/,"With the widespread application of digital twin (DT) technology in network optimization and intelligent management, its integration with large language models (LLMs) presents immense potential. LLMs excel in natural language processing, multimodal analysis, and real-time optimization, enabling innovative solutions for intelligent monitoring, resource allocation, and decision-making in complex network environments. This paper systematically reviews the development of DTs and LLMs, elaborates on their core principles and application scenarios, and examines the capabilities of LLM-driven DTs in key network optimization tasks, including traffic prediction, fault diagnosis, resource allocation, and multi-objective optimization. By leveraging real-time data from DTs, LLMs can dynamically generate optimization strategies, enabling precise monitoring and intelligent tuning. Furthermore, this paper explores the potential of integrating LLMs and DTs to address complex challenges such as data quality, latency sensitivity, and energy consumption demands, while summarizing existing technical bottlenecks. Finally, the paper proposes several potential research directions to address these challenges, offering a comprehensive perspective for advancing the efficiency and automation of next-generation intelligent networks.",2025,2025-05-22 09:47:55,2025-05-26 13:38:53,2025-05-22 09:47:55,,,,,,,,,,,,,,,,,,,Google Scholar,,Publisher: IEEE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
N3MPYQP7,conferencePaper,2025.0,"Singh, Aditi; Shetty, Akash; Ehtesham, Abul; Kumar, Saket; Khoei, Tala Talaei","A Survey of Large Language Model-Based Generative AI for Text-to-SQL: Benchmarks, Applications, Use Cases, and Challenges",2025 IEEE 15th Annual Computing and Communication Workshop and Conference (CCWC),,,,https://ieeexplore.ieee.org/abstract/document/10903689/,"Text-to-SQL systems facilitate smooth interaction with databases by translating natural language queries into Structured Query Language (SQL), bridging the gap between non-technical users and complex database management systems. This survey provides a comprehensive overview of the evolution of AI-driven text-to-SQL systems, highlighting their foundational components, advancements in large language model (LLM) architectures, and the critical role of datasets such as Spider, WikiSQL, and CoSQL in driving progress. We examine the applications of text-to-SQL in domains like healthcare, education, and finance, emphasizing their transformative potential for improving data accessibility. Additionally, we analyze persistent challenges, including domain generalization, query optimization, support for multi-turn conversational interactions, and the limited availability of datasets tailored for NoSQL databases and dynamic real-world scenarios. To address these challenges, we outline future research directions, such as extending text-to-SQL capabilities to support NoSQL databases, designing datasets for dynamic multi-turn interactions, and optimizing systems for real-world scalability and robustness. By surveying current advancements and identifying key gaps, this paper aims to guide the next generation of research and applications in LLM-based text-to-SQL systems.",2025,2025-05-22 09:49:53,2025-05-26 13:38:26,2025-05-22 09:49:20,00015–00021,,,,,,A Survey of Large Language Model-Based Generative AI for Text-to-SQL,,,,,IEEE,,,,,,,Google Scholar,,,,"/Users/ksoares/Zotero/storage/Z8GDVHCW/Singh et al. - 2025 - A Survey of Large Language Model-Based Generative AI for Text-to-SQL Benchmarks, Applications, Use.pdf",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JTHKZLGG,journalArticle,2025.0,"Guizani, Sghaier; Mazhar, Tehseen; Shahzad, Tariq; Ahmad, Wasim; Bibi, Afsha; Hamam, Habib",A systematic literature review to implement large language model in higher education: issues and solutions,Discover Education,,2731-5525,10.1007/s44217-025-00424-7,https://link.springer.com/10.1007/s44217-025-00424-7,"Artificial intelligence-driven Chatbots, especially large language models (LLMs) like GPT-4, represent significant progress in digital education. These models excel in mimicking human-like text and transforming learning and teaching methods. This study examines the development, application, and impact of LLMs in education. It highlights their role in automating instructional tasks and promoting personalized learning experiences. Despite integration concerns and ethical debates, LLMs showcase the potential of AI to improve educational practices. Our research concludes that LLMs offer transformative opportunities for education. However, their incorporation requires careful ethical considerations, data privacy measures, and a balance between human educators and AI technologies. The findings suggest strategies for integrating LLMs into educational frameworks to enhance learning outcomes while preserving educational integrity.",2025-02-15,2025-05-22 09:49:53,2025-05-26 13:41:29,2025-05-22 09:49:24,35,,1.0,4.0,,Discov Educ,A systematic literature review to implement large language model in higher education,,,,,,,en,,,,,DOI.org (Crossref),,,,/Users/ksoares/Zotero/storage/DUQ72E7Y/Guizani et al. - 2025 - A systematic literature review to implement large language model in higher education issues and sol.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7MSNXVC7,preprint,2025.0,"Zhang, Yu; Qiao, Shutong; Zhang, Jiaqi; Lin, Tzu-Heng; Gao, Chen; Li, Yong",A Survey of Large Language Model Empowered Agents for Recommendation and Search: Towards Next-Generation Information Retrieval,,,,10.48550/arXiv.2503.05659,http://arxiv.org/abs/2503.05659,"Information technology has profoundly altered the way humans interact with information. The vast amount of content created, shared, and disseminated online has made it increasingly difficult to access relevant information. Over the past two decades, recommender systems and search (collectively referred to as information retrieval systems) have evolved significantly to address these challenges. Recent advances in large language models (LLMs) have demonstrated capabilities that surpass human performance in various language-related tasks and exhibit general understanding, reasoning, and decision-making abilities. This paper explores the transformative potential of LLM agents in enhancing recommender and search systems. We discuss the motivations and roles of LLM agents, and establish a classification framework to elaborate on the existing research. We highlight the immense potential of LLM agents in addressing current challenges in recommendation and search, providing insights into future research directions. This paper is the first to systematically review and classify the research on LLM agents in these domains, offering a novel perspective on leveraging this advanced AI technology for information retrieval. To help understand the existing works, we list the existing papers on LLM agent based recommendation and search at this link: https://github.com/tsinghua-fib-lab/LLM-Agent-for-Recommendation-and-Search.",2025-04-11,2025-05-22 09:49:53,2025-05-22 09:49:53,2025-05-22 09:49:26,,,,,,,A Survey of Large Language Model Empowered Agents for Recommendation and Search,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2503.05659 [cs],,/Users/ksoares/Zotero/storage/8C7L2VRE/Zhang et al. - 2025 - A Survey of Large Language Model Empowered Agents for Recommendation and Search Towards Next-Genera.pdf; /Users/ksoares/Zotero/storage/NFDF6JWR/2503.html,,,Computer Science - Information Retrieval,,,,,,,,,,,,,,,,,,,arXiv:2503.05659,,,,,,,,,,,,,,,,,,,,,,,,,,,
BXAGH7ME,preprint,2024.0,"Sun, Maojun; Han, Ruijian; Jiang, Binyan; Qi, Houduo; Sun, Defeng; Yuan, Yancheng; Huang, Jian",A Survey on Large Language Model-based Agents for Statistics and Data Science,,,,10.48550/arXiv.2412.14222,http://arxiv.org/abs/2412.14222,"In recent years, data science agents powered by Large Language Models (LLMs), known as ""data agents,"" have shown significant potential to transform the traditional data analysis paradigm. This survey provides an overview of the evolution, capabilities, and applications of LLM-based data agents, highlighting their role in simplifying complex data tasks and lowering the entry barrier for users without related expertise. We explore current trends in the design of LLM-based frameworks, detailing essential features such as planning, reasoning, reflection, multi-agent collaboration, user interface, knowledge integration, and system design, which enable agents to address data-centric problems with minimal human intervention. Furthermore, we analyze several case studies to demonstrate the practical applications of various data agents in real-world scenarios. Finally, we identify key challenges and propose future research directions to advance the development of data agents into intelligent statistical analysis software.",2024-12-18,2025-05-22 09:49:53,2025-05-22 09:49:53,2025-05-22 09:49:27,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2412.14222 [cs],,/Users/ksoares/Zotero/storage/UY6YMTMT/Sun et al. - 2024 - A Survey on Large Language Model-based Agents for Statistics and Data Science.pdf; /Users/ksoares/Zotero/storage/N4QAF23S/2412.html,,,Computer Science - Computation and Language; Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Other Statistics,,,,,,,,,,,,,,,,,,,arXiv:2412.14222,,,,,,,,,,,,,,,,,,,,,,,,,,,
YJUIXT2K,journalArticle,2024.0,"Zhu, Xiaohu; Li, Qian; Cui, Lizhen; Liu, Yongkang",Large language model enhanced text-to-sql generation: A survey,arXiv preprint arXiv:2410.06011,,,,https://arxiv.org/abs/2410.06011,"Abstract—Text-to-SQL translates natural language queries into Structured Query Language (SQL) commands, enabling users to interact with databases using natural language. Essentially, the text-to-SQL task is a text generation task and its development primarily dependent on changes in language models. Especially with the rapid development of Large Language Models (LLMs), the pattern of text-to-SQL has undergone significant changes. Existing survey work mainly focuses on rule-based and neural-based approaches, still lacking a survey of Text-to-SQL with LLMs. In this paper, we survey the large language model enhanced text-to-SQL generations, classifying them into prompt engineering, fine-tuning, pre-trained and Agent groups according to training strategies. And we also summarize datasets and evaluation metrics comprehensively. This survey could help people better understand the pattern, research status, and challenges of LLM-based text-to-SQL generations.",2024,2025-05-22 09:49:53,2025-05-26 13:48:09,2025-05-22 09:49:29,,,,,,,Large language model enhanced text-to-sql generation,,,,,,,,,,,,Google Scholar,,,,/Users/ksoares/Zotero/storage/SR2HQ5S8/Zhu et al. - 2024 - Large language model enhanced text-to-sql generation A survey.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UBD59NLQ,journalArticle,2024.0,"Li, Haoyang; Li, Yiming; Tian, Anxin; Tang, Tianhao; Xu, Zhanchao; Chen, Xuejia; Hu, Nicole; Dong, Wei; Li, Qing; Chen, Lei",A survey on large language model acceleration based on kv cache management,arXiv preprint arXiv:2412.19442,,,,https://arxiv.org/abs/2412.19442,"Large Language Models (LLMs) have revolutionized a wide range of domains such as natural language processing, computer vision, and multi-modal tasks due to their ability to comprehend context and perform logical reasoning. However, the computational and memory demands of LLMs, particularly during inference, pose significant challenges when scaling them to real-world, long-context, and real-time applications. Key-Value (KV) cache management has emerged as a critical optimization technique for accelerating LLM inference by reducing redundant computations and improving memory utilization. This survey provides a comprehensive overview of KV cache management strategies for LLM acceleration, categorizing them into token-level, modellevel, and system-level optimizations. Token-level strategies include KV cache selection, budget allocation, merging, quantization, and low-rank decomposition, while model-level optimizations focus on architectural innovations and attention mechanisms to enhance KV reuse. System-level approaches address memory management, scheduling, and hardwareaware designs to improve efficiency across diverse computing environments. Additionally, the survey provides an overview of both text and multimodal datasets and benchmarks used to evaluate these strategies. By presenting detailed taxonomies and comparative analyses, this work aims to offer useful insights for researchers and practitioners to support the development of efficient and scalable KV cache management techniques, contributing to the practical deployment of LLMs in real-world applications. The curated paper list for KV cache management is in: https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management.",2024,2025-05-22 09:49:53,2025-05-26 13:39:43,2025-05-22 09:49:31,,,,,,,,,,,,,,,,,,,Google Scholar,,,,/Users/ksoares/Zotero/storage/K5MCYNID/Li et al. - 2024 - A survey on large language model acceleration based on kv cache management.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
STSN6LXU,journalArticle,2025.0,"Khan, Javed Ali; Qayyum, Shamaila; Dar, Hafsa Shareef",Large Language Model for Requirements Engineering: A Systematic Literature Review,,,,,https://www.researchsquare.com/article/rs-5589929/latest,"Large language models (LLMs) have been applied to various domains, including software engineering (SE), with comparatively better success. Recently, researchers have started exploring the capabilities of LLMs in requirements engineering (RE) by proposing various approaches for automating RE activities. Moreover, a comprehensive understanding of LLM application in various RE-related activities is still emerging and growing. For this purpose, we conducted a systematic literature review (SLR) by exploring LLMs applications in RE (LLM4RE), aiming to understand how LLMs can help improve various RE activities, such as requirements elicitation, analysis, modelling, validation, specification, prioritization, and tracing. Using inclusion and exclusion criteria to answer various research questions, we identify and critically analyze 35 research papers from 2023 to October 2024. In particular, we are interested in answering LLMs application in various RE activities, identifying various LLM strategies to train LLMs for particular RE activities, determining what different evaluation matrices are used to evaluate LLMs performance, and determining which RE activities have been effectively answered to date. Through the proposed research questions, we identified that several RE activities have been comparatively improved and possibly automated. Also, various evaluation metrics have been identified that effectively evaluate the outputs generated by the LLMs. The study can prove to be effective for software vendors and researchers in automating various RE-related activities for improved software quality and user satisfaction.",2025,2025-05-22 09:49:53,2025-05-26 13:48:27,2025-05-22 09:49:33,,,,,,,Large Language Model for Requirements Engineering,,,,,,,,,,,,Google Scholar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4SPUM37I,preprint,2024.0,"Zeng, Pai; Ning, Zhenyu; Zhao, Jieru; Cui, Weihao; Xu, Mengwei; Guo, Liwei; Chen, Xusheng; Shan, Yizhou",The CAP Principle for LLM Serving: A Survey of Long-Context Large Language Model Serving,,,,10.48550/arXiv.2405.11299,http://arxiv.org/abs/2405.11299,"We survey the large language model (LLM) serving area to understand the intricate dynamics between cost-efficiency and accuracy, which is magnified by the growing need for longer contextual understanding when deploying models at a massive scale. Our findings reveal that works in this space optimize along three distinct but conflicting goals: improving serving context length (C), improving serving accuracy (A), and improving serving performance (P). Drawing inspiration from the CAP theorem in databases, we propose a CAP principle for LLM serving, which suggests that any optimization can improve at most two of these three goals simultaneously. Our survey categorizes existing works within this framework. We find the definition and continuity of user-perceived measurement metrics are crucial in determining whether a goal has been met, akin to prior CAP databases in the wild. We recognize the CAP principle for LLM serving as a guiding principle, rather than a formal theorem, to inform designers of the inherent and dynamic trade-offs in serving models. As serving accuracy and performance have been extensively studied, this survey focuses on works that extend serving context length and address the resulting challenges.",2024-05-27,2025-05-22 09:49:53,2025-05-22 09:49:53,2025-05-22 09:49:35,,,,,,,The CAP Principle for LLM Serving,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2405.11299 [cs],,/Users/ksoares/Zotero/storage/QQYM3L6L/Zeng et al. - 2024 - The CAP Principle for LLM Serving A Survey of Long-Context Large Language Model Serving.pdf; /Users/ksoares/Zotero/storage/C9PEJMN4/2405.html,,,Computer Science - Machine Learning; Computer Science - Databases,,,,,,,,,,,,,,,,,,,arXiv:2405.11299,,,,,,,,,,,,,,,,,,,,,,,,,,,
9Z7TYLAF,journalArticle,2024.0,"Lou, Renze; Zhang, Kai; Yin, Wenpeng",Large language model instruction following: A survey of progresses and challenges,Computational Linguistics,,,,https://direct.mit.edu/coli/article-pdf/doi/10.1162/coli_a_00523/2464827/coli_a_00523.pdf,"Task semantics can be expressed by a set of input-output examples or a piece of textual instruction. Conventional machine learning approaches for natural language processing (NLP) mainly rely on the availability of large-scale sets of task-specific examples. Two issues arise: First, collecting task-specific labeled examples does not apply to scenarios where tasks may be too complicated or costly to annotate, or the system is required to handle a new task immediately; second, this is not user-friendly since end-users are probably more willing to provide task description rather than a set of examples before using the system. Therefore, the community is paying increasing interest in a new supervision-seeking paradigm for NLP: learning to follow task instructions, that is, instruction following. Despite its impressive progress, there are some unsolved research equations that the community struggles with. This survey tries to summarize and provide insights into the current research on instruction following, particularly, by answering the following questions: (i) What is task instruction, and what instruction types exist? (ii) How should we model instructions? (iii) What are popular instruction following datasets and evaluation metrics? (iv) What factors influence and explain the instructions performance? (v) What challenges remain in instruction following? To our knowledge, this is the first comprehensive survey about instruction following.",2024,2025-05-22 09:49:53,2025-05-26 13:49:08,2025-05-22 09:49:35,1053–1095,,3.0,50.0,,,Large language model instruction following,,,,,,,,,,,,Google Scholar,,"Publisher: MIT Press 255 Main Street, 9th Floor, Cambridge, Massachusetts 02142, USA …",,/Users/ksoares/Zotero/storage/BWH4CPAZ/Lou et al. - 2024 - Large language model instruction following A survey of progresses and challenges.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TJZBYPFR,journalArticle,2025.0,"Liang, Xinxin; Wang, Zuoxu; Liu, Jihong",A survey of large language model-augmented knowledge graphs for advanced complex product design,Journal of Manufacturing Systems,,,,https://www.sciencedirect.com/science/article/pii/S0278612525001050,"In the Human-AI collaboration rapid development era, the design and development of knowledge-intensive complex products should enable the design process with the help of advanced AI technology, and enhance the reasoning and application of design domain knowledge. Extracting and reusing domain knowledge would greatly facilitate the success of complex product design. Knowledge graphs (KGs), a powerful knowledge representation and storage technology, have been widely deployed in advanced complex product design because of their advantages in mining and applying large-scale, complex, and specialized domain knowledge. But merely KG and its related reasoning approaches still cannot fully support the ill-defined product design tasks. In the future complex product design, Human-AI collaboration will become a mainstream prevention trend. Large language models (LLMs) have outstanding performance in natural language understanding and generation, showing promising potential to collaborate with KGs in complex product design and development. Till 2024/03/04, only a few studies have systematically reviewed the current status of LLM and KG applications in the engineering field, not to mention a further detailed review in the complex product design field, leaving many issues not covered or fully examined. To fill this gap, 100 articles published in the last 4 years (i.e., 2021–2024) were screened and surveyed. This study provides a statistical analysis of the screened research articles, mainstream techniques of LLM & KG, and LLM & KG applications were analyze. To understand how KG and LLM could support complex product design, a framework of LLMs-augmented KG in advanced complex product design was proposed, which contains data layer, KG & LLM collaboration layer, enhanced design capability layer, and design task layer. Furthermore, we also discussed the challenges and future research directions of the LLM-KG-collaborated complex product design paradigm. As an exploratory review paper, it provides insightful ideas for implementing more specialized domain KGs in product design field.",2025,2025-05-22 09:49:53,2025-05-26 13:38:19,2025-05-22 09:49:37,883–901,,,80.0,,,,,,,,,,,,,,,Google Scholar,,Publisher: Elsevier,,/Users/ksoares/Zotero/storage/F4ESRTTD/S0278612525001050.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
J3G763II,preprint,2025.0,"Zou, Henry Peng; Huang, Wei-Chieh; Wu, Yaozu; Chen, Yankai; Miao, Chunyu; Nguyen, Hoang; Zhou, Yue; Zhang, Weizhi; Fang, Liancheng; He, Langzhou; Li, Yangning; Li, Dongyuan; Jiang, Renhe; Liu, Xue; Yu, Philip S.",A Survey on Large Language Model based Human-Agent Systems,,,,10.48550/arXiv.2505.00753,http://arxiv.org/abs/2505.00753,"Recent advances in large language models (LLMs) have sparked growing interest in building fully autonomous agents. However, fully autonomous LLM-based agents still face significant challenges, including limited reliability due to hallucinations, difficulty in handling complex tasks, and substantial safety and ethical risks, all of which limit their feasibility and trustworthiness in real-world applications. To overcome these limitations, LLM-based human-agent systems (LLM-HAS) incorporate human-provided information, feedback, or control into the agent system to enhance system performance, reliability and safety. This paper provides the first comprehensive and structured survey of LLM-HAS. It clarifies fundamental concepts, systematically presents core components shaping these systems, including environment & profiling, human feedback, interaction types, orchestration and communication, explores emerging applications, and discusses unique challenges and opportunities. By consolidating current knowledge and offering a structured overview, we aim to foster further research and innovation in this rapidly evolving interdisciplinary field. Paper lists and resources are available at https://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-Systems.",2025-05-20,2025-05-22 09:49:53,2025-05-22 09:49:53,2025-05-22 09:49:39,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2505.00753 [cs],,/Users/ksoares/Zotero/storage/G2SMP64U/Zou et al. - 2025 - A Survey on Large Language Model based Human-Agent Systems.pdf; /Users/ksoares/Zotero/storage/QC88KGL6/2505.html,,,Computer Science - Computation and Language; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,arXiv:2505.00753,,,,,,,,,,,,,,,,,,,,,,,,,,,
B9CYWRK7,journalArticle,2025.0,"Scherbakov, Dmitry; Hubig, Nina; Jansari, Vinita; Bakumenko, Alexander; Lenert, Leslie A.",The emergence of large language models as tools in literature reviews: a large language model-assisted systematic review,Journal of the American Medical Informatics Association,,,,https://academic.oup.com/jamia/advance-article-abstract/doi/10.1093/jamia/ocaf063/8126534,"Objectives: This study aims to summarize the usage of large language models (LLMs) in the process of creating a scientific review by looking at the methodological papers that describe the use of LLMs in review automation and the review papers that mention they were made with the support of LLMs.  Materials and Methods: The search was conducted in June 2024 in PubMed, Scopus, Dimensions, and Google Scholar by human reviewers. Screening and extraction process took place in Covidence with the help of LLM add-on based on the OpenAI GPT-4o model. ChatGPT and Scite.ai were used in cleaning the data, generating the code for figures, and drafting the manuscript.  Results: Of the 3788 articles retrieved, 172 studies were deemed eligible for the final review. ChatGPT and GPT-based LLM emerged as the most dominant architecture for review automation (n 1⁄4 126, 73.2%). A significant number of review automation projects were found, but only a limited number of papers (n 1⁄4 26, 15.1%) were actual reviews that acknowledged LLM usage. Most citations focused on the automation of a particular stage of review, such as Searching for publications (n 1⁄4 60, 34.9%) and Data extraction (n 1⁄4 54, 31.4%). When comparing the pooled performance of GPT-based and BERT-based models, the former was better in data extraction with a mean precision of 83.0% (SD 1⁄4 10.4) and a recall of 86.0% (SD 1⁄4 9.8).  Discussion and Conclusion: Our LLM-assisted systematic review revealed a significant number of research projects related to review automation using LLMs. Despite limitations, such as lower accuracy of extraction for numeric data, we anticipate that LLMs will soon change the way scientific reviews are conducted.",2025,2025-05-22 09:49:53,2025-05-26 13:53:02,2025-05-22 09:49:41,ocaf063,,,,,,The emergence of large language models as tools in literature reviews,,,,,,,,,,,,Google Scholar,,Publisher: Oxford University Press,,/Users/ksoares/Zotero/storage/4R5AJY44/Scherbakov et al. - 2025 - The emergence of large language models as tools in literature reviews a large language model-assist.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JCSK6VDM,journalArticle,2025.0,"Guo, Huijie; Xing, Xudong; Zhou, Yongjie; Jiang, Wenjiao; Chen, Xiaoyi; Wang, Ting; Jiang, Zixuan; Wang, Yibing; Hou, Junyan; Jiang, Yukun",A Survey of Large Language Model for Drug Research and Development,IEEE Access,,,,https://ieeexplore.ieee.org/abstract/document/10930479/,"Drug research and development (drug R&D) is a sophisticated, cost-intensive, and time-consuming procedure with historically low success rates. The advent of Artificial Intelligence (AI) technologies has introduced innovative methods into drug R&D, particularly by leveraging AI capabilities. Large language models (LLMs), a breakthrough in generative AI, have revolutionized drug discovery. With their extensive datasets, numerous parameters, and strong multitasking abilities, LLMs have significantly improved efficiency across various related domains, providing unparalleled support to drug R&D. These models have facilitated a deeper understanding of intricate disease mechanisms and the identification of novel therapeutic strategies, ushering in a new era in drug development and clinical applications. As a result, the advancement of LLMs is poised to drive significant transformations in drug R&D, emphasizing the importance of effectively leveraging this technology. This review provides insights into the architecture and characteristics of LLMs, explores their applications in drug R&D, and highlights their research implications in bioinformatics data, including proteins, genes, and chemical compounds. Furthermore, it investigates the practical strategies of LLMs in drug discovery, drug repositioning, and clinical inquiries, presenting an innovative approach to research and future advancements in this field.",2025,2025-05-22 09:49:53,2025-05-26 13:37:28,2025-05-22 09:49:45,,,,,,,,,,,,,,,,,,,Google Scholar,,Publisher: IEEE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BTFLSBBC,journalArticle,2025.0,"Sargeant, Holli; Izzidien, Ahmed; Steffek, Felix",Topic Classification of Case Law Using a Large Language Model and a New Taxonomy for UK Law: AI Insights into Summary Judgment,Artificial Intelligence and Law,,"0924-8463, 1572-8382",10.1007/s10506-025-09434-0,http://arxiv.org/abs/2405.12910,"This paper addresses a critical gap in legal analytics by developing and applying a novel taxonomy for topic classification of summary judgment cases in the United Kingdom. Using a curated dataset of summary judgment cases, we use the Large Language Model Claude 3 Opus to explore functional topics and trends. We find that Claude 3 Opus correctly classified the topic with an accuracy of 87.13% and an F1 score of 0.87. The analysis reveals distinct patterns in the application of summary judgments across various legal domains. As case law in the United Kingdom is not originally labelled with keywords or a topic filtering option, the findings not only refine our understanding of the thematic underpinnings of summary judgments but also illustrate the potential of combining traditional and AI-driven approaches in legal classification. Therefore, this paper provides a new and general taxonomy for UK law. The implications of this work serve as a foundation for further research and policy discussions in the field of judicial administration and computational legal research methodologies.",2025-02-25,2025-05-22 09:49:53,2025-05-22 09:49:53,2025-05-22 09:49:46,,,,,,Artif Intell Law,Topic Classification of Case Law Using a Large Language Model and a New Taxonomy for UK Law,,,,,,,,,,,,arXiv.org,,arXiv:2405.12910 [cs],,/Users/ksoares/Zotero/storage/VSMD3V3I/Sargeant et al. - 2025 - Topic Classification of Case Law Using a Large Language Model and a New Taxonomy for UK Law AI Insi.pdf; /Users/ksoares/Zotero/storage/W7Q45VWZ/2405.html,,,Computer Science - Computation and Language; Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZX7X7VJL,preprint,2025.0,"Chen, Zihan; Wang, Song; Tan, Zhen; Fu, Xingbo; Lei, Zhenyu; Wang, Peng; Liu, Huan; Shen, Cong; Li, Jundong",A Survey of Scaling in Large Language Model Reasoning,,,,10.48550/arXiv.2504.02181,http://arxiv.org/abs/2504.02181,"The rapid advancements in large Language models (LLMs) have significantly enhanced their reasoning capabilities, driven by various strategies such as multi-agent collaboration. However, unlike the well-established performance improvements achieved through scaling data and model size, the scaling of reasoning in LLMs is more complex and can even negatively impact reasoning performance, introducing new challenges in model alignment and robustness. In this survey, we provide a comprehensive examination of scaling in LLM reasoning, categorizing it into multiple dimensions and analyzing how and to what extent different scaling strategies contribute to improving reasoning capabilities. We begin by exploring scaling in input size, which enables LLMs to process and utilize more extensive context for improved reasoning. Next, we analyze scaling in reasoning steps that improves multi-step inference and logical consistency. We then examine scaling in reasoning rounds, where iterative interactions refine reasoning outcomes. Furthermore, we discuss scaling in training-enabled reasoning, focusing on optimization through iterative model improvement. Finally, we review applications of scaling across domains and outline future directions for further advancing LLM reasoning. By synthesizing these diverse perspectives, this survey aims to provide insights into how scaling strategies fundamentally enhance the reasoning capabilities of LLMs and further guide the development of next-generation AI systems.",2025-04-02,2025-05-22 09:49:53,2025-05-22 09:49:53,2025-05-22 09:49:47,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2504.02181 [cs],,/Users/ksoares/Zotero/storage/HH767B39/Chen et al. - 2025 - A Survey of Scaling in Large Language Model Reasoning.pdf; /Users/ksoares/Zotero/storage/IHWWPXV9/2504.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,arXiv:2504.02181,,,,,,,,,,,,,,,,,,,,,,,,,,,
TRRHYYDA,journalArticle,2025.0,"Ozkan, Ecem; Tekin, Aysun; Ozkan, Mahmut Can; Cabrera, Daniel; Niven, Alexander; Dong, Yue",Global Health care Professionals’ Perceptions of Large Language Model Use In Practice: Cross-Sectional Survey Study,JMIR Medical Education,,,,https://mededu.jmir.org/2025/1/e58801,"Abstract Background: ChatGPT is a large language model-based chatbot developed by OpenAI. ChatGPT has many potential applications to health care, including enhanced diagnostic accuracy and efficiency, improved treatment planning, and better patient outcomes. However, health care professionals’ perceptions of ChatGPT and similar artificial intelligence tools are not well known. Understanding these attitudes is important to inform the best approaches to exploring their use in medicine. Objective: Our aim was to evaluate the health care professionals’ awareness and perceptions regarding potential applications of ChatGPT in the medical field, including potential benefits and challenges of adoption. Methods: We designed a 33-question online survey that was distributed among health care professionals via targeted emails and professional Twitter and LinkedIn accounts. The survey included a range of questions to define respondents’ demographic characteristics, familiarity with ChatGPT, perceptions of this tool’s usefulness and reliability, and opinions on its potential to improve patient care, research, and education efforts. Results: One hundred and fifteen health care professionals from 21 countries responded to the survey, including physicians, nurses, researchers, and educators. Of these, 101 (87.8%) had heard of ChatGPT, mainly from peers, social media, and news, and 77 (76.2%) had used ChatGPT at least once. Participants found ChatGPT to be helpful for writing manuscripts (n=31, 45.6%), emails (n=25, 36.8%), and grants (n=12, 17.6%); accessing the latest research and evidence-based guidelines (n=21, 30.9%); providing suggestions on diagnosis or treatment (n=15, 22.1%); and improving patient communication (n=12, 17.6%). Respondents also felt that the ability of ChatGPT to access and summarize research articles (n=22, 46.8%), provide quick answers to clinical questions (n=15, 31.9%), and generate patient education materials (n=10, 21.3%) was helpful. However, there are concerns regarding the use of ChatGPT, for example, the accuracy of responses (n=14, 29.8%), limited applicability in specific practices (n=18, 38.3%), and legal and ethical considerations (n=6, 12.8%), mainly related to plagiarism or copyright violations. Participants stated that safety protocols such as data encryption (n=63, 62.4%) and access control (n=52, 51.5%) could assist in ensuring patient privacy and data security. Conclusions: Our findings show that ChatGPT use is widespread among health care professionals in daily clinical, research, and educational activities. The majority of our participants found ChatGPT to be useful; however, there are concerns about patient privacy, data security, and its legal and ethical issues as well as the accuracy of its information. Further studies are required to understand the impact of ChatGPT and other large language models on clinical, educational, and research outcomes, and the concerns regarding its use must be addressed systematically and through appropriate methods.",2025,2025-05-22 09:49:53,2025-05-26 13:44:27,2025-05-22 09:49:49,e58801,,,11.0,,,Global Health care Professionals’ Perceptions of Large Language Model Use In Practice,,,,,,,,,,,,Google Scholar,,"Publisher: JMIR Publications Toronto, Canada",,/Users/ksoares/Zotero/storage/PFH4BJU3/e58801.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9CWUUFPR,preprint,2024.0,"Feng, Xiachong; Dou, Longxu; Li, Ella; Wang, Qinghao; Wang, Haochuan; Guo, Yu; Ma, Chang; Kong, Lingpeng",A Survey on Large Language Model-Based Social Agents in Game-Theoretic Scenarios,,,,10.48550/arXiv.2412.03920,http://arxiv.org/abs/2412.03920,"Game-theoretic scenarios have become pivotal in evaluating the social intelligence of Large Language Model (LLM)-based social agents. While numerous studies have explored these agents in such settings, there is a lack of a comprehensive survey summarizing the current progress. To address this gap, we systematically review existing research on LLM-based social agents within game-theoretic scenarios. Our survey organizes the findings into three core components: Game Framework, Social Agent, and Evaluation Protocol. The game framework encompasses diverse game scenarios, ranging from choice-focusing to communication-focusing games. The social agent part explores agents' preferences, beliefs, and reasoning abilities. The evaluation protocol covers both game-agnostic and game-specific metrics for assessing agent performance. By reflecting on the current research and identifying future research directions, this survey provides insights to advance the development and evaluation of social agents in game-theoretic scenarios.",2024-12-05,2025-05-22 09:49:53,2025-05-22 09:49:53,2025-05-22 09:49:51,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2412.03920 [cs],,/Users/ksoares/Zotero/storage/CKM58788/Feng et al. - 2024 - A Survey on Large Language Model-Based Social Agents in Game-Theoretic Scenarios.pdf; /Users/ksoares/Zotero/storage/TD7U2TIP/2412.html,,,Computer Science - Computation and Language; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,arXiv:2412.03920,,,,,,,,,,,,,,,,,,,,,,,,,,,
D3KZJ4NC,journalArticle,2025.0,"Fu, Yu; Wang, Mingguo; Wang, Chengbin; Dong, Shuaixian; Chen, Jianguo; Wang, Jiyuan; Yu, Hongping; Huang, Jing; Chang, Liheng; Wang, Bo",GeoMinLM: A large language model in geology and mineral survey in Yunnan Province,Ore Geology Reviews,,,,https://www.sciencedirect.com/science/article/pii/S0169136825001982,"In recent years, the development of artificial intelligence and big data technologies has led to the advancement of tools and solutions for transforming the geological and mineral survey paradigm, which requires a large amount of geological knowledge in a complex and arduous working environment. The large language model (LLM) has a significant advantage in answering generative intelligent questions. However, LLMs for general fields have limitations in answering professional questions in a vertical domain like geology. To overcome this challenge, we proposed and developed GeoMinLM, an LLM for geological and mineral exploration scenarios in Yunnan Province, and explored its applications in intelligent Q&A. Leveraging a proprietary dataset of 5.16 million words in geology and mineral exploration, we trained GeoMinLM based on Baichuan-2, achieving superior performance through fine-tuning and hyperparameter optimization. By integrating expert knowledge via a knowledge graph, we significantly reduced hallucinations and enhanced professionalism. This study proves that GeoMinLM is helpful for accurate information retrieval and knowledge dissemination, thereby supporting the intelligent advancement of geological and mineral fields.",2025,2025-05-22 09:49:53,2025-05-26 13:44:11,2025-05-22 09:49:53,106638,,,,,,GeoMinLM,,,,,,,,,,,,Google Scholar,,Publisher: Elsevier,,/Users/ksoares/Zotero/storage/86LHETWG/S0169136825001982.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DRW22KKW,journalArticle,2025.0,"Song, Shezheng; Li, Xiaopeng; Li, Shasha; Zhao, Shan; Yu, Jie; Ma, Jun; Mao, Xiaoguang; Zhang, Weimin; Wang, Meng",How to Bridge the Gap between Modalities: Survey on Multimodal Large Language Model,IEEE Transactions on Knowledge and Data Engineering,,,,https://ieeexplore.ieee.org/abstract/document/10841938/,"We explore Multimodal Large Language Models (MLLMs), which integrate LLMs like GPT-4 to handle multimodal data, including text, images, audio, and more. MLLMs demonstrate capabilities such as generating image captions and answering image-based questions, bridging the gap towards real-world human-computer interactions and hinting at a potential pathway to artificial general intelligence. However, MLLMs still face challenges in addressing the semantic gap in multimodal data, which may lead to erroneous outputs, posing potential risks to society. Selecting the appropriate modality alignment method is crucial, as improper methods might require more parameters without significant performance improvements. This paper aims to explore modality alignment methods for LLMs and their current capabilities. Implementing effective modality alignment can help LLMs address environmental issues and enhance accessibility. The study surveys existing modality alignment methods for MLLMs, categorizing them into four groups: (1) Multimodal Converter, which transforms data into a format that LLMs can understand; (2) Multimodal Perceiver, which improves how LLMs percieve different types of data; (3) Tool Learning, which leverages external tools to convert data into a common format, usually text; and (4) Data-Driven Method, which teaches LLMs to understand specific data types within datasets.",2025,2025-05-22 09:51:42,2025-05-26 13:44:34,2025-05-22 09:51:06,,,,,,,How to Bridge the Gap between Modalities,,,,,,,,,,,,Google Scholar,,Publisher: IEEE,,/Users/ksoares/Zotero/storage/6JK3GVPK/scholar.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7QMDN745,journalArticle,2025.0,"Sargeant, Holli; Izzidien, Ahmed; Steffek, Felix",Topic classification of case law using a large language model and a new taxonomy for UK law: AI insights into summary judgment,Artificial Intelligence and Law,,"0924-8463, 1572-8382",10.1007/s10506-025-09434-0,https://link.springer.com/10.1007/s10506-025-09434-0,"Abstract             This paper addresses a critical gap in legal analytics by developing and applying a novel taxonomy for topic classification of summary judgment cases in the United Kingdom. Using a curated dataset of summary judgment cases, we use the Large Language Model Claude 3 Opus to explore functional topics and trends. We find that Claude 3 Opus correctly classified the topic with an accuracy of 87.13% and an F1 score of 0.87. The analysis reveals distinct patterns in the application of summary judgments across various legal domains. As case law in the United Kingdom is not originally labelled with keywords or a topic filtering option, the findings not only refine our understanding of the thematic underpinnings of summary judgments but also illustrate the potential of combining traditional and AI-driven approaches in legal classification. Therefore, this paper provides a new and general taxonomy for UK law. The implications of this work serve as a foundation for further research and policy discussions in the field of judicial administration and computational legal research methodologies.",2025-02-25,2025-05-22 09:51:42,2025-05-22 09:51:42,2025-05-22 09:51:09,,,,,,Artif Intell Law,Topic classification of case law using a large language model and a new taxonomy for UK law,,,,,,,en,,,,,DOI.org (Crossref),,,,/Users/ksoares/Zotero/storage/ERPWZL9V/Sargeant et al. - 2025 - Topic classification of case law using a large language model and a new taxonomy for UK law AI insi.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LWV6GX95,journalArticle,2025.0,"De Carvalho, Gabriel Peixoto; Sawanobori, Tetsuya; Horii, Takato","Data-Driven Motion Planning: A Survey on Deep Neural Networks, Reinforcement Learning, and Large Language Model Approaches",IEEE Access,,,,https://ieeexplore.ieee.org/abstract/document/10930422/,"Motion planning is a fundamental challenge in robotics, involving the creation of trajectories from start to goal states while meeting constraints like collision avoidance and joint limits. Its complexity increases with the number of robot joints. Several traditional approaches tackle this problem, such as sampling motion planning, grid-based methods, potential fields, and optimization techniques. Recent advancements in deep neural networks, reinforcement learning, and large language models enable new possibilities for solving motion planning problems by improving sampling efficiency, optimizing control policies, and enabling task planning through natural language prompts. This survey comprehensively reviews these novel approaches, providing background knowledge, analyzing key contributions, and identifying common patterns, limitations, and research gaps. Our work is the first to integrate all three major data-driven approaches, discussing their applications and future research directions.",2025,2025-05-22 09:51:42,2025-05-26 13:42:26,2025-05-22 09:51:12,,,,,,,Data-Driven Motion Planning,,,,,,,,,,,,Google Scholar,,Publisher: IEEE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MZ2LQIVV,journalArticle,2025.0,"Du, Shangheng; Zhao, Jiabao; Shi, Jinxin; Xie, Zhentao; Jiang, Xin; Bai, Yanhong; He, Liang",A Survey on the Optimization of Large Language Model-based Agents,arXiv preprint arXiv:2503.12434,,,,https://arxiv.org/abs/2503.12434,"With the rapid development of Large Language Models (LLMs), LLM-based agents have been widely adopted in various fields, becoming essential for autonomous decision-making and interactive tasks. However, current work typically relies on prompt design or fine-tuning strategies applied to vanilla LLMs, which often leads to limited effectiveness or suboptimal performance in complex agent-related environments. Although LLM optimization techniques can improve model performance across many general tasks, they lack specialized optimization towards critical agent functionalities such as long-term planning, dynamic environmental interaction, and complex decision-making. Although numerous recent studies have explored various strategies to optimize LLM-based agents for complex agent tasks, a systematic review summarizing and comparing these methods from a holistic perspective is still lacking. In this survey, we provide a comprehensive review of LLM-based agent optimization approaches, categorizing them into parameter-driven and parameter-free methods. We first focus on parameter-driven optimization, covering fine-tuning-based optimization, reinforcement learning-based optimization, and hybrid strategies, analyzing key aspects such as trajectory data construction, fine-tuning techniques, reward function design, and optimization algorithms. Additionally, we briefly discuss parameter-free strategies that optimize agent behavior through prompt engineering and external knowledge retrieval. Finally, we summarize the datasets and benchmarks used for evaluation and tuning, review key applications of LLM-based agents, and discuss major challenges and promising future directions. Our repository for related references is available at this https URL.",2025,2025-05-22 09:51:42,2025-05-26 13:40:50,2025-05-22 09:51:12,,,,,,,,,,,,,,,,,,,Google Scholar,,,,/Users/ksoares/Zotero/storage/U2EX32IQ/Du et al. - 2025 - A Survey on the Optimization of Large Language Model-based Agents.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
J9RMUUGE,journalArticle,2024.0,"Yamagata, Yuki; Yamada, Ryota",Survey on large language model annotation of cellular senescence from figures in review articles,Genomics & Informatics,,2234-0742,10.1186/s44342-024-00011-6,https://genomicsinform.biomedcentral.com/articles/10.1186/s44342-024-00011-6,"Abstract             This study evaluated large language models (LLMs), particularly the GPT-4 with vision (GPT-4 V) and GPT-4 Turbo, for annotating biomedical figures, focusing on cellular senescence. We assessed the ability of LLMs to categorize and annotate complex biomedical images to enhance their accuracy and efficiency. Our experiments employed prompt engineering with figures from review articles, achieving more than 70% accuracy for label extraction and approximately 80% accuracy for node-type classification. Challenges were noted in the correct annotation of the relationship between directionality and inhibitory processes, which were exacerbated as the number of nodes increased. Using figure legends was a more precise identification of sources and targets than using captions, but sometimes lacked pathway details. This study underscores the potential of LLMs in decoding biological mechanisms from text and outlines avenues for improving inhibitory relationship representations in biomedical informatics.",2024-06-17,2025-05-22 09:51:42,2025-05-22 09:51:42,2025-05-22 09:51:14,7,,1.0,22.0,,Genom. Inform.,,,,,,,,en,,,,,DOI.org (Crossref),,,,/Users/ksoares/Zotero/storage/YW9A4B6Y/Yamagata et Yamada - 2024 - Survey on large language model annotation of cellular senescence from figures in review articles.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZFDQEJ96,preprint,2025.0,"Yan, Yibo; Su, Jiamin; He, Jianxiang; Fu, Fangteng; Zheng, Xu; Lyu, Yuanhuiyi; Wang, Kun; Wang, Shen; Wen, Qingsong; Hu, Xuming","A Survey of Mathematical Reasoning in the Era of Multimodal Large Language Model: Benchmark, Method & Challenges",,,,10.48550/arXiv.2412.11936,http://arxiv.org/abs/2412.11936,"Mathematical reasoning, a core aspect of human cognition, is vital across many domains, from educational problem-solving to scientific advancements. As artificial general intelligence (AGI) progresses, integrating large language models (LLMs) with mathematical reasoning tasks is becoming increasingly significant. This survey provides the first comprehensive analysis of mathematical reasoning in the era of multimodal large language models (MLLMs). We review over 200 studies published since 2021, and examine the state-of-the-art developments in Math-LLMs, with a focus on multimodal settings. We categorize the field into three dimensions: benchmarks, methodologies, and challenges. In particular, we explore multimodal mathematical reasoning pipeline, as well as the role of (M)LLMs and the associated methodologies. Finally, we identify five major challenges hindering the realization of AGI in this domain, offering insights into the future direction for enhancing multimodal reasoning capabilities. This survey serves as a critical resource for the research community in advancing the capabilities of LLMs to tackle complex multimodal reasoning tasks.",2025-05-20,2025-05-22 09:51:42,2025-05-22 09:51:42,2025-05-22 09:51:16,,,,,,,A Survey of Mathematical Reasoning in the Era of Multimodal Large Language Model,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2412.11936 [cs],,"/Users/ksoares/Zotero/storage/QHUYHI7S/Yan et al. - 2025 - A Survey of Mathematical Reasoning in the Era of Multimodal Large Language Model Benchmark, Method.pdf; /Users/ksoares/Zotero/storage/ZIUXT86K/2412.html",,,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,arXiv:2412.11936,,,,,,,,,,,,,,,,,,,,,,,,,,,
W797U933,preprint,2025.0,"Feng, Jie; Zeng, Jinwei; Long, Qingyue; Chen, Hongyi; Zhao, Jie; Xi, Yanxin; Zhou, Zhilun; Yuan, Yuan; Wang, Shengyuan; Zeng, Qingbin; Li, Songwei; Zhang, Yunke; Lin, Yuming; Li, Tong; Ding, Jingtao; Gao, Chen; Xu, Fengli; Li, Yong","A Survey of Large Language Model-Powered Spatial Intelligence Across Scales: Advances in Embodied Agents, Smart Cities, and Earth Science",,,,10.48550/arXiv.2504.09848,http://arxiv.org/abs/2504.09848,"Over the past year, the development of large language models (LLMs) has brought spatial intelligence into focus, with much attention on vision-based embodied intelligence. However, spatial intelligence spans a broader range of disciplines and scales, from navigation and urban planning to remote sensing and earth science. What are the differences and connections between spatial intelligence across these fields? In this paper, we first review human spatial cognition and its implications for spatial intelligence in LLMs. We then examine spatial memory, knowledge representations, and abstract reasoning in LLMs, highlighting their roles and connections. Finally, we analyze spatial intelligence across scales -- from embodied to urban and global levels -- following a framework that progresses from spatial memory and understanding to spatial reasoning and intelligence. Through this survey, we aim to provide insights into interdisciplinary spatial intelligence research and inspire future studies.",2025-04-14,2025-05-22 09:51:42,2025-05-22 09:51:42,2025-05-22 09:51:16,,,,,,,A Survey of Large Language Model-Powered Spatial Intelligence Across Scales,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2504.09848 [cs],,/Users/ksoares/Zotero/storage/6BE4K6DN/Feng et al. - 2025 - A Survey of Large Language Model-Powered Spatial Intelligence Across Scales Advances in Embodied Ag.pdf; /Users/ksoares/Zotero/storage/22GEH8FF/2504.html,,,Computer Science - Computation and Language; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,arXiv:2504.09848,,,,,,,,,,,,,,,,,,,,,,,,,,,
SIZYGBUE,preprint,2025.0,"Liu, Liangdong; Zheng, Zhitong; Wang, Cong; Su, Tianhuang; Yang, Zhenyu",Binary Neural Networks for Large Language Model: A Survey,,,,10.48550/arXiv.2502.19008,http://arxiv.org/abs/2502.19008,"Large language models (LLMs) have wide applications in the field of natural language processing(NLP), such as GPT-4 and Llama. However, with the exponential growth of model parameter sizes, LLMs bring significant resource overheads. Low-bit quantization, as a key technique, reduces memory usage and computational demands by decreasing the bit-width of model parameters, activations, and gradients. Previous quantization methods for LLMs have largely employed Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT). PTQ does not require any retraining of the original model, while QAT involves optimizing precision during training to achieve the best quantization parameters. The BitNet team proposed a radically different approach, where quantization is performed from the start of model training, utilizing low-precision binary weights during the training process. This approach has led to the emergence of many binary quantization techniques for large language models. This paper provides a comprehensive review of these binary quantization techniques. Specifically, we will introduce binary quantization techniques in deep neural networks and further explore their application to LLMs, reviewing their various contributions, implementations, and applications.",2025-02-26,2025-05-22 09:51:42,2025-05-22 09:51:42,2025-05-22 09:51:16,,,,,,,Binary Neural Networks for Large Language Model,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2502.19008 [cs],,/Users/ksoares/Zotero/storage/DTR3W29B/Liu et al. - 2025 - Binary Neural Networks for Large Language Model A Survey.pdf; /Users/ksoares/Zotero/storage/WK5U9K9J/2502.html,,,Computer Science - Computation and Language; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,arXiv:2502.19008,,,,,,,,,,,,,,,,,,,,,,,,,,,
XIGFH7FK,preprint,2025.0,"Shu, Dong; Zhao, Haiyan; Hu, Jingyu; Liu, Weiru; Payani, Ali; Cheng, Lu; Du, Mengnan",Large Vision-Language Model Alignment and Misalignment: A Survey Through the Lens of Explainability,,,,10.48550/arXiv.2501.01346,http://arxiv.org/abs/2501.01346,"Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities in processing both visual and textual information. However, the critical challenge of alignment between visual and textual representations is not fully understood. This survey presents a comprehensive examination of alignment and misalignment in LVLMs through an explainability lens. We first examine the fundamentals of alignment, exploring its representational and behavioral aspects, training methodologies, and theoretical foundations. We then analyze misalignment phenomena across three semantic levels: object, attribute, and relational misalignment. Our investigation reveals that misalignment emerges from challenges at multiple levels: the data level, the model level, and the inference level. We provide a comprehensive review of existing mitigation strategies, categorizing them into parameter-frozen and parameter-tuning approaches. Finally, we outline promising future research directions, emphasizing the need for standardized evaluation protocols and in-depth explainability studies.",2025-02-06,2025-05-22 09:51:42,2025-05-22 09:51:42,2025-05-22 09:51:17,,,,,,,Large Vision-Language Model Alignment and Misalignment,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2501.01346 [cs],,/Users/ksoares/Zotero/storage/HTCLS4GP/Shu et al. - 2025 - Large Vision-Language Model Alignment and Misalignment A Survey Through the Lens of Explainability.pdf; /Users/ksoares/Zotero/storage/FYFB5UHF/2501.html,,,Computer Science - Computation and Language; Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,arXiv:2501.01346,,,,,,,,,,,,,,,,,,,,,,,,,,,
B7A3CR9H,preprint,2025.0,"Nie, Zhijie; Feng, Zhangchi; Li, Mingxin; Zhang, Cunwang; Zhang, Yanzhao; Long, Dingkun; Zhang, Richong",When Text Embedding Meets Large Language Model: A Comprehensive Survey,,,,10.48550/arXiv.2412.09165,http://arxiv.org/abs/2412.09165,"Text embedding has become a foundational technology in natural language processing (NLP) during the deep learning era, driving advancements across a wide array of downstream tasks. While many natural language understanding challenges can now be modeled using generative paradigms and leverage the robust generative and comprehension capabilities of large language models (LLMs), numerous practical applications - such as semantic matching, clustering, and information retrieval - continue to rely on text embeddings for their efficiency and effectiveness. Therefore, integrating LLMs with text embeddings has become a major research focus in recent years. In this survey, we categorize the interplay between LLMs and text embeddings into three overarching themes: (1) LLM-augmented text embedding, enhancing traditional embedding methods with LLMs; (2) LLMs as text embedders, adapting their innate capabilities for high-quality embedding; and (3) Text embedding understanding with LLMs, leveraging LLMs to analyze and interpret embeddings. By organizing recent works based on interaction patterns rather than specific downstream applications, we offer a novel and systematic overview of contributions from various research and application domains in the era of LLMs. Furthermore, we highlight the unresolved challenges that persisted in the pre-LLM era with pre-trained language models (PLMs) and explore the emerging obstacles brought forth by LLMs. Building on this analysis, we outline prospective directions for the evolution of text embedding, addressing both theoretical and practical opportunities in the rapidly advancing landscape of NLP.",2025-03-20,2025-05-22 09:51:42,2025-05-22 09:51:42,2025-05-22 09:51:22,,,,,,,When Text Embedding Meets Large Language Model,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2412.09165 [cs],,/Users/ksoares/Zotero/storage/8V7Y6HXX/Nie et al. - 2025 - When Text Embedding Meets Large Language Model A Comprehensive Survey.pdf; /Users/ksoares/Zotero/storage/9IXXGT82/2412.html,,,Computer Science - Computation and Language; Computer Science - Artificial Intelligence; Computer Science - Information Retrieval,,,,,,,,,,,,,,,,,,,arXiv:2412.09165,,,,,,,,,,,,,,,,,,,,,,,,,,,
ESYRQFDP,journalArticle,2024.0,"Li, Haochen; Leung, Jonathan; Shen, Zhiqi",Towards goal-oriented large language model prompting: A survey,arXiv e-prints,,,,https://ui.adsabs.harvard.edu/abs/2024arXiv240114043L/abstract,"Large Language Models (LLMs) have shown prominent performance in various downstream tasks and prompt engineering plays a pivotal role in optimizing LLMs' performance. This paper, not only as an overview of current prompt engineering methods, but also aims to highlight the limitation of designing prompts based on an anthropomorphic assumption that expects LLMs to think like humans. From our review of 50 representative studies, we demonstrate that a goal-oriented prompt formulation, which guides LLMs to follow established human logical thinking, significantly improves the performance of LLMs. Furthermore, We introduce a novel taxonomy that categorizes goal-oriented prompting methods into five interconnected stages and we demonstrate the broad applicability of our framework. With four future directions proposed, we hope to further emphasize the power and potential of goal-oriented prompt engineering in all fields.",2024,2025-05-22 09:51:42,2025-05-26 13:53:37,2025-05-22 09:51:22,arXiv–2401,,,,,,Towards goal-oriented large language model prompting,,,,,,,,,,,,Google Scholar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NGNYX4R9,journalArticle,2025.0,"Li, Ruimiao; Li, Manli; Qiao, Weifeng",Engineering Students’ Use of Large Language Model Tools: An Empirical Study Based on a Survey of Students from 12 Universities,Education Sciences,,,,https://www.mdpi.com/2227-7102/15/3/280,"Large language model (LLM) tools, such as ChatGPT, are rapidly transforming engineering education by enhancing tasks like information retrieval, coding, and writing refinement, which are critical to the problem-solving and technical focus of engineering disciplines. This study investigates how engineering students use LLM tools and the challenges they face, offering insights into the adoption of AI technologies in academic settings. A survey of 539 engineering students from 12 leading Chinese universities, using the UTAUT framework, examines factors such as technological expectations, environmental support, and personal characteristics. The key findings include the following: (1) Over 40% of engineering students use LLM tools, with 18.8% regarding them as indispensable. (2) Trust in AI-generated content remains a central challenge, as students must critically evaluate its accuracy and reliability. (3) Environmental support significantly affects usage, with notable regional disparities, particularly between eastern and other regions in China. (4) A persistent digital divide, influenced by gender, academic level, and socioeconomic background, affects the depth and effectiveness of tool use. These results underscore the need for targeted support to address regional and demographic disparities and optimize LLM tool integration in engineering education.",2025,2025-05-22 09:51:42,2025-05-26 13:42:37,2025-05-22 09:51:24,280,,3.0,15.0,,,Engineering Students’ Use of Large Language Model Tools,,,,,,,,,,,,Google Scholar,,Publisher: MDPI,,/Users/ksoares/Zotero/storage/Q8P7MZ4A/280.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KU6Z7MQD,preprint,2025.0,"Huo, Yukang; Tang, Hao",When Continue Learning Meets Multimodal Large Language Model: A Survey,,,,10.48550/arXiv.2503.01887,http://arxiv.org/abs/2503.01887,"Recent advancements in Artificial Intelligence have led to the development of Multimodal Large Language Models (MLLMs). However, adapting these pre-trained models to dynamic data distributions and various tasks efficiently remains a challenge. Fine-tuning MLLMs for specific tasks often causes performance degradation in the model's prior knowledge domain, a problem known as 'Catastrophic Forgetting'. While this issue has been well-studied in the Continual Learning (CL) community, it presents new challenges for MLLMs. This review paper, the first of its kind in MLLM continual learning, presents an overview and analysis of 440 research papers in this area.The review is structured into four sections. First, it discusses the latest research on MLLMs, covering model innovations, benchmarks, and applications in various fields. Second, it categorizes and overviews the latest studies on continual learning, divided into three parts: non-large language models unimodal continual learning (Non-LLM Unimodal CL), non-large language models multimodal continual learning (Non-LLM Multimodal CL), and continual learning in large language models (CL in LLM). The third section provides a detailed analysis of the current state of MLLM continual learning research, including benchmark evaluations, architectural innovations, and a summary of theoretical and empirical studies.Finally, the paper discusses the challenges and future directions of continual learning in MLLMs, aiming to inspire future research and development in the field. This review connects the foundational concepts, theoretical insights, method innovations, and practical applications of continual learning for multimodal large models, providing a comprehensive understanding of the research progress and challenges in this field, aiming to inspire researchers in the field and promote the advancement of related technologies.",2025-02-27,2025-05-22 09:51:42,2025-05-22 09:51:42,2025-05-22 09:51:26,,,,,,,When Continue Learning Meets Multimodal Large Language Model,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2503.01887 [cs],,/Users/ksoares/Zotero/storage/DETPR8JA/Huo et Tang - 2025 - When Continue Learning Meets Multimodal Large Language Model A Survey.pdf; /Users/ksoares/Zotero/storage/F7W6H44Y/2503.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,arXiv:2503.01887,,,,,,,,,,,,,,,,,,,,,,,,,,,
BDYQ33AU,preprint,2025.0,"Varangot-Reille, Clovis; Bouvard, Christophe; Gourru, Antoine; Ciancone, Mathieu; Schaeffer, Marion; Jacquenet, François",Doing More with Less -- Implementing Routing Strategies in Large Language Model-Based Systems: An Extended Survey,,,,10.48550/arXiv.2502.00409,http://arxiv.org/abs/2502.00409,"Large Language Models (LLM)-based systems, i.e. interconnected elements that include an LLM as a central component (e.g., conversational agents), are typically monolithic static architectures that rely on a single LLM for all user queries. However, they often require different preprocessing strategies, levels of reasoning, or knowledge. Generalist LLMs (e.g. GPT-4) trained on very large multi-topic corpora can perform well in a variety of tasks. They require significant financial, energy, and hardware resources that may not be justified for basic tasks. This implies potentially investing in unnecessary costs for a given query. To overcome this problem, a routing mechanism routes user queries to the most suitable components, such as smaller LLMs or experts in specific topics. This approach may improve response quality while minimising costs. Routing can be expanded to other components of the conversational agent architecture, such as the selection of optimal embedding strategies. This paper explores key considerations for integrating routing into LLM-based systems, focusing on resource management, cost definition, and strategy selection. Our main contributions include a formalisation of the problem, a novel taxonomy of existing approaches emphasising relevance and resource efficiency, and a comparative analysis of these strategies in relation to industry practices. Finally, we identify critical challenges and directions for future research.",2025-02-04,2025-05-22 09:51:42,2025-05-22 09:51:42,2025-05-22 09:51:28,,,,,,,Doing More with Less -- Implementing Routing Strategies in Large Language Model-Based Systems,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2502.00409 [cs],,/Users/ksoares/Zotero/storage/HPPYS5E2/Varangot-Reille et al. - 2025 - Doing More with Less -- Implementing Routing Strategies in Large Language Model-Based Systems An Ex.pdf; /Users/ksoares/Zotero/storage/AWBKPXB3/2502.html,,,Computer Science - Computation and Language; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,arXiv:2502.00409,,,,,,,,,,,,,,,,,,,,,,,,,,,
PRMG6724,journalArticle,2025.0,"Salehin, Imrus; Sajib, Md Tomal Ahmed; Badhon, Nazmul Huda; Rifat, Md Sakibul Hassan; Amin, Nazrul; Moon, Nazmun Nessa","Systematic Literature Review of LLM-Large Language Model in Medical: Digital Health, Technology and Applications",,,,,https://www.authorea.com/doi/full/10.22541/au.174587258.81848862,"\papertype Original Article Large language models (LLMs), like the GPT series, have recently emerged as transformative tools in the medical field due to their human-like language generation and understanding. This systematic review examines the evolution, applications, and challenges of medical LLMs in digital health and clinical technology. A structured search was conducted across ScienceDirect, PubMed, Scopus, and manual sources from 2007 to 2024, following PRISMA 2020 guidelines. After applying inclusion and exclusion criteria, 179 studies were selected from an initial pool of 698 papers. Among the 30 papers reviewed, most research centered on GPT-based models, with over 81% demonstrating strong performance in language generation, diagnostic assistance, and clinical documentation, based on automated metrics and human feedback. Notably, some models achieved up to 90% satisfaction from healthcare professionals. The findings reveal LLMs’ potential to enhance patient interaction, decision support, and overall healthcare efficiency. This review contributes by synthesizing key advancements, assessing model performance, and outlining ethical challenges such as trust, privacy, and safe deployment. It offers novel insights for researchers and practitioners seeking to adopt or improve LLM integration in healthcare. Future directions include improving transparency, developing domain-specific models, and establishing regulatory frameworks for responsible use.",2025,2025-05-22 09:51:42,2025-05-26 13:52:36,2025-05-22 09:51:28,,,,,,,Systematic Literature Review of LLM-Large Language Model in Medical,,,,,,,,,,,,Google Scholar,,,,"/Users/ksoares/Zotero/storage/IFNZ7HJF/Salehin et al. - 2025 - Systematic Literature Review of LLM-Large Language Model in Medical Digital Health, Technology and.pdf",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2E2NUEVR,conferencePaper,2024.0,"Lu, Yuting; Sun, Chao; Yan, Yuchao; Zhu, Hegong; Song, Dongdong; Peng, Qing; Yu, Li; Wang, Xiaozheng; Jiang, Jian; Ye, Xiaolong",A Comprehensive Survey of Datasets for Large Language Model Evaluation,2024 5th Information Communication Technologies Conference (ICTC),,,,https://ieeexplore.ieee.org/abstract/document/10601918/,"Natural Language Processing is an important branch of Artificial Intelligence. In the past few years, we have witnessed the remarkable advancement of large language models, however, how to evaluate them in a comprehensive way has become an urgent problem to be solved. Datasets can help evaluate and compare their performance and clarify their weaknesses. In order to guide the subsequent research work and promote the technological progress in the field, this paper collects 147 popular evaluation datasets, and proposes a new classification method to categorize them into six categories according to the evaluation capabilities. In addition, we organize several common evaluation metrics and usage scenarios. We compile the list of datasets and the main features (introduction, samples, metrics, links, etc.) into a document, which is consistently maintain available online at: https://github.com/lyt719/LLM-evaluation-datasets.",2024,2025-05-22 09:51:42,2025-05-26 13:37:01,2025-05-22 09:51:34,330–336,,,,,,,,,,,IEEE,,,,,,,Google Scholar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MG8A832A,conferencePaper,2025.0,"Huber, Thomas; Niklaus, Christina",LLMs meet Bloom’s Taxonomy: A Cognitive View on Large Language Model Evaluations,Proceedings of the 31st International Conference on Computational Linguistics,,,,https://aclanthology.org/2025.coling-main.350/,"Current evaluation approaches for Large Language Models (LLMs) lack a structured approach that reflects the underlying cognitive abilities required for solving the tasks. This hinders a thorough understanding of the current level of LLM capabilities. For instance, it is widely accepted that LLMs perform well in terms of grammar, but it is unclear in what specific cognitive areas they excel or struggle in. This paper introduces a novel perspective on the evaluation of LLMs that leverages a hierarchical classification of tasks. Specifically, we explore the most widely used benchmarks for LLMs to systematically identify how well these existing evaluation methods cover the levels of Bloom’s Taxonomy, a hierarchical framework for categorizing cognitive skills. This comprehensive analysis allows us to identify strengths and weaknesses in current LLM assessment strategies in terms of cognitive abilities and suggest directions for both future benchmark development as well as highlight potential avenues for LLM research. Our findings reveal that LLMs generally perform better on the lower end of Bloom’s Taxonomy. Additionally, we find that there are significant gaps in the coverage of cognitive skills in the most commonly used benchmarks.",2025,2025-05-22 09:51:42,2025-05-26 13:50:58,2025-05-22 09:51:36,5211–5246,,,,,,LLMs meet Bloom’s Taxonomy,,,,,,,,,,,,Google Scholar,,,,/Users/ksoares/Zotero/storage/IATLAGH4/Huber et Niklaus - 2025 - LLMs meet Bloom’s Taxonomy A Cognitive View on Large Language Model Evaluations.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
F6SKJTWE,journalArticle,2024.0,"Roy, Asitava Deb; Jaiswal, Ichchhit Bharat; Tiu, Devendra Nath; Das, Dipmala; Mondal, Shaikat; Behera, Joshil Kumar; Mondal, Himel; Jaiswal, Ichchhit B.; Tiu, Devendra; Behera IV, Joshil K.",Assessing the Utilization of Large Language Model Chatbots for Educational Purposes by Medical Teachers: A Nationwide Survey From India,Cureus,,,,https://www.cureus.com/articles/312879-assessing-the-utilization-of-large-language-model-chatbots-for-educational-purposes-by-medical-teachers-a-nationwide-survey-from-india.pdf,"Abstract  Background  Large language models (LLMs) are increasingly explored in healthcare and education. In medical education,  they hold the potential to enhance learning by supporting personalized teaching, resource development, and  student engagement. However, LLM use also raises concerns about ethics, accuracy, and reliance.  Understanding how educators leverage LLMs can help assess their role and implications in medical  education.  Methods  This cross-sectional online survey was conducted among medical teachers in India from December 2023 to  March 2024. A validated questionnaire with acceptable internal consistency and test-retest reliability was  used. It collected data on LLM chatbot usage patterns, as well as teachers' knowledge, attitudes, and  practices regarding LLMs for educational purposes.  Results  A total of 396 medical teachers with an average teaching experience of 4.12±2.47 (minimum six months,  maximum 13 years) years participated from different parts of India. The majority of the teachers heard about  ChatGPT (OpenAI, San Francisco, CA, USA) (85%), followed by Copilot/Bing (Microsoft, Washington, DC,  USA) (53%), and Gemini/Bard (Google, Mountain View, CA, USA) (45%) (p-value < 0.0001). However, 29% of  the respondents never used it and 47% rarely use LLMs for educational purposes (p-value < 0.0001). The  majority of the teachers use it for making any topic simple (55%), generating text for PowerPoint slides  (55%), generating multiple-choice questions (MCQs) (52%), and finding answers to student’s queries (35%).  Knowledge (3.4±0.47) showed the highest score, followed by practice (3.3±0.81) and attitude (3.14±0.46) (pvalue = 0.0023).  Conclusion  While awareness of LLMs was high among medical teachers in India, their actual usage for educational  purposes remains limited. Despite recognizing the potential of LLMs for simplifying topics, generating  teaching materials, and addressing student queries, a significant proportion of educators seldom integrate  these technologies into their teaching practices. Institutions may provide training to help medical educators  effectively integrate LLMs into teaching practices.",2024,2025-05-22 09:51:42,2025-05-26 13:41:42,2025-05-22 09:51:38,,,11.0,16.0,,,Assessing the Utilization of Large Language Model Chatbots for Educational Purposes by Medical Teachers,,,,,,,,,,,,Google Scholar,,Publisher: Cureus,,/Users/ksoares/Zotero/storage/XJNDP2SM/Roy et al. - 2024 - Assessing the Utilization of Large Language Model Chatbots for Educational Purposes by Medical Teach.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KCENCZ5B,journalArticle,2025.0,"Huang, Heyan; Li, Silin; Lan, Tianwei; Qiu, Yuli; Liu, Zeming; Yao, Jiashu; Zeng, Li; Shan, Yingyu; Shi, Xiaoming; Guo, Yuhang","A survey on the safety of large language model: Classification, evaluation, attribution, mitigation and prospect",CAAI Transactions on Intelligent Systems,,,,https://pure.bit.edu.cn/zh/publications/a-survey-on-the-safety-of-large-language-model-classification-eva,"Large language models can provide answers comparable to human levels in multiple fields. It demonstrates a wealth of emergent capabilities in fields and tasks that have not been trained. However, at present, there are many hidden dangers in artificial intelligence system based on large language model. The artificial intelligence systems based on large language model have many potential safety hazard. For example, large language models are vulnerable to undetectable attacks, including intricately elusive ones. The content generated by those models may have problems such as illegality, leaks, hatred, bias, errors, etc. Whafs more, in practical applications, the abuse of large language models is also an important issue. The content generated by the model may cause troubles at multiple levels such as countries, social groups, and fields. This paper aims to deeply explore and classify the safety risks feced by large language models, review existing evaluation methods, study the causal mechanisms behind the safety risks, and summarizes existing solutions. Specifically, this paper identifies 10 safety risks of large language models and categorizes them into two aspects: The safety risks of the model itself and the safety risks of the generated content. What's more, this paper systematically analyzes the safety risks of the large language model itself from two perspectives of life cycle and hazard level, and introduces the methods for risk assessment of existing large language models, the causes for occurrence of safety risks of large language model and corresponding mitigation methods. The safety risk of large language models is an important issue that needs to be solved urgently.",2025,2025-05-22 09:51:42,2025-05-26 13:56:38,2025-05-22 09:51:40,2–32,,1.0,20.0,,,A survey on the safety of large language model,,,,,,,,,,,,Google Scholar,,Publisher: Editorial Department of CAAI Transactions on Intelligent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
447VX95H,journalArticle,2024.0,"Gasparini, Loretta; Phillipson, Nitya; Capurro, Daniel; Rosenberg, Revital; Buttery, Jim; Howley, Jayne; Ranganathan, Sarath; Quinlan, Catherine; Selvadurai, Niloufer; Wildenauer, Michael","A survey of Large Language Model use in a hospital, research, and teaching campus",medRxiv,,,,https://www.medrxiv.org/content/10.1101/2024.09.11.24313512.abstract,"Abstract Background The use of Large Language Models (LLMs) has exploded since November 2022 but there is sparse evidence regarding LLM use in health, medical and research contexts. Objective To summarise the current uses of and attitudes towards LLMs across the clinical, research and teaching contexts in our campus. Design We administered a survey about LLM uses and attitudes. We conducted summary quantitative analysis and inductive qualitative analysis of free text responses. Setting In August-September 2023, we circulated the survey amongst all staff and students across our campus (approximately n=7500), a fully integrated paediatric academic hospital and research institute. Participants We received 281 anonymous survey responses. Main outcome measures We asked about participants’ knowledge of LLMs, their current use of LLMs in professional or learning contexts, and perspectives on possible future uses, opportunities, and risks of LLM use. Results Over 90% of respondents have heard of LLM tools and about two-thirds have used them in their work on our campus. Respondents reported using LLMs for a range of uses, including for generating or editing text and exploring ideas. Many, but not necessarily all, respondents seem aware of the limitations and potential risks of LLMs, including privacy and security risks. Various respondents expressed enthusiasm about opportunities of LLM use, including increased efficiency. Conclusions Our findings show LLM tools are already widely used on our campus. Guidelines and governance are needed to keep up with practice. We have developed recommendations for the use of LLMs on our campus using insights from this survey. What is known The known: The use of Large Language Models (LLMs) has increased rapidly since the introduction of ChatGPT in November 2022. The new: Most survey respondents are aware of, if not using, LLMs in their work across our hospital, research, and university campus. Diverse uses were reported, including generating or editing text and exploring ideas. There were varying attitudes towards LLMs. Perceived risks included privacy and security risks. A key perceived opportunity was increased efficiency. The implications: LLM tools are already widely used on our campus, highlighting the need for guidelines and governance to keep up with practice.",2024,2025-05-22 09:51:42,2025-05-26 13:38:09,2025-05-22 09:51:42,2024–09,,,,,,,,,,,,,,,,,,Google Scholar,,Publisher: Cold Spring Harbor Laboratory Press,,"/Users/ksoares/Zotero/storage/58ZMT55Q/Gasparini et al. - 2024 - A survey of Large Language Model use in a hospital, research, and teaching campus.pdf",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RZGXZN2M,journalArticle,2024.0,"Crandall, Johannah L.; Crandall, Aaron S.",Large Language Model-Supported Software Testing with the CS Matrix Taxonomy,Journal of Computing Sciences in Colleges,,,,https://www.ccsc.org/publications/journals/CCSCNW2024Final.pdf#page=49,"New breakthroughs in code synthesis from Generative Pre-Trained Transformers (GPT) and Large Language Model (LLM) algorithms are driving significant changes to software engineering education. Having algorithms able to generate components of a software project means that software developers will need stronger skills in requirements specification to guide code generation as well as stronger skills in code review, testing, and integration to incorporate AI-generated code into projects. Shifts in industry and classroom practices are already occurring with the availability of inline code generation tools like GitHub’s Copilot, which makes discussion of pedagogical strategies in this area a timely topic. Of immediate concern in computer science education is the potential for LLM-generated code and code help to undermine the learning of CS students. In order to avoid such undermining in even intentional uses of LLM-enhanced learning supports, it is necessary to clarify the roles such supports need to play in the pedagogical process. The Computer Science Matrix Taxonomy provides a strong framework for organizing software testing learning outcomes as well as delineating the operational space in which LLM-based feedback tools should operate to support those learning outcomes. In this paper, the authors operationalize the CS Matrix Taxonomy for software testing learning outcomes and illustrate the integration of LLM-generated test strategy suggestions as an extension of the peer coding/testing model. The work includes examples of AI-generated code testing suggestions that students would use to help guide their own code synthesis for assignments or projects.",2024,2025-05-22 09:58:02,2025-05-26 13:50:36,2025-05-22 09:57:26,49–58,,1.0,40.0,,,,,,,,,,,,,,,Google Scholar,,Publisher: Consortium for Computing Sciences in Colleges,,/Users/ksoares/Zotero/storage/TMH299YM/Crandall et Crandall - 2024 - Large Language Model-Supported Software Testing with the CS Matrix Taxonomy.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GZQW56KA,conferencePaper,2024.0,"Nguyen, Tuyen T.; Vu, Huyen TT; Nguyen, Hoa N.","Security, Privacy, and Ethical Challenges of Artificial Intelligence in Large Language Model Scope: A Comprehensive Survey",2024 1st International Conference On Cryptography And Information Security (VCRIS),,,,https://ieeexplore.ieee.org/abstract/document/10813382/,"Artificial intelligence (AI) models like ChatGPT and LLama have changed how we understand and create human-like language. They understand language well, can generate text like a person, know the context, and solve problems effectively. This makes them useful in many areas like search engines, customer service, and translation. Recently, these models have also caught the attention of the security field, uncovering weaknesses in security and proving useful for security tasks. This paper offers an extensive review of the security, privacy, and ethical issues associated with AI, focusing on LLMs and their impact on various domains. We explore the vulnerabilities of LLMs, propose poten-tial defense mechanisms, and highlight the broader implications of AI on human society. This survey intends to offer a clear perspective for researchers and practitioners, and policymakers on responsibly developing and deploying AI technologies.",2024,2025-05-22 09:58:02,2025-05-26 13:51:50,2025-05-22 09:57:28,1–6,,,,,,"Security, Privacy, and Ethical Challenges of Artificial Intelligence in Large Language Model Scope",,,,,IEEE,,,,,,,Google Scholar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VW3HH4K2,journalArticle,2024.0,"Gresham, Dylan",Exploring and Evaluating Large Language Model Survey Paper Categories,,,,,https://www.preprints.org/frontend/manuscript/bf48e3c9d32a32f3e3585fca5754db91/download_pub,"In February of 2024, Dr. Jun Zhuang and Dr. Casey Kennington published a paper [1] in which they  classified large language model (LLM) survey papers into different taxonomies utilizing graph learning. In this  paper, I evaluate the dataset they created and used and propose that in its current state, there is not enough  samples to classify other survey papers into specific categories.",2024,2025-05-22 09:58:02,2025-05-26 13:43:53,2025-05-22 09:57:30,,,,,,,,,,,,,,,,,,,Google Scholar,,,,/Users/ksoares/Zotero/storage/Q72WFSWL/Gresham - 2024 - Exploring and Evaluating Large Language Model Survey Paper Categories.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6ICG7GR9,journalArticle,2024.0,"Tania, Meherunnesa",Automated Classification and Trend Analysis of Large Language Model Survey Papers Using Machine Learning and Natural Language Processing Techniques,,,,,https://engrxiv.org/preprint/view/3984,"This study investigates the application of machine learning (ML) and natural language processing (NLP) techniques to classify academic survey papers into predefined taxonomy categories. The dataset, consisting of paper titles, summaries, release dates, taxonomy labels, and categories, was analyzed to uncover trends and patterns in the publication of research papers. Exploratory data analysis (EDA) revealed important insights through visualizations, such as publication trends over time, the distribution of taxonomy categories, and the most common terms used in paper summaries. Key NLP techniques, including Term Frequency-Inverse Document Frequency (TF-IDF), were employed to transform the textual data into numerical features, while one-hot encoding was applied to the categorical data. A Random Forest Classifier was trained on the extracted feature matrix to predict the taxonomy category of each paper. The model achieved promising accuracy, effectively capturing patterns in the dataset. The study also identified areas for future improvement, including addressing class imbalance and exploring more sophisticated models. These findings demonstrate the potential of ML and NLP for automating the classification of academic papers, providing a scalable solution for managing large collections of research literature while offering insights into publication dynamics and trends.",2024,2025-05-22 09:58:02,2025-05-26 13:41:50,2025-05-22 09:57:32,,,,,,,,,,,,,,,,,,,Google Scholar,,Publisher: Engineering Archive,,/Users/ksoares/Zotero/storage/PTVSXJS9/Tania - 2024 - Automated Classification and Trend Analysis of Large Language Model Survey Papers Using Machine Lear.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
S2DNP9MW,journalArticle,2024.0,"Dong, Kaiwen",Large Language Model Applied in Multi-agent System—A Survey,Applied and Computational Engineering,,,,https://www.ewadirect.com/proceedings/ace/article/view/17486,"The application of large language models (LLMs) in single-agent systems within complex environments has proven successful, prompting a growing interest in their use within multi-agent systems (MAS). Despite the impressive capabilities of LLMs, it remains unclear how they can be optimally integrated and utilized to empower agents in MAS. Understanding how to effectively leverage the advantages of LLMs to enhance agent performance is crucial. This survey provides a comprehensive overview of the application of LLMs in MAS, focusing on their impact on agent cooperation, reasoning, and adaptive abilities. Finally, we discuss future directions and open questions in this evolving field.",2024,2025-05-22 09:58:02,2025-05-26 13:47:46,2025-05-22 09:57:34,9–16,,,109.0,,,,,,,,,,,,,,,Google Scholar,,,,/Users/ksoares/Zotero/storage/IBT75JY8/17486.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BXEFGF38,journalArticle,,"Boateng, Desmond",Taxonomy Classification of Large Language Model Survey Papers,,,,,https://engrxiv.org/preprint/view/3975,"This study develops a taxonomy classification model using traditional and deep learning methods. After data exploration, a Linear Support Vector Classifier (SVC) provides a baseline performance. However, a deep learning model with TF-IDF vectorization, dynamic learning rate scheduling, and early stopping proves more effective",,2025-05-22 09:58:02,2025-05-26 13:52:44,2025-05-22 09:57:36,,,,,,,,,,,,,,,,,,,Google Scholar,,Publisher: Engineering Archive,,/Users/ksoares/Zotero/storage/RNI9GRIL/Boateng - Taxonomy Classification of Large Language Model Survey Papers.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RQ4DV2RA,journalArticle,,"Zhang, Guanming",Exploratory Data Analysis and Modeling of Large Language Model Survey Papers: Data analysis,,,,,https://engrxiv.org/preprint/view/3955,"In this report, we present an exploratory data analysis of a dataset containing survey papers related to large language models (LLMs). The analysis is performed using a combination of data manipulation techniques with Pandas, visualized with Matplotlib, and preprocessed using Sklearn for modeling purposes. Key insights regarding trends, popular research topics, and metadata of the surveyed papers are discussed. This analysis aims to provide a better understanding of the recent advancements and research directions in the field of LLMs, contributing to the broader study of artificial intelligence. Our report includes data exploration, modeling performance, and a discussion of the results, offering a clear methodology for analyzing academic datasets.",,2025-05-22 09:58:02,2025-05-26 13:43:03,2025-05-22 09:57:38,,,,,,,Exploratory Data Analysis and Modeling of Large Language Model Survey Papers,,,,,,,,,,,,Google Scholar,,Publisher: Engineering Archive,,/Users/ksoares/Zotero/storage/X9RM4KU9/Zhang - Exploratory Data Analysis and Modeling of Large Language Model Survey Papers Data analysis.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FGBJ77GM,journalArticle,,"Afrin, Mehenaz",Exploring Large Language Model survey papers via Machine and Ensemble Learning,,,,,https://engrxiv.org/preprint/view/3992,"Nowadays, there is an influx of researchers emphasizing Large Language Models (LLMs). While the field is broadening, it becomes difficult to keep up all the models, and techniques associated with the novel idea. To tackle this problem, a study has been conducted for assigning survey papers to taxonomy in an automated way. In this assignment, I am using their dataset for the task of exploration, manipulation, and evaluation. After finishing the instructed part, I did further exploration by using a cross tab between taxonomy and date, representing different visualizations for survey papers by taxonomy over time, and plotting the box of release day by taxonomy title. The experimental analysis indicates that Logistic Regression (LR) outperformed all the 8 Classifiers in terms of accuracy score, while GaussianNB (GNB) shows the most commendable precision score. For weighted recall and f1 score, LR shows the highest performance in text classification data.",,2025-05-22 09:58:02,2025-05-26 13:44:01,2025-05-22 09:57:40,,,,,,,,,,,,,,,,,,,Google Scholar,,Publisher: Engineering Archive,,/Users/ksoares/Zotero/storage/V8E9LVEU/Afrin - Exploring Large Language Model survey papers via Machine and Ensemble Learning.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A4HEG8TU,bookSection,2024.0,"Wang, Taowen; Fang, Zheng; Xue, Haochen; Zhang, Chong; Jin, Mingyu; Xu, Wujiang; Shu, Dong; Yang, Shanchieh; Wang, Zhenting; Liu, Dongfang",Large Vision-Language Model Security: A Survey,Frontiers in Cyber Security,978-981-96-0150-9 978-981-96-0151-6,,,https://link.springer.com/10.1007/978-981-96-0151-6_1,"In the domain of Large Vision-Language Models (LVLMs), securing these models has emerged as a critical issue for both researchers and practitioners. In this paper, we highlight and analyze the security-related issues of LVLMs, with a special emphasis on the reliability challenges in practical deployments. We begin by reviewing recent studies on threats like jailbreak and backdoor attacks, alongside discussing the potential countermeasures implemented to mitigate these risks. Additionally, we touch on real-world application problems, such as hallucinations and privacy leakages, as well as the ethical and legal related researches around them. We also outline the shortcomings observed in current studies and discuss directions for future research, with the aim of promoting LVLMs towards a safer direction. A curated list of LVLMs-security-related resources is also available at https://github.com/MingyuJ666/LVLM-Safety.",2024,2025-05-22 09:58:02,2025-05-26 13:50:47,2025-05-22 09:57:40,3-22,,,2315.0,,,Large Vision-Language Model Security,,,,,Springer Nature Singapore,Singapore,en,,,,,DOI.org (Crossref),,Series Title: Communications in Computer and Information Science DOI: 10.1007/978-981-96-0151-6_1,,/Users/ksoares/Zotero/storage/776IXIGQ/Wang et al. - 2024 - Large Vision-Language Model Security A Survey.pdf,,,,"Chen, Biwen; Fu, Xinwen; Huang, Min",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SVAHZLNU,journalArticle,2024.0,"Ahn, Sangzin",Large language model usage guidelines in Korean medical journals: a survey using human-artificial intelligence collaboration,Journal of Yeungnam Medical Science,,,,https://pmc.ncbi.nlm.nih.gov/articles/PMC11812075/,"Background Large language models (LLMs), the most recent advancements in artificial intelligence (AI), have profoundly affected academic publishing and raised important ethical and practical concerns. This study examined the prevalence and content of AI guidelines in Korean medical journals to assess the current landscape and inform future policy implementation. Methods The top 100 Korean medical journals determined by Hirsh index were surveyed. Author guidelines were collected and screened by a human researcher and AI chatbot to identify AI-related content. The key components of LLM policies were extracted and compared across journals. The journal characteristics associated with the adoption of AI guidelines were also analyzed. Results Only 18% of the surveyed journals had LLM guidelines, which is much lower than previously reported in international journals. However, the adoption rates increased over time, reaching 57.1% in the first quarter of 2024. High-impact journals were more likely to have AI guidelines. All journals with LLM guidelines required authors to declare LLM tool use and 94.4% prohibited AI authorship. The key policy components included emphasizing human responsibility (72.2%), discouraging AI-generated content (44.4%), and exempting basic AI tools (38.9%). Conclusion While the adoption of LLM guidelines among Korean medical journals is lower than the global trend, there has been a clear increase in implementation over time. The key components of these guidelines align with international standards, but greater standardization and collaboration are needed to ensure the responsible and ethical use of LLMs in medical research and writing.",2024,2025-05-22 09:58:02,2025-05-26 13:49:25,2025-05-22 09:57:44,14,,,42.0,,,Large language model usage guidelines in Korean medical journals,,,,,,,,,,,,Google Scholar,,,,/Users/ksoares/Zotero/storage/5YFYI9RG/PMC11812075.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
S5GKM6YN,journalArticle,,"Zhou, Duo; Zhang, Junyu; Feng, Tao; Sun, Yifan",A Survey on Alignment for Large Language Model Agents,,,,,https://openreview.net/forum?id=gkxt5kZS84,"As large language models (LLMs) evolve from passive text generators to autonomous agents capable of decision-making and real-world interaction, ensuring their alignment with human goals, values, and safety expectations becomes increasingly critical. This survey offers a comprehensive examination of alignment in the context of LLM-based agents, spanning technical, ethical, and sociotechnical dimensions. We begin by defining the multifaceted goals of agent alignment, including task fidelity, ethical compliance, and long-term behavioral robustness. We then analyze sources and challenges of alignment data, review alignment techniques such as reinforcement learning from human feedback (RLHF), adversarial training, and scalable oversight strategies, and assess benchmark methodologies across general intent following, safety robustness, ethical reasoning, and multimodal performance. Looking forward, we identify key research directions, including constitutional AI, graph-based multi-agent coordination, and super alignment for heterogeneous agent clusters. By synthesizing recent advances, this survey provides a roadmap toward building trustworthy and controllable LLM agents for real-world deployment.",,2025-05-22 09:58:02,2025-05-26 13:38:44,2025-05-22 09:57:46,,,,,,,,,,,,,,,,,,,Google Scholar,,,,/Users/ksoares/Zotero/storage/SXJQSY6F/Zhou et al. - A Survey on Alignment for Large Language Model Agents.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WJDQUSBR,journalArticle,2025.0,"Memmert, Lucas; Borchers, Marten; Plückhahn, Juliane; Bittner, Eva",Solving Coding Challenges Jointly with a Large Language Model: Understanding Student Journeys Through Bloom’s Taxonomy,,,,,https://scholarspace.manoa.hawaii.edu/items/f8659bbb-1974-4f11-914a-5b63a3e7deae,"With the advancement of large language models (LLMs) such as ChatGPT, students increasingly employ LLMs during university courses to improve their coding skills. However, it is still unclear what using these systems means for student learning. In our study, we explore how students employ a LLM when solving a realistic coding challenge over the course of a semester in small groups. We analyze students’ LLM chat log data through the framework of Bloom’s taxonomy, a well-established education framework, to understand their problem-solving and learning behavior. We enrich our analysis with students’ responses to weekly survey questions, contextualizing our findings with subjective experiences. We find that students primarily use LLMs for lower-order thinking skills like remembering, understanding, and applying procedural knowledge. With our study, we contribute to the growing literature on understanding the effects of working and learning with LLMs and offer practical suggestions for teachers and students.",2025,2025-05-22 09:58:02,2025-05-26 13:52:08,2025-05-22 09:57:48,,,,,,,Solving Coding Challenges Jointly with a Large Language Model,,,,,,,,,,,,Google Scholar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TCJUHHBK,journalArticle,2025.0,"Ahn, Sangzin",Large language model usage guidelines in Korean medical journals: a survey using human-artificial intelligence collaboration,Journal of Yeungnam Medical Science,,,,http://www.e-jyms.org/upload/pdf/jyms-2024-00794.pdf,"Background: Large language models (LLMs), the most recent advancements in artificial intelligence (AI), have profoundly affected academic publishing and raised important ethical and practical concerns. This study examined the prevalence and content of AI guidelines in Korean medical journals to assess the current landscape and inform future policy implementation. Methods: The top 100 Korean medical journals determined by Hirsh index were surveyed. Author guidelines were collected and screened by a human researcher and AI chatbot to identify AI-related content. The key components of LLM policies were extracted and compared across journals. The journal characteristics associated with the adoption of AI guidelines were also analyzed. Results: Only 18% of the surveyed journals had LLM guidelines, which is much lower than previously reported in international journals. However, the adoption rates increased over time, reaching 57.1% in the first quarter of 2024. High-impact journals were more likely to have AI guidelines. All journals with LLM guidelines required authors to declare LLM tool use and 94.4% prohibited AI authorship. The key policy components included emphasizing human responsibility (72.2%), discouraging AI-generated content (44.4%), and exempting basic AI tools (38.9%). Conclusion: While the adoption of LLM guidelines among Korean medical journals is lower than the global trend, there has been a clear increase in implementation over time. The key components of these guidelines align with international standards, but greater standardization and collaboration are needed to ensure the responsible and ethical use of LLMs in medical research and writing.",2025,2025-05-22 09:58:02,2025-05-26 13:49:35,2025-05-22 09:57:50,1–7,,1.0,42.0,,,Large language model usage guidelines in Korean medical journals,,,,,,,,,,,,Google Scholar,,Publisher: 영남대학교 의과대학,,/Users/ksoares/Zotero/storage/5UIZA2VX/Ahn - 2025 - Large language model usage guidelines in Korean medical journals a survey using human-artificial in.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
P6CHIMAP,journalArticle,,"Dong, Yifei; Wu, Fengyi; Zhang, Kunlin; Li, Heng; Sun, Jingdong; Cheng, Zhi-Qi","Large Language Model Agents in Finance: A Survey Bridging Research, Practice, and Real-World Deployment",,,,,https://www.researchgate.net/profile/Yifei-Dong-11/publication/389357318_Large_Language_Model_Agents_in_Finance_A_Survey_Bridging_Research_Practice_and_Real-World_Deployment/links/67bfcb9b645ef274a495cfe8/Large-Language-Model-Agents-in-Finance-A-Survey-Bridging-Research-Practice-and-Real-World-Deployment.pdf,none,,2025-05-22 09:58:02,2025-05-26 14:00:33,2025-05-22 09:57:52,,,,,,,Large Language Model Agents in Finance,,,,,,,,,,,,Google Scholar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MWFXNCK2,journalArticle,2025.0,"Man-Li, L. I.; Wei-Feng, QIAO; Rui-Miao, L. I.",Can Large Language Model Tools Promote the Development of University Students’ Higher-Order Thinking Skills?——An Empirical Analysis Based on a Questionnaire Survey of Students from 12 Double First-Class Universities,Modern Educational Technology,,,,https://www.sciopen.com/article/10.3969/j.issn.1009-8097.2025.01.004,"In the era of intelligent technology, it has gradually become a consensus to pay attention to the cultivation of university students’ higher-order thinking skills. However, there remains debate over whether university students can use large language model tools to promote the development of higher-order thinking skills is still controversial. Clarifying this dispute not only helps to theoretically analyze the mechanism of students’ higher-order thinking development under the background of AI, but also provides a reliable basis for what measures to be taken by schools and teachers in educational practice. Accordingly, this paper made an empirical analysis on university students’ use situation of large language model tools and its impact on higher-order thinking skills using questionnaire survey data of students from 12 double first-class universities in China. The results showed that more than half of university students used large language model tools, but the deep creative application still needed to be strengthened; increasing the frequency of basic execution and deep creative application of large language model tools had a significant positive effect on the development of higher-order thinking skills; interaction quality played a mediating role in the influence of the use frequency of large language model tool on higher-order thinking skills. Based on this, the paper suggested that universities should acknowledge the role of AI technology in the teaching process of higher education and actively promote the application of AI in empowering education, combine real situations inside and outside the class to promote the deep creative application of large language model tools, and strengthen the cultivation of AI literacy of teachers and students to improve the quality of human-computer interaction.",2025,2025-05-22 09:58:02,2025-05-26 13:42:16,2025-05-22 09:57:57,34–43,,1.0,35.0,,,Can Large Language Model Tools Promote the Development of University Students’ Higher-Order Thinking Skills?,,,,,,,,,,,,Google Scholar,,Publisher: 《 现代教育技术》 编辑部,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GFTAIJ5S,preprint,2024.0,"Wang, Wenxiao; Chen, Wei; Luo, Yicong; Long, Yongliu; Lin, Zhengkai; Zhang, Liye; Lin, Binbin; Cai, Deng; He, Xiaofei",Model Compression and Efficient Inference for Large Language Models: A Survey,,,,10.48550/arXiv.2402.09748,http://arxiv.org/abs/2402.09748,"Transformer based large language models have achieved tremendous success. However, the significant memory and computational costs incurred during the inference process make it challenging to deploy large models on resource-constrained devices. In this paper, we investigate compression and efficient inference methods for large language models from an algorithmic perspective. Regarding taxonomy, similar to smaller models, compression and acceleration algorithms for large language models can still be categorized into quantization, pruning, distillation, compact architecture design, dynamic networks. However, Large language models have two prominent characteristics compared to smaller models: (1) Most of compression algorithms require finetuning or even retraining the model after compression. The most notable aspect of large models is the very high cost associated with model finetuning or training. Therefore, many algorithms for large models, such as quantization and pruning, start to explore tuning-free algorithms. (2) Large models emphasize versatility and generalization rather than performance on a single task. Hence, many algorithms, such as knowledge distillation, focus on how to preserving their versatility and generalization after compression. Since these two characteristics were not very pronounced in early large models, we further distinguish large language models into medium models and ``real'' large models. Additionally, we also provide an introduction to some mature frameworks for efficient inference of large models, which can support basic compression or acceleration algorithms, greatly facilitating model deployment for users.",2024-02-15,2025-05-22 09:58:02,2025-05-22 09:58:02,2025-05-22 09:57:58,,,,,,,Model Compression and Efficient Inference for Large Language Models,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2402.09748 [cs],,/Users/ksoares/Zotero/storage/M3HPTBPD/Wang et al. - 2024 - Model Compression and Efficient Inference for Large Language Models A Survey.pdf; /Users/ksoares/Zotero/storage/VGKQV982/2402.html,,,Computer Science - Computation and Language; Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Performance,,,,,,,,,,,,,,,,,,,arXiv:2402.09748,,,,,,,,,,,,,,,,,,,,,,,,,,,
SRYPRK8V,conferencePaper,2024.0,"Kim, Hyun-Jun; Yun, Subin; Paek, Yun-Heung",A Survey on Fine-tuned Large Language Model-based Vulnerability Detection System,Annual Conference of KIPS,,,,https://koreascience.kr/article/CFKO202433162337778.page,"본 논문은 소스 코드에 내재된 취약점을 탐색하기 위해 대형 언어 모델을 취약점 탐색 태스크에 맞게 파인튜닝하여 사용하는 최신 연구들을 소개한다. 각 연구에서 대형 언어 모델을 활용하여 중점적으로 해결하려는 문제와 솔루션을 설명하고, 향후 연구 방향을 조망하려 한다.",2024,2025-05-22 09:58:02,2025-05-26 13:57:31,2025-05-22 09:57:58,212–215,,,,,,,,,,,Korea Information Processing Society,,,,,,,Google Scholar,,,,/Users/ksoares/Zotero/storage/IRSEKI39/Kim et al. - 2024 - A Survey on Fine-tuned Large Language Model-based Vulnerability Detection System.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
78YFM9M3,journalArticle,2024.0,"Zhu, Xunyu; Li, Jian; Liu, Yong; Ma, Can; Wang, Weiping",A survey on model compression for large language models,Transactions of the Association for Computational Linguistics,,,,https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00704/125482,"Large Language Models (LLMs) have transformed natural language processing tasks successfully. Yet, their large size and high computational needs pose challenges for practical use, especially in resource-limited settings. Model compression has emerged as a key research area to address these challenges. This paper presents a survey of model compression techniques for LLMs. We cover methods like quantization, pruning, and knowledge distillation, highlighting recent advancements. We also discuss benchmarking strategies and evaluation metrics crucial for assessing compressed LLMs. This survey offers valuable insights for researchers and practitioners, aiming to enhance efficiency and real-world applicability of LLMs while laying a foundation for future advancements.",2024,2025-05-22 09:58:02,2025-05-26 13:40:01,2025-05-22 09:58:00,1556–1577,,,12.0,,,,,,,,,,,,,,,Google Scholar,,"Publisher: MIT Press 255 Main Street, 9th Floor, Cambridge, Massachusetts 02142, USA …",,/Users/ksoares/Zotero/storage/CW4AEXVA/Zhu et al. - 2024 - A survey on model compression for large language models.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FX9HJXBQ,conferencePaper,2024.0,"Xu, Chen",基于大语言模型的自主智能体概述 (A Survey on Large Language Model based Autonomous Agents Xu Chen),Proceedings of the 23rd Chinese National Conference on Computational Linguistics (Volume 2: Frontier Forum),,,,https://aclanthology.org/2024.ccl-2.8/,"“近年来,基于大语言模型的自主智能体受到了学术界和工业界的广泛关注,其关键在于利用大语言模型作为核心控制器,并设计相应的辅助模块增强智能体在动态环境中的演化和适应能力,从而提升智能体自主解决任务的能力。本文通过总结过去工作,抽象出智能体设计的通用范式,并讨论了大模型时代自主智能体能力提升的途径。我们还从个体拓展到系统,深入探讨了多自主智能体系统常见的交互机制和面临的重要问题。”",2024,2025-05-22 09:58:02,2025-05-26 13:55:01,2025-05-22 09:58:02,141–150,,,,,,,,,,,,,,,,,,Google Scholar,,,,/Users/ksoares/Zotero/storage/Z6WB74YB/Xu - 2024 - 基于大语言模型的自主智能体概述 (A Survey on Large Language Model based Autonomous Agents Xu Chen).pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VH5YVN8S,conferencePaper,2024.0,"Changjiang, Gao; Hao, Zhou; Shuaijie, She; Haoming, Zhong; Sizhe, Liu; Zhejian, Lai; Zhijun, Wang; Shujian, Huang",大模型时代的多语言研究综述 (A Survey of Multilingual Research in the Large Language Model Era),Proceedings of the 23rd Chinese National Conference on Computational Linguistics (Volume 2: Frontier Forum),,,,https://aclanthology.org/2024.ccl-2.4/,"“进入大语言模型时代以来,传统的多语言研究模式发生了巨大变化。一些传统任务得到了突破性的解决,也出现了多种新任务,以及许多以多语言大模型为基础、面向大模型能力提升的多语言研究工作。本文针对研究领域中的这一新变化,整理归纳了进入大模型时代以来的多语言研究进展,包括多语言大模型、数据集、任务,以及相关的前沿研究方向、研究挑战等,希望能为大模型范式下的多语言研究的未来发展提供参考和帮助。”",2024,2025-05-22 09:59:26,2025-05-26 13:55:12,2025-05-22 09:59:26,63–85,,,,,,,,,,,,,,,,,,Google Scholar,,,,/Users/ksoares/Zotero/storage/RXUF92ZC/Changjiang et al. - 2024 - 大模型时代的多语言研究综述 (A Survey of Multilingual Research in the Large Language Model Era).pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
